{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# So code is automatically reloaded when saved in different modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  \n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import sys\n",
    "sys.path.append('src/taggerSystem/')\n",
    "from my_util import print_sentence, write_conll, read_conll\n",
    "from my_data_util import load_and_preprocess_data, load_embeddings, ModelHelper\n",
    "logger = logging.getLogger(\"hw3.q2\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = \"data/smallIcd9NotesDataTable_test.csv\"\n",
    "data_valid = \"data/smallIcd9NotesDataTable_valid.csv\"\n",
    "vocab = \"src/taggerSystem/data_hw3_delete/vocab.txt\"\n",
    "wordVecs = \"src/taggerSystem/data_hw3_delete/wordVectors.txt\"\n",
    "output_path = 'results/{}/{:%Y%m%d_%H%M%S}/\".format(self.cell, datetime.now())'\n",
    "log_output = output_path + \"log\"\n",
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "NUM = \"NNNUMMM\"\n",
    "UNK = \"UUUNKKK\"\n",
    "EMBED_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Loading training data...\n",
      "INFO:Done. Read 7 notes\n",
      "INFO:Loading dev data...\n",
      "INFO:Done. Read 7 notes\n",
      "INFO:Total read time 0.008938\n",
      "INFO:Built dictionary for 15 features.\n",
      "INFO:There are a total of 4 ICD codes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev raw\n",
      "[(['dog', 'dog', 'dog', 'horse'], ['4240', '486']), (['cat', 'cat', 'horse'], ['7212']), (['horse', 'horse', 'horse', 'Batman'], ['4240', '486', '5845']), (['the', 'the', 'of', 'and'], ['4240']), (['the'], ['4240']), (['and'], ['4240']), (['with'], ['4240'])]\n",
      "dev\n",
      "[([[2, 9], [2, 9], [2, 9], [1, 9]], array([ 0.,  1.,  0.,  1.])), ([[4, 9], [4, 9], [1, 9]], array([ 0.,  0.,  1.,  0.])), ([[1, 9], [1, 9], [1, 9], [8, 10]], array([ 1.,  1.,  0.,  1.])), ([[3, 9], [3, 9], [6, 9], [5, 9]], array([ 0.,  0.,  0.,  1.])), ([[3, 9]], array([ 0.,  0.,  0.,  1.])), ([[5, 9]], array([ 0.,  0.,  0.,  1.])), ([[7, 9]], array([ 0.,  0.,  0.,  1.]))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Initialized embeddings.\n"
     ]
    }
   ],
   "source": [
    "helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid)\n",
    "print('dev raw')\n",
    "print(dev_raw)\n",
    "print('dev')\n",
    "print(dev)\n",
    "embeddings = load_embeddings(vocab, wordVecs, helper)\n",
    "helper.save(output_path)# token2id and max length saved to output_path\n",
    "handler = logging.FileHandler(log_output)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "logging.getLogger().addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def LSTM(x, weight, bias):\n",
    "    cell = rnn_cell.LSTMCell(n_hidden,state_is_tuple = True)\n",
    "    multi_layer_cell = tf.nn.rnn_cell.MultiRNNCell([cell] * 2)\n",
    "    output, state = tf.nn.dynamic_rnn(multi_layer_cell, x, dtype = tf.float32)\n",
    "    output_flattened = tf.reshape(output, [-1, n_hidden])\n",
    "    output_logits = tf.add(tf.matmul(output_flattened,weight),bias)\n",
    "    output_all = tf.nn.sigmoid(output_logits)\n",
    "    output_reshaped = tf.reshape(output_all,[-1,n_steps,n_classes])\n",
    "    output_last = tf.gather(tf.transpose(output_reshaped,[1,0,2]), n_steps - 1)  \n",
    "    #output = tf.transpose(output, [1, 0, 2])\n",
    "    #last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "    #output_last = tf.nn.sigmoid(tf.matmul(last, weight) + bias)\n",
    "    return output_last, output_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    n_word_features = 1 # Number of features for every word in the input.\n",
    "    window_size = 1\n",
    "    n_features = (2 * window_size + 1) * n_word_features # Number of features for every word in the input.\n",
    "    max_length = 120 # longest sequence to parse\n",
    "    n_classes = 5\n",
    "    max_n_labels = 100# max number of labels a note can have. Will be changed later. actually set in data_utils.py\n",
    "    # above is deprecated. Remove soon\n",
    "    dropout = 0.5\n",
    "    embed_size = 50\n",
    "    hidden_size = 300\n",
    "    batch_size = 2\n",
    "    n_epochs = 10\n",
    "    max_grad_norm = 10.\n",
    "    lr = 0.001\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    n_input = 1\n",
    "    n_steps = 10\n",
    "    n_hidden = 64\n",
    "    n_classes = 3\n",
    "\n",
    "    alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "# Initial state of the LSTM memory.\n",
    "state = tf.zeros([batch_size, lstm.state_size])\n",
    "probabilities = []\n",
    "loss = 0.0\n",
    "for current_batch_of_words in words_in_dataset:\n",
    "    # The value of state is updated after processing each batch of words.\n",
    "    output, state = lstm(current_batch_of_words, state)\n",
    "\n",
    "    # The LSTM output can be used to make next word predictions\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    probabilities.append(tf.nn.softmax(logits))\n",
    "    loss += loss_function(probabilities, target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extract Data and Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Loading training data...\n",
      "INFO:Done. Read 18 notes\n",
      "INFO:Loading dev data...\n",
      "INFO:Done. Read 4 notes\n",
      "INFO:Total read time 0.017306\n",
      "INFO:Built dictionary for 7 features.\n",
      "INFO:There are a total of 4 ICD codes\n",
      "INFO:Initialized embeddings.\n"
     ]
    }
   ],
   "source": [
    "helper, train, dev, train_raw, dev_raw = load_and_preprocess_data(data_train = data_train, \n",
    "                                                                  data_valid = data_valid)\n",
    "embeddings = load_embeddings(vocab, wordVecs, helper)\n",
    "embed_size = embeddings.shape[1]\n",
    "helper.save(output_path)# token2id and max length saved to output_path\n",
    "handler = logging.FileHandler(log_output)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "logging.getLogger().addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_examples = self.preprocess_sequence_data(train_examples_raw)\n",
    "dev_set = self.preprocess_sequence_data(dev_set_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['dog', 'dog', 'dog', 'horse'], ['4240', '486']),\n",
       " (['cat', 'cat', 'horse'], ['7212']),\n",
       " (['horse', 'horse', 'horse', 'Batman'], ['4240', '486', '5845']),\n",
       " (['Batman'], ['4240'])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([[2], [2], [2], [1]], array([ 0.,  1.,  1.,  0.])),\n",
       " ([[3], [3], [1]], array([ 0.,  0.,  0.,  1.])),\n",
       " ([[1], [1], [1], [4]], array([ 1.,  1.,  1.,  0.])),\n",
       " ([[4]], array([ 0.,  0.,  1.,  0.]))]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:clinicalNoteTagger]",
   "language": "python",
   "name": "conda-env-clinicalNoteTagger-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
