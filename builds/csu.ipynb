{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the LSTM on your Data\n",
    "\n",
    "This notebook will take you through the steps necessary to train an LSTM to recognize ICD-9 codes, or items from similar dictionaries, from free text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Make sure that the below packages are installed on the server on which this program will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/taggerSystem/')\n",
    "from data_util import load_and_preprocess_data, load_embeddings, ModelHelper, lastTrueWordIdxs\n",
    "import tensorflow as tf\n",
    "from simpleLSTMWithNNetModel import Model, reloadModel\n",
    "from trainModel import trainModel\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pickle\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories\n",
    "\n",
    "Executing the following cell will prompt the user to type in the directories corresponding to the training and validation sets, the vocabulary, and the word vector mappings. Defaults are given in the comments below.\n",
    "\n",
    "Note that the training and test data needs to have one column dedicated to free text (`noteIdx`) and another dedicated to top-level ICD-9 codes (`codeIdx`) associated with each patient. Preferably, the latter should be strung together using '-' as the delimiter (e.g. for patient 1, 1-2-6-4).\n",
    "\n",
    "Please make sure that the parameters such as the embed size, maximum note length, learning rate, number of maximum training epochs, batch size, hidden layer size, number of neural net layers, and probabilities are to your specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put sample file with require file headers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adjust training data headers to match small icd9 training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the path to the training data? data/0815/csu_nodiag_no17_0815_train.csv\n",
      "What is the path to the validation data? data/0815/csu_nodiag_no17_0815_valid.csv\n",
      "What is the path to the vocabulary? data/icd9Vocab.txt\n",
      "What is the path to the vocabulary? data/newgloveicd9.txt\n",
      "Which column contains top-level ICD-9 codes (outputs) in the training and test data? Default is 9. 9\n",
      "Which column contains notes/text (inputs) in the training and test data? Default is 6. 6\n"
     ]
    }
   ],
   "source": [
    "data_train = raw_input('What is the path to the training data? ') #default: data/icd9NotesDataTable_train.csv\n",
    "data_valid = raw_input('What is the path to the validation data? ') #default: data/icd9NotesDataTable_valid.csv\n",
    "vocab = raw_input('What is the path to the vocabulary? ') #default: data/icd9Vocab.txt\n",
    "wordVecs = raw_input('What is the path to the vocabulary? ') #data/newgloveicd9.txt. These are length 300 word vectors from GloVE\n",
    "\n",
    "NUM = \"NNNUMMM\"\n",
    "UNK = \"UUUNKKK\"\n",
    "EMBED_SIZE = 300 # this should correspond to the length of the word vectors\n",
    "maxAllowedNoteLength = 1000\n",
    "max_grad_norm = 5\n",
    "codeIdx = raw_input('Which column contains top-level ICD-9 codes (outputs) in the training and test data? Default is 9. ')\n",
    "textIdx = raw_input('Which column contains notes/text (inputs) in the training and test data? Default is 6. ')\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 256\n",
    "n_hidden = 200\n",
    "output_keep_prob = 0.5\n",
    "input_keep_prob = 1\n",
    "numLayers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# Fixing some issue ashley had\n",
    "\n",
    "textIdx = int(textIdx)\n",
    "codeIdx = int(codeIdx)# I'm not sure how models were trinaed before if this part was broken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, make sure that the output path is specified as you would like. By default, the program saves the output in a folder with the name of your choice within the folder `results`.\n",
    "\n",
    "If there exists a folder with results that you would like to load again, use that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where are models and performances (to be) saved? 0815_csu\n"
     ]
    }
   ],
   "source": [
    "output_path = raw_input('Where are models and performances (to be) saved? ')\n",
    "output_path = os.path.join('results', output_path)\n",
    "if output_path == 'results/':\n",
    "    output_path = 'results/temp'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Executing the following cell will ask whether or not there is a previously saved model; if not, the model will train features from scratch, and if so, the features will be loaded.\n",
    "\n",
    "Note that AZ added \"int() to the codeIdx and textIdx to resolve some errors that were preventing it from initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a previously trained model? [Y/n] n\n",
      "Total number of batches per epoch: 245\n",
      "Maximum note length: 1000\n",
      "Number of ICD-9 codes: 17\n",
      "There are a total of: 17 ICD-9 codes\n",
      "{   'cat:1': 8,\n",
      "    'cat:10': 15,\n",
      "    'cat:11': 14,\n",
      "    'cat:12': 13,\n",
      "    'cat:13': 12,\n",
      "    'cat:14': 11,\n",
      "    'cat:15': 10,\n",
      "    'cat:16': 9,\n",
      "    'cat:18': 16,\n",
      "    'cat:2': 7,\n",
      "    'cat:3': 6,\n",
      "    'cat:4': 5,\n",
      "    'cat:5': 4,\n",
      "    'cat:6': 3,\n",
      "    'cat:7': 2,\n",
      "    'cat:8': 1,\n",
      "    'cat:9': 0}\n",
      "xDev shape: nObs = 26733, nWords = 1000\n",
      "yDev shape: nObs = 26733, nClasses = 17\n",
      "xTrain shape: nObs = 62858, nWords = 1000\n",
      "yTrain shape: nObs = 62858, nClasses = 17\n",
      "Embeddings shape: nWords = 10008, wordVec length = 300\n"
     ]
    }
   ],
   "source": [
    "sizeList = [n_hidden, 150, 75] # these are the weights we will be using\n",
    "\n",
    "def query_yes_no(question, default=\"yes\"):\n",
    "    \"\"\"Ask a yes/no question via raw_input() and return their answer.\n",
    "\n",
    "    \"question\" is a string that is presented to the user.\n",
    "    \"default\" is the presumed answer if the user just hits <Enter>.\n",
    "        It must be \"yes\" (the default), \"no\" or None (meaning\n",
    "        an answer is required of the user).\n",
    "\n",
    "    The \"answer\" return value is True for \"yes\" or False for \"no\".\n",
    "    \"\"\"\n",
    "    valid = {\"yes\": True, \"y\": True, \"ye\": True,\n",
    "             \"no\": False, \"n\": False}\n",
    "    if default is None:\n",
    "        prompt = \" [y/n] \"\n",
    "    elif default == \"yes\":\n",
    "        prompt = \" [Y/n] \"\n",
    "    elif default == \"no\":\n",
    "        prompt = \" [y/N] \"\n",
    "    else:\n",
    "        raise ValueError(\"invalid default answer: '%s'\" % default)\n",
    "\n",
    "    while True:\n",
    "        sys.stdout.write(question + prompt)\n",
    "        choice = raw_input().lower()\n",
    "        if default is not None and choice == '':\n",
    "            return valid[default]\n",
    "        elif choice in valid:\n",
    "            return valid[choice]\n",
    "        else:\n",
    "            sys.stdout.write(\"Please respond with 'yes' or 'no' \"\n",
    "                             \"(or 'y' or 'n').\\n\")\n",
    "            \n",
    "prev_model = query_yes_no(\"Is there a previously trained model?\")\n",
    "\n",
    "if prev_model:\n",
    "    helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = int(codeIdx), textIdx = int(textIdx),\n",
    "    helperLoadPath = output_path)\n",
    "else:\n",
    "    #print codeIdx\n",
    "    #print textIdx\n",
    "    helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = int(codeIdx), textIdx = int(textIdx))\n",
    "    \n",
    "embeddings = load_embeddings(vocab, wordVecs, helper, embeddingSize = EMBED_SIZE)\n",
    "lastTrueWordIdx_train = lastTrueWordIdxs(train)\n",
    "lastTrueWordIdx_dev = lastTrueWordIdxs(dev)\n",
    "helper.save(output_path) # token2id and max length saved to output_path\n",
    "sizeList.append(helper.n_labels)\n",
    "\n",
    "total_batches = (xTrain.shape[0]//batch_size)\n",
    "print('Total number of batches per epoch: %d'%(total_batches))\n",
    "print('Maximum note length: %d'%(helper.max_length))\n",
    "print('Number of ICD-9 codes: %d'%(helper.n_labels))\n",
    "print('There are a total of: {} ICD-9 codes'.format(len(helper.icdDict.keys())))\n",
    "pp.pprint(helper.icdDict)\n",
    "print('xDev shape: nObs = %d, nWords = %d'%(xDev.shape))\n",
    "print('yDev shape: nObs = %d, nClasses = %d'%(yDev.shape))\n",
    "print('xTrain shape: nObs = %d, nWords = %d'%(xTrain.shape))\n",
    "print('yTrain shape: nObs = %d, nClasses = %d'%(yTrain.shape))\n",
    "print('Embeddings shape: nWords = %d, wordVec length = %d'%(embeddings.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell initializes the dictionary of hyperparameters for the model that fully describe the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'EMBED_SIZE': 300,\n",
      "    'batchSize': 256,\n",
      "    'inputKeepProb': 1,\n",
      "    'learningRate': 0.001,\n",
      "    'maxGradNorm': 5,\n",
      "    'maxNoteLength': 1000,\n",
      "    'n_hidden': 200,\n",
      "    'numLayers': 1,\n",
      "    'outputKeepProb': 0.5,\n",
      "    'sizeList': [200, 150, 75, 17],\n",
      "    'trainingEpochsMax': 100}\n"
     ]
    }
   ],
   "source": [
    "hyperParamDict = {'EMBED_SIZE': EMBED_SIZE,\n",
    "                  'maxNoteLength': maxAllowedNoteLength,\n",
    "                  'maxGradNorm': max_grad_norm,\n",
    "                  'outputKeepProb': output_keep_prob,\n",
    "                  'inputKeepProb': input_keep_prob,\n",
    "                  'learningRate': learning_rate,\n",
    "                  'trainingEpochsMax': training_epochs,\n",
    "                  'batchSize': batch_size,\n",
    "                  'n_hidden': n_hidden,\n",
    "                 'numLayers': numLayers,\n",
    "                 'sizeList':sizeList}\n",
    "pp.pprint(hyperParamDict)\n",
    "with open(os.path.join(output_path, 'hyperParamDict.pickle'), 'wb') as handle:\n",
    "    pickle.dump(hyperParamDict, handle, protocol = 2)\n",
    "    #dumping with 2 because ALTUD uses python 2.7 right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, the model is trained (be wary - it will take some time; on an Amazon Deep Learning AMI, it took around an hour to train)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embeddings\n",
      "(?, 1000, 300)\n",
      "<class 'tensorflow.python.ops.rnn_cell_impl.MultiRNNCell'>\n",
      "<tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f4e01245990>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(?, 1000, 300)\n",
      "cell output size\n",
      "200\n",
      "cell state size\n",
      "(LSTMStateTuple(c=200, h=200),)\n",
      "output shape\n",
      "(?, 1000, 200)\n",
      "offset shape\n",
      "(?, 1)\n",
      "output shape new shape\n",
      "(?, 200)\n",
      "flattened indices shape\n",
      "(?, 1)\n",
      "output flattened shape\n",
      "(?, 200)\n",
      "(200, 150)\n",
      "W_1 shape\n",
      "(150,)\n",
      "b_1 shape\n",
      "(150, 75)\n",
      "W_2 shape\n",
      "(75,)\n",
      "bias shape\n",
      "(17,)\n",
      "U shape\n",
      "(75, 17)\n",
      "bias shape\n",
      "(17,)\n",
      "output wx + b\n",
      "(?, 17)\n",
      "(?, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************\n",
      "***************************\n",
      "Running on epoch 0\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.692575 at time 2.115283\n",
      "running iteration 25 with loss 0.346115 at time 33.993040\n",
      "running iteration 50 with loss 0.297302 at time 65.938087\n",
      "running iteration 75 with loss 0.327600 at time 97.870244\n",
      "running iteration 100 with loss 0.323432 at time 129.841913\n",
      "running iteration 125 with loss 0.313994 at time 161.733622\n",
      "running iteration 150 with loss 0.310221 at time 193.522832\n",
      "running iteration 175 with loss 0.325047 at time 225.321969\n",
      "running iteration 200 with loss 0.327015 at time 257.115479\n",
      "running iteration 225 with loss 0.336798 at time 288.898460\n",
      "average training loss 0.334706\n",
      "test loss 0.324836\n",
      "Total run time was 345.876261\n",
      "New best model found. Saving\n",
      "results/0815_csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 1\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.319320 at time 1.287202\n",
      "running iteration 25 with loss 0.295072 at time 33.218528\n",
      "running iteration 50 with loss 0.250613 at time 65.165827\n",
      "running iteration 75 with loss 0.279127 at time 97.078260\n",
      "running iteration 100 with loss 0.275430 at time 128.975762\n",
      "running iteration 125 with loss 0.282002 at time 160.886510\n",
      "running iteration 150 with loss 0.274100 at time 192.735261\n",
      "running iteration 175 with loss 0.280286 at time 224.541448\n",
      "running iteration 200 with loss 0.293736 at time 256.305864\n",
      "running iteration 225 with loss 0.305208 at time 288.096874\n",
      "average training loss 0.288492\n",
      "test loss 0.266618\n",
      "Total run time was 344.812340\n",
      "New best model found. Saving\n",
      "results/0815_csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 2\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.253594 at time 1.275932\n",
      "running iteration 25 with loss 0.254466 at time 33.215874\n",
      "running iteration 50 with loss 0.212062 at time 65.126308\n",
      "running iteration 75 with loss 0.235057 at time 97.042620\n",
      "running iteration 100 with loss 0.240216 at time 128.985790\n",
      "running iteration 125 with loss 0.250188 at time 160.812328\n",
      "running iteration 150 with loss 0.240257 at time 192.563739\n",
      "running iteration 175 with loss 0.244368 at time 224.335706\n",
      "running iteration 200 with loss 0.253382 at time 256.082671\n",
      "running iteration 225 with loss 0.273150 at time 287.848368\n",
      "average training loss 0.251080\n",
      "test loss 0.232261\n",
      "Total run time was 344.694135\n",
      "New best model found. Saving\n",
      "results/0815_csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 3\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.215186 at time 1.280267\n",
      "running iteration 25 with loss 0.210590 at time 33.207351\n",
      "running iteration 50 with loss 0.175229 at time 65.150059\n",
      "running iteration 75 with loss 0.204531 at time 97.071575\n",
      "running iteration 100 with loss 0.206273 at time 128.969524\n",
      "running iteration 125 with loss 0.210305 at time 160.859143\n",
      "running iteration 150 with loss 0.213168 at time 192.601751\n",
      "running iteration 175 with loss 0.214087 at time 224.368770\n",
      "running iteration 200 with loss 0.225227 at time 256.145469\n",
      "running iteration 225 with loss 0.250378 at time 287.948333\n",
      "average training loss 0.219725\n",
      "test loss 0.210313\n",
      "Total run time was 345.026616\n",
      "New best model found. Saving\n",
      "results/0815_csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 4\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.190337 at time 1.286178\n",
      "running iteration 25 with loss 0.191466 at time 33.219315\n",
      "running iteration 50 with loss 0.158016 at time 65.136383\n",
      "running iteration 75 with loss 0.190429 at time 97.033561\n",
      "running iteration 100 with loss 0.188975 at time 128.938851\n",
      "running iteration 125 with loss 0.194781 at time 160.779901\n",
      "running iteration 150 with loss 0.198263 at time 192.517395\n",
      "running iteration 175 with loss 0.187409 at time 224.272768\n",
      "running iteration 200 with loss 0.213178 at time 256.038388\n",
      "running iteration 225 with loss 0.229373 at time 287.818778\n",
      "average training loss 0.199478\n",
      "test loss 0.197297\n",
      "Total run time was 344.582565\n",
      "New best model found. Saving\n",
      "results/0815_csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 5\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.177471 at time 1.273628\n",
      "running iteration 25 with loss 0.177028 at time 33.177432\n",
      "running iteration 50 with loss 0.148385 at time 65.085994\n",
      "running iteration 75 with loss 0.175186 at time 96.879241\n",
      "running iteration 100 with loss 0.170969 at time 128.668664\n",
      "running iteration 125 with loss 0.174158 at time 160.481799\n",
      "running iteration 150 with loss 0.184097 at time 192.277178\n",
      "running iteration 175 with loss 0.177593 at time 224.066112\n",
      "running iteration 200 with loss 0.199535 at time 255.798480\n",
      "running iteration 225 with loss 0.216651 at time 287.449735\n",
      "average training loss 0.185821\n",
      "test loss 0.190428\n",
      "Total run time was 344.101322\n",
      "New best model found. Saving\n",
      "results/0815_csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 6\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.157192 at time 1.286710\n",
      "running iteration 25 with loss 0.162911 at time 32.947379\n",
      "running iteration 50 with loss 0.136713 at time 64.622212\n",
      "running iteration 75 with loss 0.164208 at time 96.370192\n",
      "running iteration 100 with loss 0.163785 at time 128.140825\n",
      "running iteration 125 with loss 0.170466 at time 159.941812\n",
      "running iteration 150 with loss 0.177684 at time 191.738044\n",
      "running iteration 175 with loss 0.166963 at time 223.494431\n",
      "running iteration 200 with loss 0.193552 at time 255.301517\n",
      "running iteration 225 with loss 0.198742 at time 287.103587\n",
      "average training loss 0.174955\n",
      "test loss 0.187640\n",
      "Total run time was 343.794373\n",
      "New best model found. Saving\n",
      "results/0815_csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 7\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.153452 at time 1.283932\n",
      "running iteration 25 with loss 0.162797 at time 33.091533\n",
      "running iteration 50 with loss 0.132946 at time 64.857252\n",
      "running iteration 75 with loss 0.157275 at time 96.636420\n",
      "running iteration 100 with loss 0.156193 at time 128.421720\n",
      "running iteration 125 with loss 0.160908 at time 160.204636\n",
      "running iteration 150 with loss 0.166141 at time 191.977181\n",
      "running iteration 175 with loss 0.159289 at time 223.752369\n",
      "running iteration 200 with loss 0.183224 at time 255.684640\n",
      "running iteration 225 with loss 0.195570 at time 287.648333\n",
      "average training loss 0.165408\n",
      "test loss 0.185128\n",
      "Total run time was 344.434200\n",
      "New best model found. Saving\n",
      "results/0815_csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 8\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.143895 at time 1.286358\n",
      "running iteration 25 with loss 0.153066 at time 33.208250\n",
      "running iteration 50 with loss 0.122072 at time 65.127855\n",
      "running iteration 75 with loss 0.148875 at time 97.026433\n",
      "running iteration 100 with loss 0.145942 at time 128.937182\n",
      "running iteration 125 with loss 0.147907 at time 160.844468\n",
      "running iteration 150 with loss 0.159823 at time 192.786089\n",
      "running iteration 175 with loss 0.148864 at time 224.694780\n",
      "running iteration 200 with loss 0.164864 at time 256.550006\n",
      "running iteration 225 with loss 0.183442 at time 288.307899\n",
      "average training loss 0.156003\n",
      "test loss 0.186472\n",
      "Total run time was 344.853366\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 9\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.137178 at time 1.274876\n",
      "running iteration 25 with loss 0.143130 at time 33.048939\n",
      "running iteration 50 with loss 0.114011 at time 64.978277\n",
      "running iteration 75 with loss 0.142908 at time 96.914068\n",
      "running iteration 100 with loss 0.138552 at time 128.827960\n",
      "running iteration 125 with loss 0.142970 at time 160.731444\n",
      "running iteration 150 with loss 0.148782 at time 192.635909\n",
      "running iteration 175 with loss 0.143399 at time 224.551439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iteration 200 with loss 0.158728 at time 256.466041\n",
      "running iteration 225 with loss 0.173702 at time 288.358574\n",
      "average training loss 0.147600\n",
      "test loss 0.187914\n",
      "Total run time was 345.088169\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 10\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.126595 at time 1.274227\n",
      "running iteration 25 with loss 0.135810 at time 33.024267\n",
      "running iteration 50 with loss 0.105294 at time 64.782590\n",
      "running iteration 75 with loss 0.134040 at time 96.541443\n",
      "running iteration 100 with loss 0.127179 at time 128.302791\n",
      "running iteration 125 with loss 0.138033 at time 160.074024\n",
      "running iteration 150 with loss 0.137685 at time 191.965664\n",
      "running iteration 175 with loss 0.133304 at time 223.777667\n",
      "running iteration 200 with loss 0.149457 at time 255.705356\n",
      "running iteration 225 with loss 0.170732 at time 287.640996\n",
      "average training loss 0.139852\n",
      "test loss 0.193268\n",
      "Total run time was 344.651015\n",
      "validation Loss Increase\n",
      "Stopping early because of increasing validation loss\n"
     ]
    }
   ],
   "source": [
    "from trainModel import trainModel\n",
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "trainModel(helperObj = helper, embeddings = embeddings, hyperParamDict = hyperParamDict, \n",
    "          xDev = xDev, xTrain = xTrain, yDev = yDev, yTrain = yTrain, \n",
    "           lastTrueWordIdx_dev = lastTrueWordIdx_dev, \n",
    "           lastTrueWordIdx_train = lastTrueWordIdx_train,\n",
    "           training_epochs = training_epochs, \n",
    "           output_path = output_path, batchSizeTrain = batch_size,\n",
    "           sizeList = sizeList,\n",
    "           maxIncreasingLossCount = 100, batchSizeDev = 1500, chatty = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the session closed. You should be able to see your results in the `output_path` directory you specified earlier.\n",
    "\n",
    "To evaluate the results and generate plots and such, please check out `predictionEvaluation.ipynb` in the same repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xDev[xDev == -1] = 0\n",
    "# xTrain[xTrain == -1] = 0\n",
    "# trueWordIdxs = tf.placeholder(tf.int32, shape = (None,1))\n",
    "# with tf.Session() as session:\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     modelDict = reloadModel(session = session,\n",
    "#                             saverCheckPointPath = output_path,\n",
    "#                             saverMetaPath = output_path + '/bestModel.meta')\n",
    "#     print('here we go')\n",
    "#     for i in range(3):\n",
    "#         pred_y = session.run(modelDict['y_last'],feed_dict={modelDict['xPlaceHolder']: xDev,\n",
    "#                                       modelDict['trueWordIdxs']:lastTrueWordIdx_dev,\n",
    "#                                       modelDict['outputKeepProb']: 1.0,\n",
    "#                                       modelDict['inputKeepProb']: 1.0}, ) \n",
    "#         validLoss = tf.nn.sigmoid_cross_entropy_with_logits(logits = pred_y, \n",
    "#                                              labels = tf.cast(yDev, tf.float32))\n",
    "#         validLoss = tf.reduce_mean(validLoss)\n",
    "#         validLoss = validLoss.eval()\n",
    "#         print('test loss %f'%(validLoss))\n",
    "#         print('***********************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
