{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the LSTM on your Data\n",
    "\n",
    "This notebook will take you through the steps necessary to train an LSTM to recognize ICD-9 codes, or items from similar dictionaries, from free text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Make sure that the below packages are installed on the server on which this program will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/taggerSystem/')\n",
    "from data_util import load_and_preprocess_data, load_embeddings, ModelHelper, lastTrueWordIdxs\n",
    "import tensorflow as tf\n",
    "from simpleLSTMWithNNetModel import Model, reloadModel\n",
    "from trainModel import trainModel\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pickle\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories\n",
    "\n",
    "Executing the following cell will prompt the user to type in the directories corresponding to the training and validation sets, the vocabulary, and the word vector mappings. Defaults are given in the comments below.\n",
    "\n",
    "Note that the training and test data needs to have one column dedicated to free text (`noteIdx`) and another dedicated to top-level ICD-9 codes (`codeIdx`) associated with each patient. Preferably, the latter should be strung together using '-' as the delimiter (e.g. for patient 1, 1-2-6-4).\n",
    "\n",
    "Please make sure that the parameters such as the embed size, maximum note length, learning rate, number of maximum training epochs, batch size, hidden layer size, number of neural net layers, and probabilities are to your specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put sample file with require file headers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adjust training data headers to match small icd9 training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the path to the training data? data/0815/mimic+csu_0122_train.csv\n",
      "What is the path to the validation data? data/0815/mimic+csu_0122_valid.csv\n",
      "What is the path to the vocabulary? data/icd9Vocab.txt\n",
      "What is the path to the vocabulary? data/newgloveicd9.txt\n",
      "Which column contains top-level ICD-9 codes (outputs) in the training and test data? Default is 9. 9\n",
      "Which column contains notes/text (inputs) in the training and test data? Default is 6. 6\n"
     ]
    }
   ],
   "source": [
    "data_train = raw_input('What is the path to the training data? ') #default: data/icd9NotesDataTable_train.csv\n",
    "data_valid = raw_input('What is the path to the validation data? ') #default: data/icd9NotesDataTable_valid.csv\n",
    "vocab = raw_input('What is the path to the vocabulary? ') #default: data/icd9Vocab.txt\n",
    "wordVecs = raw_input('What is the path to the vocabulary? ') #data/newgloveicd9.txt. These are length 300 word vectors from GloVE\n",
    "\n",
    "NUM = \"NNNUMMM\"\n",
    "UNK = \"UUUNKKK\"\n",
    "EMBED_SIZE = 300 # this should correspond to the length of the word vectors\n",
    "maxAllowedNoteLength = 1000\n",
    "max_grad_norm = 5\n",
    "codeIdx = raw_input('Which column contains top-level ICD-9 codes (outputs) in the training and test data? Default is 9. ')\n",
    "textIdx = raw_input('Which column contains notes/text (inputs) in the training and test data? Default is 6. ')\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 256\n",
    "n_hidden = 200\n",
    "output_keep_prob = 0.5\n",
    "input_keep_prob = 1\n",
    "numLayers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# Fixing some issue ashley had\n",
    "\n",
    "textIdx = int(textIdx)\n",
    "codeIdx = int(codeIdx)# I'm not sure how models were trinaed before if this part was broken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, make sure that the output path is specified as you would like. By default, the program saves the output in a folder with the name of your choice within the folder `results`.\n",
    "\n",
    "If there exists a folder with results that you would like to load again, use that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where are models and performances (to be) saved? 0122_mimic+csu\n"
     ]
    }
   ],
   "source": [
    "output_path = raw_input('Where are models and performances (to be) saved? ')\n",
    "output_path = os.path.join('results', output_path)\n",
    "if output_path == 'results/':\n",
    "    output_path = 'results/temp'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Executing the following cell will ask whether or not there is a previously saved model; if not, the model will train features from scratch, and if so, the features will be loaded.\n",
    "\n",
    "Note that AZ added \"int() to the codeIdx and textIdx to resolve some errors that were preventing it from initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a previously trained model? [Y/n] n\n",
      "Total number of batches per epoch: 399\n",
      "Maximum note length: 1000\n",
      "Number of ICD-9 codes: 19\n",
      "There are a total of: 19 ICD-9 codes\n",
      "{   'cat:1': 8,\n",
      "    'cat:10': 16,\n",
      "    'cat:11': 15,\n",
      "    'cat:12': 14,\n",
      "    'cat:13': 13,\n",
      "    'cat:14': 12,\n",
      "    'cat:15': 11,\n",
      "    'cat:16': 10,\n",
      "    'cat:17': 9,\n",
      "    'cat:18': 18,\n",
      "    'cat:19': 17,\n",
      "    'cat:2': 7,\n",
      "    'cat:3': 6,\n",
      "    'cat:4': 5,\n",
      "    'cat:5': 4,\n",
      "    'cat:6': 3,\n",
      "    'cat:7': 2,\n",
      "    'cat:8': 1,\n",
      "    'cat:9': 0}\n",
      "xDev shape: nObs = 39914, nWords = 1000\n",
      "yDev shape: nObs = 39914, nClasses = 19\n",
      "xTrain shape: nObs = 102399, nWords = 1000\n",
      "yTrain shape: nObs = 102399, nClasses = 19\n",
      "Embeddings shape: nWords = 10008, wordVec length = 300\n"
     ]
    }
   ],
   "source": [
    "sizeList = [n_hidden, 150, 75] # these are the weights we will be using\n",
    "\n",
    "def query_yes_no(question, default=\"yes\"):\n",
    "    \"\"\"Ask a yes/no question via raw_input() and return their answer.\n",
    "\n",
    "    \"question\" is a string that is presented to the user.\n",
    "    \"default\" is the presumed answer if the user just hits <Enter>.\n",
    "        It must be \"yes\" (the default), \"no\" or None (meaning\n",
    "        an answer is required of the user).\n",
    "\n",
    "    The \"answer\" return value is True for \"yes\" or False for \"no\".\n",
    "    \"\"\"\n",
    "    valid = {\"yes\": True, \"y\": True, \"ye\": True,\n",
    "             \"no\": False, \"n\": False}\n",
    "    if default is None:\n",
    "        prompt = \" [y/n] \"\n",
    "    elif default == \"yes\":\n",
    "        prompt = \" [Y/n] \"\n",
    "    elif default == \"no\":\n",
    "        prompt = \" [y/N] \"\n",
    "    else:\n",
    "        raise ValueError(\"invalid default answer: '%s'\" % default)\n",
    "\n",
    "    while True:\n",
    "        sys.stdout.write(question + prompt)\n",
    "        choice = raw_input().lower()\n",
    "        if default is not None and choice == '':\n",
    "            return valid[default]\n",
    "        elif choice in valid:\n",
    "            return valid[choice]\n",
    "        else:\n",
    "            sys.stdout.write(\"Please respond with 'yes' or 'no' \"\n",
    "                             \"(or 'y' or 'n').\\n\")\n",
    "            \n",
    "prev_model = query_yes_no(\"Is there a previously trained model?\")\n",
    "\n",
    "if prev_model:\n",
    "    helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = int(codeIdx), textIdx = int(textIdx),\n",
    "    helperLoadPath = output_path)\n",
    "else:\n",
    "    #print codeIdx\n",
    "    #print textIdx\n",
    "    helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = int(codeIdx), textIdx = int(textIdx))\n",
    "    \n",
    "embeddings = load_embeddings(vocab, wordVecs, helper, embeddingSize = EMBED_SIZE)\n",
    "lastTrueWordIdx_train = lastTrueWordIdxs(train)\n",
    "lastTrueWordIdx_dev = lastTrueWordIdxs(dev)\n",
    "helper.save(output_path) # token2id and max length saved to output_path\n",
    "sizeList.append(helper.n_labels)\n",
    "\n",
    "total_batches = (xTrain.shape[0]//batch_size)\n",
    "print('Total number of batches per epoch: %d'%(total_batches))\n",
    "print('Maximum note length: %d'%(helper.max_length))\n",
    "print('Number of ICD-9 codes: %d'%(helper.n_labels))\n",
    "print('There are a total of: {} ICD-9 codes'.format(len(helper.icdDict.keys())))\n",
    "pp.pprint(helper.icdDict)\n",
    "print('xDev shape: nObs = %d, nWords = %d'%(xDev.shape))\n",
    "print('yDev shape: nObs = %d, nClasses = %d'%(yDev.shape))\n",
    "print('xTrain shape: nObs = %d, nWords = %d'%(xTrain.shape))\n",
    "print('yTrain shape: nObs = %d, nClasses = %d'%(yTrain.shape))\n",
    "print('Embeddings shape: nWords = %d, wordVec length = %d'%(embeddings.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell initializes the dictionary of hyperparameters for the model that fully describe the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'EMBED_SIZE': 300,\n",
      "    'batchSize': 256,\n",
      "    'inputKeepProb': 1,\n",
      "    'learningRate': 0.001,\n",
      "    'maxGradNorm': 5,\n",
      "    'maxNoteLength': 1000,\n",
      "    'n_hidden': 200,\n",
      "    'numLayers': 1,\n",
      "    'outputKeepProb': 0.5,\n",
      "    'sizeList': [200, 150, 75, 19],\n",
      "    'trainingEpochsMax': 100}\n"
     ]
    }
   ],
   "source": [
    "hyperParamDict = {'EMBED_SIZE': EMBED_SIZE,\n",
    "                  'maxNoteLength': maxAllowedNoteLength,\n",
    "                  'maxGradNorm': max_grad_norm,\n",
    "                  'outputKeepProb': output_keep_prob,\n",
    "                  'inputKeepProb': input_keep_prob,\n",
    "                  'learningRate': learning_rate,\n",
    "                  'trainingEpochsMax': training_epochs,\n",
    "                  'batchSize': batch_size,\n",
    "                  'n_hidden': n_hidden,\n",
    "                 'numLayers': numLayers,\n",
    "                 'sizeList':sizeList}\n",
    "pp.pprint(hyperParamDict)\n",
    "with open(os.path.join(output_path, 'hyperParamDict.pickle'), 'wb') as handle:\n",
    "    pickle.dump(hyperParamDict, handle, protocol = 2)\n",
    "    #dumping with 2 because ALTUD uses python 2.7 right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, the model is trained (be wary - it will take some time; on an Amazon Deep Learning AMI, it took around an hour to train)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embeddings\n",
      "(?, 1000, 300)\n",
      "<class 'tensorflow.python.ops.rnn_cell_impl.MultiRNNCell'>\n",
      "<tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f3459345b50>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(?, 1000, 300)\n",
      "cell output size\n",
      "200\n",
      "cell state size\n",
      "(LSTMStateTuple(c=200, h=200),)\n",
      "output shape\n",
      "(?, 1000, 200)\n",
      "offset shape\n",
      "(?, 1)\n",
      "output shape new shape\n",
      "(?, 200)\n",
      "flattened indices shape\n",
      "(?, 1)\n",
      "output flattened shape\n",
      "(?, 200)\n",
      "(200, 150)\n",
      "W_1 shape\n",
      "(150,)\n",
      "b_1 shape\n",
      "(150, 75)\n",
      "W_2 shape\n",
      "(75,)\n",
      "bias shape\n",
      "(19,)\n",
      "U shape\n",
      "(75, 19)\n",
      "bias shape\n",
      "(19,)\n",
      "output wx + b\n",
      "(?, 19)\n",
      "(?, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************\n",
      "***************************\n",
      "Running on epoch 0\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.694111 at time 1.913710\n",
      "running iteration 25 with loss 0.505345 at time 33.513476\n",
      "running iteration 50 with loss 0.491403 at time 65.317354\n",
      "running iteration 75 with loss 0.485652 at time 97.167353\n",
      "running iteration 100 with loss 0.476429 at time 129.056188\n",
      "running iteration 125 with loss 0.457441 at time 160.957767\n",
      "running iteration 150 with loss 0.451077 at time 192.748714\n",
      "running iteration 175 with loss 0.318106 at time 224.493858\n",
      "running iteration 200 with loss 0.282233 at time 256.227998\n",
      "running iteration 225 with loss 0.289715 at time 287.980604\n",
      "running iteration 250 with loss 0.291630 at time 319.673097\n",
      "running iteration 275 with loss 0.282804 at time 351.405420\n",
      "running iteration 300 with loss 0.301155 at time 383.161734\n",
      "running iteration 325 with loss 0.292791 at time 414.923249\n",
      "running iteration 350 with loss 0.310582 at time 446.697190\n",
      "running iteration 375 with loss 0.318224 at time 478.470086\n",
      "average training loss 0.375033\n",
      "test loss 0.530258\n",
      "Total run time was 556.499244\n",
      "New best model found. Saving\n",
      "results/0122_mimic+csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 1\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.994668 at time 1.281587\n",
      "running iteration 25 with loss 0.469549 at time 33.128062\n",
      "running iteration 50 with loss 0.509499 at time 64.913141\n",
      "running iteration 75 with loss 0.465492 at time 96.657437\n",
      "running iteration 100 with loss 0.456286 at time 128.387975\n",
      "running iteration 125 with loss 0.447172 at time 160.105911\n",
      "running iteration 150 with loss 0.450867 at time 191.849779\n",
      "running iteration 175 with loss 0.301265 at time 223.577014\n",
      "running iteration 200 with loss 0.261232 at time 255.309576\n",
      "running iteration 225 with loss 0.267213 at time 286.963485\n",
      "running iteration 250 with loss 0.269982 at time 318.611013\n",
      "running iteration 275 with loss 0.262565 at time 350.288048\n",
      "running iteration 300 with loss 0.280271 at time 382.003539\n",
      "running iteration 325 with loss 0.264749 at time 413.765690\n",
      "running iteration 350 with loss 0.280329 at time 445.540956\n",
      "running iteration 375 with loss 0.296106 at time 477.316590\n",
      "average training loss 0.356156\n",
      "test loss 0.477419\n",
      "Total run time was 555.581518\n",
      "New best model found. Saving\n",
      "results/0122_mimic+csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 2\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.939176 at time 1.277310\n",
      "running iteration 25 with loss 0.464787 at time 33.020747\n",
      "running iteration 50 with loss 0.451937 at time 64.818747\n",
      "running iteration 75 with loss 0.444841 at time 96.640592\n",
      "running iteration 100 with loss 0.452806 at time 128.466360\n",
      "running iteration 125 with loss 0.446241 at time 160.311810\n",
      "running iteration 150 with loss 0.440709 at time 192.153528\n",
      "running iteration 175 with loss 0.281155 at time 223.908413\n",
      "running iteration 200 with loss 0.236373 at time 255.616958\n",
      "running iteration 225 with loss 0.244651 at time 287.319491\n",
      "running iteration 250 with loss 0.247725 at time 319.056947\n",
      "running iteration 275 with loss 0.232784 at time 350.727220\n",
      "running iteration 300 with loss 0.254994 at time 382.386124\n",
      "running iteration 325 with loss 0.239857 at time 414.035309\n",
      "running iteration 350 with loss 0.258097 at time 445.731668\n",
      "running iteration 375 with loss 0.284170 at time 477.504768\n",
      "average training loss 0.337689\n",
      "test loss 0.421331\n",
      "Total run time was 554.794010\n",
      "New best model found. Saving\n",
      "results/0122_mimic+csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 3\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.789977 at time 1.278843\n",
      "running iteration 25 with loss 0.451909 at time 33.027936\n",
      "running iteration 50 with loss 0.447849 at time 64.746581\n",
      "running iteration 75 with loss 0.439906 at time 96.455513\n",
      "running iteration 100 with loss 0.440795 at time 128.147633\n",
      "running iteration 125 with loss 0.433283 at time 159.822737\n",
      "running iteration 150 with loss 0.434542 at time 191.498467\n",
      "running iteration 175 with loss 0.256951 at time 223.125290\n",
      "running iteration 200 with loss 0.217890 at time 254.762477\n",
      "running iteration 225 with loss 0.221793 at time 286.392385\n",
      "running iteration 250 with loss 0.226344 at time 318.021539\n",
      "running iteration 275 with loss 0.208528 at time 349.653788\n",
      "running iteration 300 with loss 0.238004 at time 381.273990\n",
      "running iteration 325 with loss 0.222546 at time 412.900112\n",
      "running iteration 350 with loss 0.236189 at time 444.508761\n",
      "running iteration 375 with loss 0.259358 at time 476.130254\n",
      "average training loss 0.319508\n",
      "test loss 0.380131\n",
      "Total run time was 552.699789\n",
      "New best model found. Saving\n",
      "results/0122_mimic+csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 4\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.713827 at time 1.275652\n",
      "running iteration 25 with loss 0.440609 at time 32.932841\n",
      "running iteration 50 with loss 0.438134 at time 64.586589\n",
      "running iteration 75 with loss 0.430135 at time 96.231501\n",
      "running iteration 100 with loss 0.433067 at time 127.882355\n",
      "running iteration 125 with loss 0.426724 at time 159.539463\n",
      "running iteration 150 with loss 0.429673 at time 191.209161\n",
      "running iteration 175 with loss 0.233258 at time 222.836022\n",
      "running iteration 200 with loss 0.196173 at time 254.458669\n",
      "running iteration 225 with loss 0.192831 at time 286.082963\n",
      "running iteration 250 with loss 0.206189 at time 317.704934\n",
      "running iteration 275 with loss 0.189252 at time 349.336055\n",
      "running iteration 300 with loss 0.214531 at time 380.936315\n",
      "running iteration 325 with loss 0.206290 at time 412.536857\n",
      "running iteration 350 with loss 0.222993 at time 444.133250\n",
      "running iteration 375 with loss 0.239369 at time 475.743087\n",
      "average training loss 0.301648\n",
      "test loss 0.361726\n",
      "Total run time was 552.619878\n",
      "New best model found. Saving\n",
      "results/0122_mimic+csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 5\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.656506 at time 1.277446\n",
      "running iteration 25 with loss 0.429765 at time 33.024682\n",
      "running iteration 50 with loss 0.428585 at time 64.806315\n",
      "running iteration 75 with loss 0.426768 at time 96.602178\n",
      "running iteration 100 with loss 0.424667 at time 128.415899\n",
      "running iteration 125 with loss 0.411173 at time 160.234982\n",
      "running iteration 150 with loss 0.417297 at time 192.059239\n",
      "running iteration 175 with loss 0.208566 at time 223.858317\n",
      "running iteration 200 with loss 0.172938 at time 255.573901\n",
      "running iteration 225 with loss 0.177742 at time 287.260459\n",
      "running iteration 250 with loss 0.190226 at time 318.926963\n",
      "running iteration 275 with loss 0.172321 at time 350.589656\n",
      "running iteration 300 with loss 0.198154 at time 382.235106\n",
      "running iteration 325 with loss 0.183044 at time 413.856595\n",
      "running iteration 350 with loss 0.207731 at time 445.475708\n",
      "running iteration 375 with loss 0.226551 at time 477.097298\n",
      "average training loss 0.285629\n",
      "test loss 0.336222\n",
      "Total run time was 553.874365\n",
      "New best model found. Saving\n",
      "results/0122_mimic+csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 6\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.585100 at time 1.274805\n",
      "running iteration 25 with loss 0.419327 at time 32.946099\n",
      "running iteration 50 with loss 0.418005 at time 64.627204\n",
      "running iteration 75 with loss 0.413341 at time 96.301844\n",
      "running iteration 100 with loss 0.408344 at time 127.981120\n",
      "running iteration 125 with loss 0.389752 at time 159.663148\n",
      "running iteration 150 with loss 0.400570 at time 191.354886\n",
      "running iteration 175 with loss 0.184339 at time 222.994848\n",
      "running iteration 200 with loss 0.158077 at time 254.631276\n",
      "running iteration 225 with loss 0.166110 at time 286.255827\n",
      "running iteration 250 with loss 0.169090 at time 317.890435\n",
      "running iteration 275 with loss 0.157909 at time 349.523219\n",
      "running iteration 300 with loss 0.182259 at time 381.155883\n",
      "running iteration 325 with loss 0.168990 at time 412.778272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iteration 350 with loss 0.191551 at time 444.409511\n",
      "running iteration 375 with loss 0.209118 at time 476.037630\n",
      "average training loss 0.269061\n",
      "test loss 0.301671\n",
      "Total run time was 552.818587\n",
      "New best model found. Saving\n",
      "results/0122_mimic+csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 7\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.489747 at time 1.279828\n",
      "running iteration 25 with loss 0.397990 at time 32.985200\n",
      "running iteration 50 with loss 0.402021 at time 64.671233\n",
      "running iteration 75 with loss 0.395285 at time 96.351255\n",
      "running iteration 100 with loss 0.391791 at time 128.035086\n",
      "running iteration 125 with loss 0.379887 at time 159.711882\n",
      "running iteration 150 with loss 0.382480 at time 191.381336\n",
      "running iteration 175 with loss 0.173306 at time 223.025837\n",
      "running iteration 200 with loss 0.146771 at time 254.657842\n",
      "running iteration 225 with loss 0.152559 at time 286.276891\n",
      "running iteration 250 with loss 0.161462 at time 317.907160\n",
      "running iteration 275 with loss 0.151040 at time 349.517256\n",
      "running iteration 300 with loss 0.172686 at time 381.128708\n",
      "running iteration 325 with loss 0.160012 at time 412.755412\n",
      "running iteration 350 with loss 0.179103 at time 444.400042\n",
      "running iteration 375 with loss 0.201119 at time 476.039441\n",
      "average training loss 0.255494\n",
      "test loss 0.287429\n",
      "Total run time was 552.807918\n",
      "New best model found. Saving\n",
      "results/0122_mimic+csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 8\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.453633 at time 1.278767\n",
      "running iteration 25 with loss 0.386266 at time 32.947726\n",
      "running iteration 50 with loss 0.383779 at time 64.623343\n",
      "running iteration 75 with loss 0.382592 at time 96.288898\n",
      "running iteration 100 with loss 0.379322 at time 127.972391\n",
      "running iteration 125 with loss 0.369597 at time 159.650563\n",
      "running iteration 150 with loss 0.381250 at time 191.324378\n",
      "running iteration 175 with loss 0.158503 at time 222.967952\n",
      "running iteration 200 with loss 0.141025 at time 254.578748\n",
      "running iteration 225 with loss 0.144814 at time 286.223907\n",
      "running iteration 250 with loss 0.149582 at time 317.873190\n",
      "running iteration 275 with loss 0.150423 at time 349.509610\n",
      "running iteration 300 with loss 0.163441 at time 381.134467\n",
      "running iteration 325 with loss 0.153157 at time 412.743755\n",
      "running iteration 350 with loss 0.173243 at time 444.357298\n",
      "running iteration 375 with loss 0.194109 at time 475.985408\n",
      "average training loss 0.245016\n",
      "test loss 0.276702\n",
      "Total run time was 552.789132\n",
      "New best model found. Saving\n",
      "results/0122_mimic+csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 9\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.430624 at time 1.274776\n",
      "running iteration 25 with loss 0.374618 at time 32.972259\n",
      "running iteration 50 with loss 0.372757 at time 64.641134\n",
      "running iteration 75 with loss 0.369392 at time 96.327284\n",
      "running iteration 100 with loss 0.365594 at time 128.006363\n",
      "running iteration 125 with loss 0.357440 at time 159.706497\n",
      "running iteration 150 with loss 0.363500 at time 191.394890\n",
      "running iteration 175 with loss 0.150567 at time 223.031416\n",
      "running iteration 200 with loss 0.129116 at time 254.666526\n",
      "running iteration 225 with loss 0.143754 at time 286.280560\n",
      "running iteration 250 with loss 0.141789 at time 317.896349\n",
      "running iteration 275 with loss 0.136464 at time 349.518920\n",
      "running iteration 300 with loss 0.155498 at time 381.146203\n",
      "running iteration 325 with loss 0.144583 at time 412.787856\n",
      "running iteration 350 with loss 0.163548 at time 444.421955\n",
      "running iteration 375 with loss 0.184210 at time 476.045526\n",
      "average training loss 0.235583\n",
      "test loss 0.271154\n",
      "Total run time was 552.846774\n",
      "New best model found. Saving\n",
      "results/0122_mimic+csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 10\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.407006 at time 1.277839\n",
      "running iteration 25 with loss 0.365711 at time 32.971637\n",
      "running iteration 50 with loss 0.364478 at time 64.651238\n",
      "running iteration 75 with loss 0.360381 at time 96.331091\n",
      "running iteration 100 with loss 0.357068 at time 127.999162\n",
      "running iteration 125 with loss 0.345509 at time 159.670302\n",
      "running iteration 150 with loss 0.351992 at time 191.351600\n",
      "running iteration 175 with loss 0.144847 at time 222.985880\n",
      "running iteration 200 with loss 0.127007 at time 254.627862\n",
      "running iteration 225 with loss 0.130333 at time 286.254965\n",
      "running iteration 250 with loss 0.139008 at time 317.893161\n",
      "running iteration 275 with loss 0.132816 at time 349.555182\n",
      "running iteration 300 with loss 0.147231 at time 381.199681\n",
      "running iteration 325 with loss 0.142370 at time 412.852245\n",
      "running iteration 350 with loss 0.159110 at time 444.505943\n",
      "running iteration 375 with loss 0.180832 at time 476.164394\n",
      "average training loss 0.227250\n",
      "test loss 0.268625\n",
      "Total run time was 552.983834\n",
      "New best model found. Saving\n",
      "results/0122_mimic+csu\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 11\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.394202 at time 1.276108\n",
      "running iteration 25 with loss 0.354614 at time 32.975525\n",
      "running iteration 50 with loss 0.360491 at time 64.662732\n",
      "running iteration 75 with loss 0.347154 at time 96.358652\n",
      "running iteration 100 with loss 0.346731 at time 128.078082\n",
      "running iteration 125 with loss 0.332482 at time 159.839731\n",
      "running iteration 150 with loss 0.344252 at time 191.623884\n",
      "running iteration 175 with loss 0.140506 at time 223.401023\n",
      "running iteration 200 with loss 0.119164 at time 255.186837\n",
      "running iteration 225 with loss 0.124459 at time 286.982350\n",
      "running iteration 250 with loss 0.133225 at time 318.785710\n",
      "running iteration 275 with loss 0.128911 at time 350.583875\n",
      "running iteration 300 with loss 0.140443 at time 382.401721\n",
      "running iteration 325 with loss 0.136332 at time 414.198539\n",
      "running iteration 350 with loss 0.146917 at time 445.902472\n",
      "running iteration 375 with loss 0.171563 at time 477.591530\n",
      "average training loss 0.220015\n",
      "test loss 0.271000\n",
      "Total run time was 554.590288\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 12\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.384663 at time 1.267886\n",
      "running iteration 25 with loss 0.347975 at time 32.971046\n",
      "running iteration 50 with loss 0.349453 at time 64.678386\n",
      "running iteration 75 with loss 0.341101 at time 96.372516\n",
      "running iteration 100 with loss 0.344533 at time 128.055045\n",
      "running iteration 125 with loss 0.330154 at time 159.738814\n",
      "running iteration 150 with loss 0.338665 at time 191.441235\n",
      "running iteration 175 with loss 0.132063 at time 223.093393\n",
      "running iteration 200 with loss 0.116276 at time 254.722379\n",
      "running iteration 225 with loss 0.119418 at time 286.368821\n",
      "running iteration 250 with loss 0.125756 at time 317.989307\n",
      "running iteration 275 with loss 0.123532 at time 349.625578\n",
      "running iteration 300 with loss 0.136809 at time 381.261695\n",
      "running iteration 325 with loss 0.132350 at time 412.915710\n",
      "running iteration 350 with loss 0.139991 at time 444.574206\n",
      "running iteration 375 with loss 0.156664 at time 476.228617\n",
      "average training loss 0.213677\n",
      "test loss 0.273303\n",
      "Total run time was 553.089527\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 13\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.376531 at time 1.270273\n",
      "running iteration 25 with loss 0.333413 at time 32.988204\n",
      "running iteration 50 with loss 0.339940 at time 64.706607\n",
      "running iteration 75 with loss 0.334885 at time 96.410940\n",
      "running iteration 100 with loss 0.335748 at time 128.134542\n",
      "running iteration 125 with loss 0.322902 at time 159.844633\n",
      "running iteration 150 with loss 0.332016 at time 191.534132\n",
      "running iteration 175 with loss 0.123419 at time 223.182533\n",
      "running iteration 200 with loss 0.113967 at time 254.827530\n",
      "running iteration 225 with loss 0.119208 at time 286.475038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iteration 250 with loss 0.120099 at time 318.126578\n",
      "running iteration 275 with loss 0.117606 at time 349.783245\n",
      "running iteration 300 with loss 0.131287 at time 381.429474\n",
      "running iteration 325 with loss 0.127612 at time 413.079893\n",
      "running iteration 350 with loss 0.138505 at time 444.721063\n",
      "running iteration 375 with loss 0.157592 at time 476.361741\n",
      "average training loss 0.207494\n",
      "test loss 0.275520\n",
      "Total run time was 553.210641\n",
      "validation Loss Increase\n",
      "Stopping early because of increasing validation loss\n"
     ]
    }
   ],
   "source": [
    "from trainModel import trainModel\n",
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "trainModel(helperObj = helper, embeddings = embeddings, hyperParamDict = hyperParamDict, \n",
    "          xDev = xDev, xTrain = xTrain, yDev = yDev, yTrain = yTrain, \n",
    "           lastTrueWordIdx_dev = lastTrueWordIdx_dev, \n",
    "           lastTrueWordIdx_train = lastTrueWordIdx_train,\n",
    "           training_epochs = training_epochs, \n",
    "           output_path = output_path, batchSizeTrain = batch_size,\n",
    "           sizeList = sizeList,\n",
    "           maxIncreasingLossCount = 100, batchSizeDev = 1500, chatty = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the session closed. You should be able to see your results in the `output_path` directory you specified earlier.\n",
    "\n",
    "To evaluate the results and generate plots and such, please check out `predictionEvaluation.ipynb` in the same repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR USE IN LOADING OLD MODELS AND REEVALUATING\n",
    "# xDev[xDev == -1] = 0\n",
    "# xTrain[xTrain == -1] = 0\n",
    "# trueWordIdxs = tf.placeholder(tf.int32, shape = (None,1))\n",
    "# with tf.Session() as session:\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     modelDict = reloadModel(session = session,\n",
    "#                             saverCheckPointPath = output_path,\n",
    "#                             saverMetaPath = output_path + '/bestModel.meta')\n",
    "#     print('here we go')\n",
    "#     for i in range(3):\n",
    "#         pred_y = session.run(modelDict['y_last'],feed_dict={modelDict['xPlaceHolder']: xDev,\n",
    "#                                       modelDict['trueWordIdxs']:lastTrueWordIdx_dev,\n",
    "#                                       modelDict['outputKeepProb']: 1.0,\n",
    "#                                       modelDict['inputKeepProb']: 1.0}, ) \n",
    "#         validLoss = tf.nn.sigmoid_cross_entropy_with_logits(logits = pred_y, \n",
    "#                                              labels = tf.cast(yDev, tf.float32))\n",
    "#         validLoss = tf.reduce_mean(validLoss)\n",
    "#         validLoss = validLoss.eval()\n",
    "#         print('test loss %f'%(validLoss))\n",
    "#         print('***********************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
