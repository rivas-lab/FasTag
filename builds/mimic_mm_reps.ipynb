{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the LSTM on your Data\n",
    "\n",
    "This notebook will take you through the steps necessary to train an LSTM to recognize ICD-9 codes, or items from similar dictionaries, from free text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Make sure that the below packages are installed on the server on which this program will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/taggerSystem/')\n",
    "from data_util import load_and_preprocess_data, load_embeddings, ModelHelper, lastTrueWordIdxs\n",
    "import tensorflow as tf\n",
    "from simpleLSTMWithNNetModel import Model, reloadModel\n",
    "from trainModel import trainModel\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pickle\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories\n",
    "\n",
    "Executing the following cell will prompt the user to type in the directories corresponding to the training and validation sets, the vocabulary, and the word vector mappings. Defaults are given in the comments below.\n",
    "\n",
    "Note that the training and test data needs to have one column dedicated to free text (`noteIdx`) and another dedicated to top-level ICD-9 codes (`codeIdx`) associated with each patient. Preferably, the latter should be strung together using '-' as the delimiter (e.g. for patient 1, 1-2-6-4).\n",
    "\n",
    "Please make sure that the parameters such as the embed size, maximum note length, learning rate, number of maximum training epochs, batch size, hidden layer size, number of neural net layers, and probabilities are to your specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put sample file with require file headers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adjust training data headers to match small icd9 training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the path to the training data? data/0815/mimic_mm_reps_0815_train.csv\n",
      "What is the path to the validation data? data/0815/mimic_mm_reps_0815_valid.csv\n",
      "What is the path to the vocabulary? data/icd9Vocab.txt\n",
      "What is the path to the vocabulary? data/newgloveicd9.txt\n",
      "Which column contains top-level ICD-9 codes (outputs) in the training and test data? Default is 9. 9\n",
      "Which column contains notes/text (inputs) in the training and test data? Default is 6. 6\n"
     ]
    }
   ],
   "source": [
    "data_train = raw_input('What is the path to the training data? ') #default: data/icd9NotesDataTable_train.csv\n",
    "data_valid = raw_input('What is the path to the validation data? ') #default: data/icd9NotesDataTable_valid.csv\n",
    "vocab = raw_input('What is the path to the vocabulary? ') #default: data/icd9Vocab.txt\n",
    "wordVecs = raw_input('What is the path to the vocabulary? ') #data/newgloveicd9.txt. These are length 300 word vectors from GloVE\n",
    "\n",
    "NUM = \"NNNUMMM\"\n",
    "UNK = \"UUUNKKK\"\n",
    "EMBED_SIZE = 300 # this should correspond to the length of the word vectors\n",
    "maxAllowedNoteLength = 1000\n",
    "max_grad_norm = 5\n",
    "codeIdx = raw_input('Which column contains top-level ICD-9 codes (outputs) in the training and test data? Default is 9. ')\n",
    "textIdx = raw_input('Which column contains notes/text (inputs) in the training and test data? Default is 6. ')\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 256\n",
    "n_hidden = 200\n",
    "output_keep_prob = 0.5\n",
    "input_keep_prob = 1\n",
    "numLayers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# Fixing some issue ashley had\n",
    "\n",
    "textIdx = int(textIdx)\n",
    "codeIdx = int(codeIdx)# I'm not sure how models were trinaed before if this part was broken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, make sure that the output path is specified as you would like. By default, the program saves the output in a folder with the name of your choice within the folder `results`.\n",
    "\n",
    "If there exists a folder with results that you would like to load again, use that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where are models and performances (to be) saved? 0815_mimic_mm_reps\n"
     ]
    }
   ],
   "source": [
    "output_path = raw_input('Where are models and performances (to be) saved? ')\n",
    "output_path = os.path.join('results', output_path)\n",
    "if output_path == 'results/':\n",
    "    output_path = 'results/temp'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Executing the following cell will ask whether or not there is a previously saved model; if not, the model will train features from scratch, and if so, the features will be loaded.\n",
    "\n",
    "Note that AZ added \"int() to the codeIdx and textIdx to resolve some errors that were preventing it from initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a previously trained model? [Y/n] n\n",
      "Total number of batches per epoch: 154\n",
      "Maximum note length: 1000\n",
      "Number of ICD-9 codes: 19\n",
      "There are a total of: 19 ICD-9 codes\n",
      "{   'cat:1': 8,\n",
      "    'cat:10': 16,\n",
      "    'cat:11': 15,\n",
      "    'cat:12': 14,\n",
      "    'cat:13': 13,\n",
      "    'cat:14': 12,\n",
      "    'cat:15': 11,\n",
      "    'cat:16': 10,\n",
      "    'cat:17': 9,\n",
      "    'cat:18': 18,\n",
      "    'cat:19': 17,\n",
      "    'cat:2': 7,\n",
      "    'cat:3': 6,\n",
      "    'cat:4': 5,\n",
      "    'cat:5': 4,\n",
      "    'cat:6': 3,\n",
      "    'cat:7': 2,\n",
      "    'cat:8': 1,\n",
      "    'cat:9': 0}\n",
      "xDev shape: nObs = 13181, nWords = 1000\n",
      "yDev shape: nObs = 13181, nClasses = 19\n",
      "xTrain shape: nObs = 39541, nWords = 1000\n",
      "yTrain shape: nObs = 39541, nClasses = 19\n",
      "Embeddings shape: nWords = 10008, wordVec length = 300\n"
     ]
    }
   ],
   "source": [
    "sizeList = [n_hidden, 150, 75] # these are the weights we will be using\n",
    "\n",
    "def query_yes_no(question, default=\"yes\"):\n",
    "    \"\"\"Ask a yes/no question via raw_input() and return their answer.\n",
    "\n",
    "    \"question\" is a string that is presented to the user.\n",
    "    \"default\" is the presumed answer if the user just hits <Enter>.\n",
    "        It must be \"yes\" (the default), \"no\" or None (meaning\n",
    "        an answer is required of the user).\n",
    "\n",
    "    The \"answer\" return value is True for \"yes\" or False for \"no\".\n",
    "    \"\"\"\n",
    "    valid = {\"yes\": True, \"y\": True, \"ye\": True,\n",
    "             \"no\": False, \"n\": False}\n",
    "    if default is None:\n",
    "        prompt = \" [y/n] \"\n",
    "    elif default == \"yes\":\n",
    "        prompt = \" [Y/n] \"\n",
    "    elif default == \"no\":\n",
    "        prompt = \" [y/N] \"\n",
    "    else:\n",
    "        raise ValueError(\"invalid default answer: '%s'\" % default)\n",
    "\n",
    "    while True:\n",
    "        sys.stdout.write(question + prompt)\n",
    "        choice = raw_input().lower()\n",
    "        if default is not None and choice == '':\n",
    "            return valid[default]\n",
    "        elif choice in valid:\n",
    "            return valid[choice]\n",
    "        else:\n",
    "            sys.stdout.write(\"Please respond with 'yes' or 'no' \"\n",
    "                             \"(or 'y' or 'n').\\n\")\n",
    "            \n",
    "prev_model = query_yes_no(\"Is there a previously trained model?\")\n",
    "\n",
    "if prev_model:\n",
    "    helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = int(codeIdx), textIdx = int(textIdx),\n",
    "    helperLoadPath = output_path)\n",
    "else:\n",
    "    #print codeIdx\n",
    "    #print textIdx\n",
    "    helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = int(codeIdx), textIdx = int(textIdx))\n",
    "    \n",
    "embeddings = load_embeddings(vocab, wordVecs, helper, embeddingSize = EMBED_SIZE)\n",
    "lastTrueWordIdx_train = lastTrueWordIdxs(train)\n",
    "lastTrueWordIdx_dev = lastTrueWordIdxs(dev)\n",
    "helper.save(output_path) # token2id and max length saved to output_path\n",
    "sizeList.append(helper.n_labels)\n",
    "\n",
    "total_batches = (xTrain.shape[0]//batch_size)\n",
    "print('Total number of batches per epoch: %d'%(total_batches))\n",
    "print('Maximum note length: %d'%(helper.max_length))\n",
    "print('Number of ICD-9 codes: %d'%(helper.n_labels))\n",
    "print('There are a total of: {} ICD-9 codes'.format(len(helper.icdDict.keys())))\n",
    "pp.pprint(helper.icdDict)\n",
    "print('xDev shape: nObs = %d, nWords = %d'%(xDev.shape))\n",
    "print('yDev shape: nObs = %d, nClasses = %d'%(yDev.shape))\n",
    "print('xTrain shape: nObs = %d, nWords = %d'%(xTrain.shape))\n",
    "print('yTrain shape: nObs = %d, nClasses = %d'%(yTrain.shape))\n",
    "print('Embeddings shape: nWords = %d, wordVec length = %d'%(embeddings.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell initializes the dictionary of hyperparameters for the model that fully describe the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'EMBED_SIZE': 300,\n",
      "    'batchSize': 256,\n",
      "    'inputKeepProb': 1,\n",
      "    'learningRate': 0.001,\n",
      "    'maxGradNorm': 5,\n",
      "    'maxNoteLength': 1000,\n",
      "    'n_hidden': 200,\n",
      "    'numLayers': 1,\n",
      "    'outputKeepProb': 0.5,\n",
      "    'sizeList': [200, 150, 75, 19],\n",
      "    'trainingEpochsMax': 100}\n"
     ]
    }
   ],
   "source": [
    "hyperParamDict = {'EMBED_SIZE': EMBED_SIZE,\n",
    "                  'maxNoteLength': maxAllowedNoteLength,\n",
    "                  'maxGradNorm': max_grad_norm,\n",
    "                  'outputKeepProb': output_keep_prob,\n",
    "                  'inputKeepProb': input_keep_prob,\n",
    "                  'learningRate': learning_rate,\n",
    "                  'trainingEpochsMax': training_epochs,\n",
    "                  'batchSize': batch_size,\n",
    "                  'n_hidden': n_hidden,\n",
    "                 'numLayers': numLayers,\n",
    "                 'sizeList':sizeList}\n",
    "pp.pprint(hyperParamDict)\n",
    "with open(os.path.join(output_path, 'hyperParamDict.pickle'), 'wb') as handle:\n",
    "    pickle.dump(hyperParamDict, handle, protocol = 2)\n",
    "    #dumping with 2 because ALTUD uses python 2.7 right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, the model is trained (be wary - it will take some time; on an Amazon Deep Learning AMI, it took around an hour to train)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embeddings\n",
      "(?, 1000, 300)\n",
      "<class 'tensorflow.python.ops.rnn_cell_impl.MultiRNNCell'>\n",
      "<tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f3f7753a810>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(?, 1000, 300)\n",
      "cell output size\n",
      "200\n",
      "cell state size\n",
      "(LSTMStateTuple(c=200, h=200),)\n",
      "output shape\n",
      "(?, 1000, 200)\n",
      "offset shape\n",
      "(?, 1)\n",
      "output shape new shape\n",
      "(?, 200)\n",
      "flattened indices shape\n",
      "(?, 1)\n",
      "output flattened shape\n",
      "(?, 200)\n",
      "(200, 150)\n",
      "W_1 shape\n",
      "(150,)\n",
      "b_1 shape\n",
      "(150, 75)\n",
      "W_2 shape\n",
      "(75,)\n",
      "bias shape\n",
      "(19,)\n",
      "U shape\n",
      "(75, 19)\n",
      "bias shape\n",
      "(19,)\n",
      "output wx + b\n",
      "(?, 19)\n",
      "(?, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************\n",
      "***************************\n",
      "Running on epoch 0\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.707734 at time 2.115112\n",
      "running iteration 25 with loss 0.488445 at time 35.934637\n",
      "running iteration 50 with loss 0.479368 at time 70.016128\n",
      "running iteration 75 with loss 0.455916 at time 103.899081\n",
      "running iteration 100 with loss 0.455509 at time 138.100619\n",
      "running iteration 125 with loss 0.446371 at time 172.197541\n",
      "running iteration 150 with loss 0.468054 at time 206.468645\n",
      "average training loss 0.479196\n",
      "test loss 0.488370\n",
      "Total run time was 227.569332\n",
      "New best model found. Saving\n",
      "results/0815_mimic_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 1\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.436597 at time 1.400265\n",
      "running iteration 25 with loss 0.449872 at time 35.604523\n",
      "running iteration 50 with loss 0.445269 at time 69.560781\n",
      "running iteration 75 with loss 0.433582 at time 103.696980\n",
      "running iteration 100 with loss 0.444717 at time 138.274160\n",
      "running iteration 125 with loss 0.435328 at time 172.906416\n",
      "running iteration 150 with loss 0.432526 at time 207.412054\n",
      "average training loss 0.443032\n",
      "test loss 0.470925\n",
      "Total run time was 228.514947\n",
      "New best model found. Saving\n",
      "results/0815_mimic_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 2\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.419263 at time 1.359610\n",
      "running iteration 25 with loss 0.435427 at time 35.406171\n",
      "running iteration 50 with loss 0.438059 at time 69.641962\n",
      "running iteration 75 with loss 0.431111 at time 104.019355\n",
      "running iteration 100 with loss 0.429253 at time 138.022756\n",
      "running iteration 125 with loss 0.418968 at time 171.973264\n",
      "running iteration 150 with loss 0.423887 at time 206.121024\n",
      "average training loss 0.431092\n",
      "test loss 0.459817\n",
      "Total run time was 227.095013\n",
      "New best model found. Saving\n",
      "results/0815_mimic_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 3\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.406968 at time 1.365801\n",
      "running iteration 25 with loss 0.421208 at time 35.378989\n",
      "running iteration 50 with loss 0.421034 at time 69.367137\n",
      "running iteration 75 with loss 0.418100 at time 103.345057\n",
      "running iteration 100 with loss 0.416122 at time 137.448918\n",
      "running iteration 125 with loss 0.404671 at time 171.510188\n",
      "running iteration 150 with loss 0.418767 at time 205.466607\n",
      "average training loss 0.418097\n",
      "test loss 0.452036\n",
      "Total run time was 226.405335\n",
      "New best model found. Saving\n",
      "results/0815_mimic_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 4\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.401904 at time 1.358953\n",
      "running iteration 25 with loss 0.544999 at time 35.566713\n",
      "running iteration 50 with loss 0.475290 at time 69.776357\n",
      "running iteration 75 with loss 0.445673 at time 104.008173\n",
      "running iteration 100 with loss 0.430758 at time 138.032077\n",
      "running iteration 125 with loss 0.421038 at time 172.043772\n",
      "running iteration 150 with loss 0.421196 at time 206.079083\n",
      "average training loss 0.442463\n",
      "test loss 0.455777\n",
      "Total run time was 227.061592\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 5\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.403370 at time 1.396417\n",
      "running iteration 25 with loss 0.425030 at time 35.406598\n",
      "running iteration 50 with loss 0.415555 at time 69.578142\n",
      "running iteration 75 with loss 0.411303 at time 103.841446\n",
      "running iteration 100 with loss 0.412707 at time 138.087486\n",
      "running iteration 125 with loss 0.401147 at time 172.340785\n",
      "running iteration 150 with loss 0.410266 at time 206.649262\n",
      "average training loss 0.412778\n",
      "test loss 0.447912\n",
      "Total run time was 227.616804\n",
      "New best model found. Saving\n",
      "results/0815_mimic_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 6\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.390536 at time 1.370371\n",
      "running iteration 25 with loss 0.409736 at time 35.435935\n",
      "running iteration 50 with loss 0.408746 at time 69.408532\n",
      "running iteration 75 with loss 0.408533 at time 103.372168\n",
      "running iteration 100 with loss 0.402880 at time 137.469003\n",
      "running iteration 125 with loss 0.393840 at time 171.743335\n",
      "running iteration 150 with loss 0.402895 at time 206.207976\n",
      "average training loss 0.403727\n",
      "test loss 0.442207\n",
      "Total run time was 227.206503\n",
      "New best model found. Saving\n",
      "results/0815_mimic_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 7\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.385630 at time 1.377146\n",
      "running iteration 25 with loss 0.404509 at time 35.608737\n",
      "running iteration 50 with loss 0.403095 at time 69.893852\n",
      "running iteration 75 with loss 0.401758 at time 104.029841\n",
      "running iteration 100 with loss 0.393737 at time 138.563663\n",
      "running iteration 125 with loss 0.384026 at time 173.146307\n",
      "running iteration 150 with loss 0.390475 at time 207.670128\n",
      "average training loss 0.395874\n",
      "test loss 0.429018\n",
      "Total run time was 228.808657\n",
      "New best model found. Saving\n",
      "results/0815_mimic_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 8\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.376578 at time 1.398905\n",
      "running iteration 25 with loss 0.391085 at time 35.950791\n",
      "running iteration 50 with loss 0.395732 at time 69.948138\n",
      "running iteration 75 with loss 0.382859 at time 103.895405\n",
      "running iteration 100 with loss 0.384958 at time 138.149920\n",
      "running iteration 125 with loss 0.371314 at time 172.544946\n",
      "running iteration 150 with loss 0.377306 at time 206.843176\n",
      "average training loss 0.382782\n",
      "test loss 0.416035\n",
      "Total run time was 227.904147\n",
      "New best model found. Saving\n",
      "results/0815_mimic_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 9\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.366386 at time 1.386532\n",
      "running iteration 25 with loss 0.378417 at time 35.160997\n",
      "running iteration 50 with loss 0.379302 at time 68.856940\n",
      "running iteration 75 with loss 0.371931 at time 102.535825\n",
      "running iteration 100 with loss 0.373702 at time 136.149709\n",
      "running iteration 125 with loss 0.359201 at time 169.743834\n",
      "running iteration 150 with loss 0.367392 at time 203.437418\n",
      "average training loss 0.370673\n",
      "test loss 0.407409\n",
      "Total run time was 224.309561\n",
      "New best model found. Saving\n",
      "results/0815_mimic_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 10\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.354369 at time 1.352039\n",
      "running iteration 25 with loss 0.371321 at time 34.980997\n",
      "running iteration 50 with loss 0.375546 at time 68.554573\n",
      "running iteration 75 with loss 0.360638 at time 102.313223\n",
      "running iteration 100 with loss 0.364313 at time 136.018840\n",
      "running iteration 125 with loss 0.347209 at time 169.627571\n",
      "running iteration 150 with loss 0.357780 at time 203.229738\n",
      "average training loss 0.360507\n",
      "test loss 0.401932\n",
      "Total run time was 224.120840\n",
      "New best model found. Saving\n",
      "results/0815_mimic_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 11\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.349216 at time 1.363091\n",
      "running iteration 25 with loss 0.360361 at time 35.021273\n",
      "running iteration 50 with loss 0.359502 at time 68.641185\n",
      "running iteration 75 with loss 0.355191 at time 102.308009\n",
      "running iteration 100 with loss 0.357174 at time 135.991624\n",
      "running iteration 125 with loss 0.337578 at time 169.620740\n",
      "running iteration 150 with loss 0.354001 at time 203.187449\n",
      "average training loss 0.351154\n",
      "test loss 0.396015\n",
      "Total run time was 224.097160\n",
      "New best model found. Saving\n",
      "results/0815_mimic_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 12\n",
      "***************************\n",
      "***************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iteration 0 with loss 0.338629 at time 1.358627\n",
      "running iteration 25 with loss 0.350645 at time 34.997101\n",
      "running iteration 50 with loss 0.350206 at time 68.649082\n",
      "running iteration 75 with loss 0.343127 at time 102.346651\n",
      "running iteration 100 with loss 0.350705 at time 135.902506\n",
      "running iteration 125 with loss 0.327114 at time 169.561318\n",
      "running iteration 150 with loss 0.343306 at time 203.317839\n",
      "average training loss 0.341758\n",
      "test loss 0.389804\n",
      "Total run time was 224.208327\n",
      "New best model found. Saving\n",
      "results/0815_mimic_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 13\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.329150 at time 1.358828\n",
      "running iteration 25 with loss 0.343595 at time 35.128139\n",
      "running iteration 50 with loss 0.343572 at time 68.853099\n",
      "running iteration 75 with loss 0.338827 at time 102.446007\n",
      "running iteration 100 with loss 0.338297 at time 136.025607\n",
      "running iteration 125 with loss 0.317195 at time 169.629037\n",
      "running iteration 150 with loss 0.334587 at time 203.279243\n",
      "average training loss 0.332069\n",
      "test loss 0.389131\n",
      "Total run time was 224.186963\n",
      "New best model found. Saving\n",
      "results/0815_mimic_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 14\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.322178 at time 1.365318\n",
      "running iteration 25 with loss 0.336297 at time 34.864374\n",
      "running iteration 50 with loss 0.330557 at time 68.412462\n",
      "running iteration 75 with loss 0.329825 at time 101.928038\n",
      "running iteration 100 with loss 0.330461 at time 135.487639\n",
      "running iteration 125 with loss 0.308996 at time 169.132219\n",
      "running iteration 150 with loss 0.332307 at time 202.649675\n",
      "average training loss 0.324066\n",
      "test loss 0.388406\n",
      "Total run time was 223.527332\n",
      "New best model found. Saving\n",
      "results/0815_mimic_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 15\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.315962 at time 1.378097\n",
      "running iteration 25 with loss 0.323762 at time 34.910988\n",
      "running iteration 50 with loss 0.323995 at time 68.476672\n",
      "running iteration 75 with loss 0.320565 at time 102.002281\n",
      "running iteration 100 with loss 0.322143 at time 135.575700\n",
      "running iteration 125 with loss 0.300522 at time 169.134935\n",
      "running iteration 150 with loss 0.322831 at time 202.602238\n",
      "average training loss 0.316616\n",
      "test loss 0.393166\n",
      "Total run time was 223.411703\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 16\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.307703 at time 1.342680\n",
      "running iteration 25 with loss 0.317932 at time 35.057480\n",
      "running iteration 50 with loss 0.321554 at time 68.712310\n",
      "running iteration 75 with loss 0.309730 at time 102.306424\n",
      "running iteration 100 with loss 0.315965 at time 135.861535\n",
      "running iteration 125 with loss 0.299253 at time 169.414197\n",
      "running iteration 150 with loss 0.313126 at time 202.863009\n",
      "average training loss 0.309534\n",
      "test loss 0.390712\n",
      "Total run time was 223.615891\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 17\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.297451 at time 1.328028\n",
      "running iteration 25 with loss 0.314527 at time 34.858102\n",
      "running iteration 50 with loss 0.316491 at time 68.453825\n",
      "running iteration 75 with loss 0.317106 at time 101.912228\n",
      "running iteration 100 with loss 0.311417 at time 135.354103\n",
      "running iteration 125 with loss 0.294310 at time 168.763036\n",
      "running iteration 150 with loss 0.304045 at time 202.272441\n",
      "average training loss 0.302091\n",
      "test loss 0.393632\n",
      "Total run time was 223.150730\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 18\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.287131 at time 1.354972\n",
      "running iteration 25 with loss 0.305860 at time 34.954603\n",
      "running iteration 50 with loss 0.307508 at time 68.438813\n",
      "running iteration 75 with loss 0.299079 at time 101.994336\n",
      "running iteration 100 with loss 0.297363 at time 135.571052\n",
      "running iteration 125 with loss 0.284306 at time 169.055343\n",
      "running iteration 150 with loss 0.294072 at time 202.592096\n",
      "average training loss 0.293470\n",
      "test loss 0.399228\n",
      "Total run time was 223.404883\n",
      "validation Loss Increase\n",
      "Stopping early because of increasing validation loss\n"
     ]
    }
   ],
   "source": [
    "from trainModel import trainModel\n",
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "trainModel(helperObj = helper, embeddings = embeddings, hyperParamDict = hyperParamDict, \n",
    "          xDev = xDev, xTrain = xTrain, yDev = yDev, yTrain = yTrain, \n",
    "           lastTrueWordIdx_dev = lastTrueWordIdx_dev, \n",
    "           lastTrueWordIdx_train = lastTrueWordIdx_train,\n",
    "           training_epochs = training_epochs, \n",
    "           output_path = output_path, batchSizeTrain = batch_size,\n",
    "           sizeList = sizeList,\n",
    "           maxIncreasingLossCount = 100, batchSizeDev = 1500, chatty = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the session closed. You should be able to see your results in the `output_path` directory you specified earlier.\n",
    "\n",
    "To evaluate the results and generate plots and such, please check out `predictionEvaluation.ipynb` in the same repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xDev[xDev == -1] = 0\n",
    "# xTrain[xTrain == -1] = 0\n",
    "# trueWordIdxs = tf.placeholder(tf.int32, shape = (None,1))\n",
    "# with tf.Session() as session:\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     modelDict = reloadModel(session = session,\n",
    "#                             saverCheckPointPath = output_path,\n",
    "#                             saverMetaPath = output_path + '/bestModel.meta')\n",
    "#     print('here we go')\n",
    "#     for i in range(3):\n",
    "#         pred_y = session.run(modelDict['y_last'],feed_dict={modelDict['xPlaceHolder']: xDev,\n",
    "#                                       modelDict['trueWordIdxs']:lastTrueWordIdx_dev,\n",
    "#                                       modelDict['outputKeepProb']: 1.0,\n",
    "#                                       modelDict['inputKeepProb']: 1.0}, ) \n",
    "#         validLoss = tf.nn.sigmoid_cross_entropy_with_logits(logits = pred_y, \n",
    "#                                              labels = tf.cast(yDev, tf.float32))\n",
    "#         validLoss = tf.reduce_mean(validLoss)\n",
    "#         validLoss = validLoss.eval()\n",
    "#         print('test loss %f'%(validLoss))\n",
    "#         print('***********************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
