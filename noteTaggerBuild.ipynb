{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/taggerSystem/')\n",
    "from data_util import load_and_preprocess_data, load_embeddings, ModelHelper, lastTrueWordIdxs\n",
    "from simpleLSTMWithNNetModel import Model, reloadModel\n",
    "from trainModel import trainModel\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pickle\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train = \"data/icd9NotesDataTable_train.csv\"\n",
    "data_valid = \"data/icd9NotesDataTable_valid.csv\"\n",
    "# data_train = \"data/smallIcd9NotesDataTable_train.csv\"\n",
    "# data_valid = \"data/smallIcd9NotesDataTable_valid.csv\"\n",
    "# vocab = \"src/taggerSystem/data_hw3_delete/vocab.txt\"\n",
    "# wordVecs = \"src/taggerSystem/data_hw3_delete/wordVectors.txt\"\n",
    "vocab = 'data/icd9Vocab.txt'# vocab for our data\n",
    "wordVecs = 'data/newgloveicd9.txt'# len 300 word vectors\n",
    "# output_path = 'results/model_description'\n",
    "# log_output = output_path + \"log\"\n",
    "# vocab = 'data/icd9Vocab.txt'# vocab for our data\n",
    "# wordVecs = 'data/newgloveicd9.txt'# len 300 word vectors\n",
    "# START_TOKEN = \"<s>\"\n",
    "# END_TOKEN = \"</s>\"\n",
    "NUM = \"NNNUMMM\"\n",
    "UNK = \"UUUNKKK\"\n",
    "EMBED_SIZE = 300 # this should not be manually set and should instead come from the word\n",
    "# vectors. Or maybe it's good that we have to set it so we know for sure the size\n",
    "# of word vecs we're using\n",
    "maxAllowedNoteLength = 1000\n",
    "max_grad_norm = 5\n",
    "codeIdx = 9\n",
    "textIdx = 6\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 256\n",
    "n_input = 1\n",
    "# n_steps = 10\n",
    "n_hidden = 200\n",
    "numNNetLayers = 4\n",
    "# n_classes = helper.n_labels\n",
    "output_keep_prob = 0.5\n",
    "input_keep_prob = 1# not sure why there are two but example used 0.5 for output. Check with TA\n",
    "numLayers = 1\n",
    "# embeddingSize = embeddings.shape[1]\n",
    "# print('Embedding size is %d'%(embeddingSize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where should models and performances be saved to?3LayerNNet_longRun\n"
     ]
    }
   ],
   "source": [
    "output_path = input(prompt = 'Where should models and performances be saved to?')\n",
    "output_path = os.path.join('results', output_path)\n",
    "if output_path == 'results/':\n",
    "    output_path = 'results/temp'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/3LayerNNet_longRun\n"
     ]
    }
   ],
   "source": [
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliverbdw4/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((39541, 1000), -1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/home/oliverbdw4/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((39541, 19), -1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/home/oliverbdw4/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((13181, 1000), -1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/home/oliverbdw4/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((13181, 19), -1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches per epoch 154\n",
      "Max note length 1000\n",
      "Number of Icd9 codes 19\n",
      "There are a total of 19 icd9 codes\n",
      "{   'cat:1': 5,\n",
      "    'cat:10': 10,\n",
      "    'cat:11': 3,\n",
      "    'cat:12': 17,\n",
      "    'cat:13': 16,\n",
      "    'cat:14': 13,\n",
      "    'cat:15': 4,\n",
      "    'cat:16': 9,\n",
      "    'cat:17': 8,\n",
      "    'cat:18': 18,\n",
      "    'cat:19': 2,\n",
      "    'cat:2': 15,\n",
      "    'cat:3': 1,\n",
      "    'cat:4': 12,\n",
      "    'cat:5': 7,\n",
      "    'cat:6': 11,\n",
      "    'cat:7': 6,\n",
      "    'cat:8': 0,\n",
      "    'cat:9': 14}\n",
      "icd9 present\n",
      "xDev shape: nObs = 13181, nWords = 1000\n",
      "yDev shape: nObs = 13181, nClasses = 19\n",
      "xTrain shape: nObs = 39541, nWords = 1000\n",
      "yTrain shape: nObs = 39541, nClasses = 19\n",
      "Embeddings shape: nWords = 10008, wordVec len = 300\n"
     ]
    }
   ],
   "source": [
    "sizeList = [n_hidden, 150, 75] # these are the weights we wil be using\n",
    "helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = codeIdx, textIdx = textIdx,\n",
    "    helperLoadPath = output_path)\n",
    "# helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "#     data_train = data_train, data_valid = data_valid, \n",
    "#     maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "#     codeIdx = codeIdx, textIdx = textIdx)\n",
    "embeddings = load_embeddings(vocab, wordVecs, helper, embeddingSize = EMBED_SIZE)\n",
    "lastTrueWordIdx_train = lastTrueWordIdxs(train)\n",
    "lastTrueWordIdx_dev = lastTrueWordIdxs(dev)\n",
    "helper.save(output_path)# token2id and max length saved to output_path\n",
    "sizeList.append(helper.n_labels)\n",
    "# sizeList.(numNNetLayers,helper.n_labels)\n",
    "# np.savetxt(os.path.join(output_path, 'yDev.gz'),yDev)\n",
    "# #np.save(os.path.join(modelRunOutputPath, 'xDev.npy'), xDev)\n",
    "# np.savetxt(os.path.join(output_path, 'devTrueIdxs.gz'), lastTrueWordIdx_dev)\n",
    "\n",
    "total_batches = (xTrain.shape[0]//batch_size)\n",
    "print('Total number of batches per epoch %d'%(total_batches))\n",
    "print('Max note length %d'%(helper.max_length))\n",
    "print('Number of Icd9 codes %d'%(helper.n_labels))\n",
    "print('There are a total of {} icd9 codes'.format(len(helper.icdDict.keys())))\n",
    "pp.pprint(helper.icdDict)\n",
    "print('icd9 present')\n",
    "print('xDev shape: nObs = %d, nWords = %d'%(xDev.shape))\n",
    "print('yDev shape: nObs = %d, nClasses = %d'%(yDev.shape))\n",
    "print('xTrain shape: nObs = %d, nWords = %d'%(xTrain.shape))\n",
    "print('yTrain shape: nObs = %d, nClasses = %d'%(yTrain.shape))\n",
    "print('Embeddings shape: nWords = %d, wordVec len = %d'%(embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numNNetLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max note length 1000\n",
      "Number of Icd9 codes 19\n",
      "There are a total of 19 icd9 codes\n",
      "{   'cat:1': 5,\n",
      "    'cat:10': 10,\n",
      "    'cat:11': 3,\n",
      "    'cat:12': 17,\n",
      "    'cat:13': 16,\n",
      "    'cat:14': 13,\n",
      "    'cat:15': 4,\n",
      "    'cat:16': 9,\n",
      "    'cat:17': 8,\n",
      "    'cat:18': 18,\n",
      "    'cat:19': 2,\n",
      "    'cat:2': 15,\n",
      "    'cat:3': 1,\n",
      "    'cat:4': 12,\n",
      "    'cat:5': 7,\n",
      "    'cat:6': 11,\n",
      "    'cat:7': 6,\n",
      "    'cat:8': 0,\n",
      "    'cat:9': 14}\n",
      "icd9 present\n",
      "xDev shape: nObs = 13181, nWords = 1000\n",
      "yDev shape: nObs = 13181, nClasses = 19\n",
      "xTrain shape: nObs = 39541, nWords = 1000\n",
      "yTrain shape: nObs = 39541, nClasses = 19\n",
      "Embeddings shape: nWords = 10008, wordVec len = 300\n"
     ]
    }
   ],
   "source": [
    "print('Max note length %d'%(helper.max_length))\n",
    "print('Number of Icd9 codes %d'%(helper.n_labels))\n",
    "print('There are a total of {} icd9 codes'.format(len(helper.icdDict.keys())))\n",
    "pp.pprint(helper.icdDict)\n",
    "print('icd9 present')\n",
    "print('xDev shape: nObs = %d, nWords = %d'%(xDev.shape))\n",
    "print('yDev shape: nObs = %d, nClasses = %d'%(yDev.shape))\n",
    "print('xTrain shape: nObs = %d, nWords = %d'%(xTrain.shape))\n",
    "print('yTrain shape: nObs = %d, nClasses = %d'%(yTrain.shape))\n",
    "print('Embeddings shape: nWords = %d, wordVec len = %d'%(embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200, 150, 75, 19]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'EMBED_SIZE': 300,\n",
      "    'batchSize': 256,\n",
      "    'inputKeepProb': 1,\n",
      "    'learningRate': 0.001,\n",
      "    'maxGradNorm': 5,\n",
      "    'maxNoteLength': 1000,\n",
      "    'n_hidden': 200,\n",
      "    'numLayers': 1,\n",
      "    'outputKeepProb': 0.5,\n",
      "    'sizeList': [200, 150, 75, 19],\n",
      "    'trainingEpochsMax': 100}\n"
     ]
    }
   ],
   "source": [
    "hyperParamDict = {'EMBED_SIZE': EMBED_SIZE,\n",
    "                  'maxNoteLength': maxAllowedNoteLength,\n",
    "                  'maxGradNorm': max_grad_norm,\n",
    "                  'outputKeepProb': output_keep_prob,\n",
    "                  'inputKeepProb': input_keep_prob,\n",
    "                  'learningRate': learning_rate,\n",
    "                  'trainingEpochsMax': training_epochs,\n",
    "                  'batchSize': batch_size,\n",
    "                  'n_hidden': n_hidden,\n",
    "                 'numLayers': numLayers,\n",
    "                 'sizeList':sizeList}\n",
    "pp.pprint(hyperParamDict)\n",
    "with open(os.path.join(output_path, 'hyperParamDict.pickle'), 'wb') as handle:\n",
    "    pickle.dump(hyperParamDict, handle, protocol = 2)\n",
    "    # dumping with 2 because ALTUD uses python 2.7 right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embeddings\n",
      "(?, 1000, 300)\n",
      "<class 'tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell'>\n",
      "<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object at 0x7fead5b68128>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(?, 1000, 300)\n",
      "cell output size\n",
      "200\n",
      "cell state size\n",
      "(LSTMStateTuple(c=200, h=200),)\n",
      "output shape\n",
      "(?, 1000, 200)\n",
      "offset shape\n",
      "(?, 1)\n",
      "output shape new shape\n",
      "(?, 200)\n",
      "flattened indices shape\n",
      "(?, 1)\n",
      "output flattened shape\n",
      "(?, 200)\n",
      "(200, 150)\n",
      "W_1 shape\n",
      "(150,)\n",
      "b_1 shape\n",
      "(150, 75)\n",
      "W_2 shape\n",
      "(75,)\n",
      "bias shape\n",
      "(19,)\n",
      "U shape\n",
      "(75, 19)\n",
      "bias shape\n",
      "(19,)\n",
      "output wx + b\n",
      "(?, 19)\n",
      "(?, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliverbdw4/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************\n",
      "***************************\n",
      "Running on epoch 0\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.692090 at time 1.140060\n",
      "running iteration 25 with loss 0.494451 at time 21.777998\n",
      "running iteration 50 with loss 0.479277 at time 42.134598\n",
      "running iteration 75 with loss 0.498533 at time 62.550422\n",
      "running iteration 100 with loss 0.474625 at time 83.140333\n",
      "running iteration 125 with loss 0.470814 at time 103.349360\n",
      "running iteration 150 with loss 0.462965 at time 123.756083\n",
      "average training loss 0.494690\n",
      "test loss 0.489485\n",
      "Total run time was 139.324978\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 1\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.473876 at time 0.822015\n",
      "running iteration 25 with loss 0.444436 at time 21.519092\n",
      "running iteration 50 with loss 0.452394 at time 42.013957\n",
      "running iteration 75 with loss 0.460045 at time 62.736555\n",
      "running iteration 100 with loss 0.445693 at time 82.998989\n",
      "running iteration 125 with loss 0.439205 at time 103.443180\n",
      "running iteration 150 with loss 0.440611 at time 124.081609\n",
      "average training loss 0.452438\n",
      "test loss 0.482164\n",
      "Total run time was 139.502498\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 2\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.449607 at time 0.843868\n",
      "running iteration 25 with loss 0.441182 at time 21.424612\n",
      "running iteration 50 with loss 0.432726 at time 42.227229\n",
      "running iteration 75 with loss 0.442071 at time 62.669753\n",
      "running iteration 100 with loss 0.444677 at time 82.958263\n",
      "running iteration 125 with loss 0.441042 at time 103.435581\n",
      "running iteration 150 with loss 0.434377 at time 123.984927\n",
      "average training loss 0.446474\n",
      "test loss 0.479238\n",
      "Total run time was 139.348846\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 3\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.442853 at time 0.863010\n",
      "running iteration 25 with loss 0.453055 at time 21.433159\n",
      "running iteration 50 with loss 0.434493 at time 42.266344\n",
      "running iteration 75 with loss 0.444929 at time 62.709697\n",
      "running iteration 100 with loss 0.428918 at time 83.234612\n",
      "running iteration 125 with loss 0.434398 at time 103.539651\n",
      "running iteration 150 with loss 0.430365 at time 124.067692\n",
      "average training loss 0.444067\n",
      "test loss 0.477651\n",
      "Total run time was 139.552866\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 4\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.438694 at time 0.840910\n",
      "running iteration 25 with loss 0.424134 at time 21.388094\n",
      "running iteration 50 with loss 0.423197 at time 41.690207\n",
      "running iteration 75 with loss 0.430754 at time 62.317539\n",
      "running iteration 100 with loss 0.427456 at time 82.769517\n",
      "running iteration 125 with loss 0.427713 at time 103.264391\n",
      "running iteration 150 with loss 0.423596 at time 123.648519\n",
      "average training loss 0.434185\n",
      "test loss 0.473107\n",
      "Total run time was 139.017232\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 5\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.433957 at time 0.805757\n",
      "running iteration 25 with loss 0.419534 at time 21.125824\n",
      "running iteration 50 with loss 0.421532 at time 41.405344\n",
      "running iteration 75 with loss 0.424206 at time 61.801299\n",
      "running iteration 100 with loss 0.420095 at time 82.266181\n",
      "running iteration 125 with loss 0.424852 at time 102.636837\n",
      "running iteration 150 with loss 0.437009 at time 123.257226\n",
      "average training loss 0.431361\n",
      "test loss 0.482853\n",
      "Total run time was 138.640851\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 6\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.447066 at time 0.881212\n",
      "running iteration 25 with loss 0.422289 at time 21.204087\n",
      "running iteration 50 with loss 0.421873 at time 41.650043\n",
      "running iteration 75 with loss 0.435133 at time 61.900166\n",
      "running iteration 100 with loss 0.419017 at time 82.004833\n",
      "running iteration 125 with loss 0.416674 at time 102.538783\n",
      "running iteration 150 with loss 0.413283 at time 122.800837\n",
      "average training loss 0.427758\n",
      "test loss 0.462178\n",
      "Total run time was 138.287097\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 7\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.428435 at time 0.837514\n",
      "running iteration 25 with loss 0.415050 at time 21.378235\n",
      "running iteration 50 with loss 0.408009 at time 41.781973\n",
      "running iteration 75 with loss 0.412800 at time 62.423656\n",
      "running iteration 100 with loss 0.408068 at time 83.059508\n",
      "running iteration 125 with loss 0.406485 at time 103.396008\n",
      "running iteration 150 with loss 0.402641 at time 123.713166\n",
      "average training loss 0.414711\n",
      "test loss 0.453680\n",
      "Total run time was 139.210743\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 8\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.414224 at time 0.859886\n",
      "running iteration 25 with loss 0.397920 at time 21.654785\n",
      "running iteration 50 with loss 0.423293 at time 42.001621\n",
      "running iteration 75 with loss 0.409092 at time 62.407664\n",
      "running iteration 100 with loss 0.400701 at time 82.926096\n",
      "running iteration 125 with loss 0.399853 at time 103.402734\n",
      "running iteration 150 with loss 0.398851 at time 123.617178\n",
      "average training loss 0.407223\n",
      "test loss 0.447787\n",
      "Total run time was 139.111146\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 9\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.406627 at time 0.913780\n",
      "running iteration 25 with loss 0.392233 at time 21.115181\n",
      "running iteration 50 with loss 0.393937 at time 41.315167\n",
      "running iteration 75 with loss 0.401107 at time 61.630161\n",
      "running iteration 100 with loss 0.399894 at time 82.158324\n",
      "running iteration 125 with loss 0.388570 at time 102.552462\n",
      "running iteration 150 with loss 0.388905 at time 123.040652\n",
      "average training loss 0.398454\n",
      "test loss 0.440413\n",
      "Total run time was 138.396662\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 10\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.398754 at time 0.805204\n",
      "running iteration 25 with loss 0.397229 at time 21.427381\n",
      "running iteration 50 with loss 0.391724 at time 41.541430\n",
      "running iteration 75 with loss 0.393128 at time 61.773547\n",
      "running iteration 100 with loss 0.387411 at time 82.411781\n",
      "running iteration 125 with loss 0.382947 at time 102.721012\n",
      "running iteration 150 with loss 0.380507 at time 122.993840\n",
      "average training loss 0.394530\n",
      "test loss 0.435125\n",
      "Total run time was 138.400195\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 11\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.389985 at time 0.843750\n",
      "running iteration 25 with loss 0.378320 at time 21.211890\n",
      "running iteration 50 with loss 0.386908 at time 41.555189\n",
      "running iteration 75 with loss 0.380920 at time 62.023455\n",
      "running iteration 100 with loss 0.378019 at time 82.637488\n",
      "running iteration 125 with loss 0.377813 at time 102.861223\n",
      "running iteration 150 with loss 0.372624 at time 123.173807\n",
      "average training loss 0.383388\n",
      "test loss 0.431540\n",
      "Total run time was 138.613617\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 12\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.382987 at time 0.859692\n",
      "running iteration 25 with loss 0.369616 at time 21.370255\n",
      "running iteration 50 with loss 0.381988 at time 41.788419\n",
      "running iteration 75 with loss 0.382869 at time 62.206252\n",
      "running iteration 100 with loss 0.372981 at time 82.809342\n",
      "running iteration 125 with loss 0.375047 at time 103.257754\n",
      "running iteration 150 with loss 0.367470 at time 123.691862\n",
      "average training loss 0.376847\n",
      "test loss 0.429418\n",
      "Total run time was 139.064100\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 13\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.378251 at time 0.808150\n",
      "running iteration 25 with loss 0.362221 at time 21.365724\n",
      "running iteration 50 with loss 0.376763 at time 41.979334\n",
      "running iteration 75 with loss 0.369279 at time 62.385850\n",
      "running iteration 100 with loss 0.361084 at time 83.012176\n",
      "running iteration 125 with loss 0.367053 at time 103.273085\n",
      "running iteration 150 with loss 0.358208 at time 123.564631\n",
      "average training loss 0.368330\n",
      "test loss 0.424427\n",
      "Total run time was 139.030367\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 14\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.368039 at time 0.802605\n",
      "running iteration 25 with loss 0.354881 at time 21.319455\n",
      "running iteration 50 with loss 0.362491 at time 41.567913\n",
      "running iteration 75 with loss 0.362200 at time 61.915476\n",
      "running iteration 100 with loss 0.357429 at time 82.353473\n",
      "running iteration 125 with loss 0.356488 at time 102.674590\n",
      "running iteration 150 with loss 0.346472 at time 123.094312\n",
      "average training loss 0.359922\n",
      "test loss 0.420868\n",
      "Total run time was 138.516532\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 15\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.358401 at time 0.855777\n",
      "running iteration 25 with loss 0.343029 at time 21.280195\n",
      "running iteration 50 with loss 0.354544 at time 41.614614\n",
      "running iteration 75 with loss 0.352087 at time 61.867759\n",
      "running iteration 100 with loss 0.345663 at time 82.201383\n",
      "running iteration 125 with loss 0.350203 at time 102.465938\n",
      "running iteration 150 with loss 0.337497 at time 122.934642\n",
      "average training loss 0.351994\n",
      "test loss 0.419285\n",
      "Total run time was 138.340480\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 16\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.352359 at time 0.835869\n",
      "running iteration 25 with loss 0.338780 at time 21.246898\n",
      "running iteration 50 with loss 0.351375 at time 41.929135\n",
      "running iteration 75 with loss 0.341894 at time 62.539796\n",
      "running iteration 100 with loss 0.341411 at time 83.013643\n",
      "running iteration 125 with loss 0.340386 at time 103.520721\n",
      "running iteration 150 with loss 0.331691 at time 123.960496\n",
      "average training loss 0.343469\n",
      "test loss 0.418106\n",
      "Total run time was 139.507640\n",
      "New best model found. Saving\n",
      "results/3LayerNNet_longRun\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 17\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.344303 at time 0.833213\n",
      "running iteration 25 with loss 0.327623 at time 21.120163\n",
      "running iteration 50 with loss 0.340281 at time 42.235646\n",
      "running iteration 75 with loss 0.333793 at time 62.590566\n",
      "running iteration 100 with loss 0.336877 at time 82.825162\n",
      "running iteration 125 with loss 0.334910 at time 103.395952\n",
      "running iteration 150 with loss 0.323932 at time 123.717424\n",
      "average training loss 0.335415\n",
      "test loss 0.419658\n",
      "Total run time was 139.197721\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 18\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.336730 at time 0.877376\n",
      "running iteration 25 with loss 0.327328 at time 21.653593\n",
      "running iteration 50 with loss 0.336859 at time 42.278937\n",
      "running iteration 75 with loss 0.327656 at time 62.735926\n",
      "running iteration 100 with loss 0.324171 at time 83.368158\n",
      "running iteration 125 with loss 0.328367 at time 104.279045\n",
      "running iteration 150 with loss 0.316446 at time 124.883559\n",
      "average training loss 0.328332\n",
      "test loss 0.422719\n",
      "Total run time was 140.311051\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 19\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.327419 at time 0.813491\n",
      "running iteration 25 with loss 0.312388 at time 21.570666\n",
      "running iteration 50 with loss 0.328130 at time 42.003574\n",
      "running iteration 75 with loss 0.320038 at time 62.599833\n",
      "running iteration 100 with loss 0.314439 at time 82.781240\n",
      "running iteration 125 with loss 0.322223 at time 103.127514\n",
      "running iteration 150 with loss 0.312559 at time 123.520439\n",
      "average training loss 0.321693\n",
      "test loss 0.425621\n",
      "Total run time was 139.018670\n",
      "validation Loss Increase\n",
      "Stopping early because of increasing validation loss\n"
     ]
    }
   ],
   "source": [
    "from trainModel import trainModel\n",
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "trainModel(helperObj = helper, embeddings = embeddings, hyperParamDict = hyperParamDict, \n",
    "          xDev = xDev, xTrain = xTrain, yDev = yDev, yTrain = yTrain, \n",
    "           lastTrueWordIdx_dev = lastTrueWordIdx_dev, \n",
    "           lastTrueWordIdx_train = lastTrueWordIdx_train,\n",
    "           training_epochs = training_epochs, \n",
    "           output_path = output_path, batchSizeTrain = batch_size,\n",
    "           sizeList = sizeList,\n",
    "           maxIncreasingLossCount = 100, batchSizeDev = 1500, chatty = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/temp'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here we go\n",
      "test loss 0.597082\n",
      "***********************************************\n",
      "test loss 0.597082\n",
      "***********************************************\n",
      "test loss 0.597082\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "trueWordIdxs = tf.placeholder(tf.int32, shape = (None,1))\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    modelDict = reloadModel(session = session,\n",
    "                            saverCheckPointPath = 'results/temp/',\n",
    "                            saverMetaPath = 'results/temp/bestModel.meta')\n",
    "    print('here we go')\n",
    "    for i in range(3):\n",
    "        pred_y = session.run(modelDict['y_last'],feed_dict={modelDict['xPlaceHolder']: xDev,\n",
    "                                      modelDict['trueWordIdxs']:lastTrueWordIdx_dev,\n",
    "                                      modelDict['outputKeepProb']: 1.0,\n",
    "                                      modelDict['inputKeepProb']: 1.0}, ) \n",
    "        validLoss = tf.nn.sigmoid_cross_entropy_with_logits(logits = pred_y, \n",
    "                                             labels = tf.cast(yDev, tf.float32))\n",
    "        validLoss = tf.reduce_mean(validLoss)\n",
    "        validLoss = validLoss.eval()\n",
    "        print('test loss %f'%(validLoss))\n",
    "        print('***********************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xDev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7217b07cbf1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxDev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xDev' is not defined"
     ]
    }
   ],
   "source": [
    "xDev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
