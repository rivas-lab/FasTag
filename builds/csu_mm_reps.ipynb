{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the LSTM on your Data\n",
    "\n",
    "This notebook will take you through the steps necessary to train an LSTM to recognize ICD-9 codes, or items from similar dictionaries, from free text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Make sure that the below packages are installed on the server on which this program will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/taggerSystem/')\n",
    "from data_util import load_and_preprocess_data, load_embeddings, ModelHelper, lastTrueWordIdxs\n",
    "import tensorflow as tf\n",
    "from simpleLSTMWithNNetModel import Model, reloadModel\n",
    "from trainModel import trainModel\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pickle\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories\n",
    "\n",
    "Executing the following cell will prompt the user to type in the directories corresponding to the training and validation sets, the vocabulary, and the word vector mappings. Defaults are given in the comments below.\n",
    "\n",
    "Note that the training and test data needs to have one column dedicated to free text (`noteIdx`) and another dedicated to top-level ICD-9 codes (`codeIdx`) associated with each patient. Preferably, the latter should be strung together using '-' as the delimiter (e.g. for patient 1, 1-2-6-4).\n",
    "\n",
    "Please make sure that the parameters such as the embed size, maximum note length, learning rate, number of maximum training epochs, batch size, hidden layer size, number of neural net layers, and probabilities are to your specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put sample file with require file headers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adjust training data headers to match small icd9 training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the path to the training data? data/0815/csu_mm_reps_nodiag_no17_0815_train.csv\n",
      "What is the path to the validation data? data/0815/csu_mm_reps_nodiag_no17_0815_valid.csv\n",
      "What is the path to the vocabulary? data/icd9Vocab.txt\n",
      "What is the path to the vocabulary? data/newgloveicd9.txt\n",
      "Which column contains top-level ICD-9 codes (outputs) in the training and test data? Default is 9. 9\n",
      "Which column contains notes/text (inputs) in the training and test data? Default is 6. 6\n"
     ]
    }
   ],
   "source": [
    "data_train = raw_input('What is the path to the training data? ') #default: data/icd9NotesDataTable_train.csv\n",
    "data_valid = raw_input('What is the path to the validation data? ') #default: data/icd9NotesDataTable_valid.csv\n",
    "vocab = raw_input('What is the path to the vocabulary? ') #default: data/icd9Vocab.txt\n",
    "wordVecs = raw_input('What is the path to the vocabulary? ') #data/newgloveicd9.txt. These are length 300 word vectors from GloVE\n",
    "\n",
    "NUM = \"NNNUMMM\"\n",
    "UNK = \"UUUNKKK\"\n",
    "EMBED_SIZE = 300 # this should correspond to the length of the word vectors\n",
    "maxAllowedNoteLength = 1000\n",
    "max_grad_norm = 5\n",
    "codeIdx = raw_input('Which column contains top-level ICD-9 codes (outputs) in the training and test data? Default is 9. ')\n",
    "textIdx = raw_input('Which column contains notes/text (inputs) in the training and test data? Default is 6. ')\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 256\n",
    "n_hidden = 200\n",
    "output_keep_prob = 0.5\n",
    "input_keep_prob = 1\n",
    "numLayers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# Fixing some issue ashley had\n",
    "\n",
    "textIdx = int(textIdx)\n",
    "codeIdx = int(codeIdx)# I'm not sure how models were trinaed before if this part was broken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, make sure that the output path is specified as you would like. By default, the program saves the output in a folder with the name of your choice within the folder `results`.\n",
    "\n",
    "If there exists a folder with results that you would like to load again, use that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where are models and performances (to be) saved? 0815_csu_mm_reps\n"
     ]
    }
   ],
   "source": [
    "output_path = raw_input('Where are models and performances (to be) saved? ')\n",
    "output_path = os.path.join('results', output_path)\n",
    "if output_path == 'results/':\n",
    "    output_path = 'results/temp'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Executing the following cell will ask whether or not there is a previously saved model; if not, the model will train features from scratch, and if so, the features will be loaded.\n",
    "\n",
    "Note that AZ added \"int() to the codeIdx and textIdx to resolve some errors that were preventing it from initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a previously trained model? [Y/n] n\n",
      "Total number of batches per epoch: 245\n",
      "Maximum note length: 1000\n",
      "Number of ICD-9 codes: 17\n",
      "There are a total of: 17 ICD-9 codes\n",
      "{   'cat:1': 8,\n",
      "    'cat:10': 15,\n",
      "    'cat:11': 14,\n",
      "    'cat:12': 13,\n",
      "    'cat:13': 12,\n",
      "    'cat:14': 11,\n",
      "    'cat:15': 10,\n",
      "    'cat:16': 9,\n",
      "    'cat:18': 16,\n",
      "    'cat:2': 7,\n",
      "    'cat:3': 6,\n",
      "    'cat:4': 5,\n",
      "    'cat:5': 4,\n",
      "    'cat:6': 3,\n",
      "    'cat:7': 2,\n",
      "    'cat:8': 1,\n",
      "    'cat:9': 0}\n",
      "xDev shape: nObs = 26733, nWords = 1000\n",
      "yDev shape: nObs = 26733, nClasses = 17\n",
      "xTrain shape: nObs = 62858, nWords = 1000\n",
      "yTrain shape: nObs = 62858, nClasses = 17\n",
      "Embeddings shape: nWords = 10008, wordVec length = 300\n"
     ]
    }
   ],
   "source": [
    "sizeList = [n_hidden, 150, 75] # these are the weights we will be using\n",
    "\n",
    "def query_yes_no(question, default=\"yes\"):\n",
    "    \"\"\"Ask a yes/no question via raw_input() and return their answer.\n",
    "\n",
    "    \"question\" is a string that is presented to the user.\n",
    "    \"default\" is the presumed answer if the user just hits <Enter>.\n",
    "        It must be \"yes\" (the default), \"no\" or None (meaning\n",
    "        an answer is required of the user).\n",
    "\n",
    "    The \"answer\" return value is True for \"yes\" or False for \"no\".\n",
    "    \"\"\"\n",
    "    valid = {\"yes\": True, \"y\": True, \"ye\": True,\n",
    "             \"no\": False, \"n\": False}\n",
    "    if default is None:\n",
    "        prompt = \" [y/n] \"\n",
    "    elif default == \"yes\":\n",
    "        prompt = \" [Y/n] \"\n",
    "    elif default == \"no\":\n",
    "        prompt = \" [y/N] \"\n",
    "    else:\n",
    "        raise ValueError(\"invalid default answer: '%s'\" % default)\n",
    "\n",
    "    while True:\n",
    "        sys.stdout.write(question + prompt)\n",
    "        choice = raw_input().lower()\n",
    "        if default is not None and choice == '':\n",
    "            return valid[default]\n",
    "        elif choice in valid:\n",
    "            return valid[choice]\n",
    "        else:\n",
    "            sys.stdout.write(\"Please respond with 'yes' or 'no' \"\n",
    "                             \"(or 'y' or 'n').\\n\")\n",
    "            \n",
    "prev_model = query_yes_no(\"Is there a previously trained model?\")\n",
    "\n",
    "if prev_model:\n",
    "    helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = int(codeIdx), textIdx = int(textIdx),\n",
    "    helperLoadPath = output_path)\n",
    "else:\n",
    "    #print codeIdx\n",
    "    #print textIdx\n",
    "    helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = int(codeIdx), textIdx = int(textIdx))\n",
    "    \n",
    "embeddings = load_embeddings(vocab, wordVecs, helper, embeddingSize = EMBED_SIZE)\n",
    "lastTrueWordIdx_train = lastTrueWordIdxs(train)\n",
    "lastTrueWordIdx_dev = lastTrueWordIdxs(dev)\n",
    "helper.save(output_path) # token2id and max length saved to output_path\n",
    "sizeList.append(helper.n_labels)\n",
    "\n",
    "total_batches = (xTrain.shape[0]//batch_size)\n",
    "print('Total number of batches per epoch: %d'%(total_batches))\n",
    "print('Maximum note length: %d'%(helper.max_length))\n",
    "print('Number of ICD-9 codes: %d'%(helper.n_labels))\n",
    "print('There are a total of: {} ICD-9 codes'.format(len(helper.icdDict.keys())))\n",
    "pp.pprint(helper.icdDict)\n",
    "print('xDev shape: nObs = %d, nWords = %d'%(xDev.shape))\n",
    "print('yDev shape: nObs = %d, nClasses = %d'%(yDev.shape))\n",
    "print('xTrain shape: nObs = %d, nWords = %d'%(xTrain.shape))\n",
    "print('yTrain shape: nObs = %d, nClasses = %d'%(yTrain.shape))\n",
    "print('Embeddings shape: nWords = %d, wordVec length = %d'%(embeddings.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell initializes the dictionary of hyperparameters for the model that fully describe the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'EMBED_SIZE': 300,\n",
      "    'batchSize': 256,\n",
      "    'inputKeepProb': 1,\n",
      "    'learningRate': 0.001,\n",
      "    'maxGradNorm': 5,\n",
      "    'maxNoteLength': 1000,\n",
      "    'n_hidden': 200,\n",
      "    'numLayers': 1,\n",
      "    'outputKeepProb': 0.5,\n",
      "    'sizeList': [200, 150, 75, 17],\n",
      "    'trainingEpochsMax': 100}\n"
     ]
    }
   ],
   "source": [
    "hyperParamDict = {'EMBED_SIZE': EMBED_SIZE,\n",
    "                  'maxNoteLength': maxAllowedNoteLength,\n",
    "                  'maxGradNorm': max_grad_norm,\n",
    "                  'outputKeepProb': output_keep_prob,\n",
    "                  'inputKeepProb': input_keep_prob,\n",
    "                  'learningRate': learning_rate,\n",
    "                  'trainingEpochsMax': training_epochs,\n",
    "                  'batchSize': batch_size,\n",
    "                  'n_hidden': n_hidden,\n",
    "                 'numLayers': numLayers,\n",
    "                 'sizeList':sizeList}\n",
    "pp.pprint(hyperParamDict)\n",
    "with open(os.path.join(output_path, 'hyperParamDict.pickle'), 'wb') as handle:\n",
    "    pickle.dump(hyperParamDict, handle, protocol = 2)\n",
    "    #dumping with 2 because ALTUD uses python 2.7 right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, the model is trained (be wary - it will take some time; on an Amazon Deep Learning AMI, it took around an hour to train)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embeddings\n",
      "(?, 1000, 300)\n",
      "<class 'tensorflow.python.ops.rnn_cell_impl.MultiRNNCell'>\n",
      "<tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fbddf11ba50>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(?, 1000, 300)\n",
      "cell output size\n",
      "200\n",
      "cell state size\n",
      "(LSTMStateTuple(c=200, h=200),)\n",
      "output shape\n",
      "(?, 1000, 200)\n",
      "offset shape\n",
      "(?, 1)\n",
      "output shape new shape\n",
      "(?, 200)\n",
      "flattened indices shape\n",
      "(?, 1)\n",
      "output flattened shape\n",
      "(?, 200)\n",
      "(200, 150)\n",
      "W_1 shape\n",
      "(150,)\n",
      "b_1 shape\n",
      "(150, 75)\n",
      "W_2 shape\n",
      "(75,)\n",
      "bias shape\n",
      "(17,)\n",
      "U shape\n",
      "(75, 17)\n",
      "bias shape\n",
      "(17,)\n",
      "output wx + b\n",
      "(?, 17)\n",
      "(?, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************\n",
      "***************************\n",
      "Running on epoch 0\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.685075 at time 1.591646\n",
      "running iteration 25 with loss 0.342956 at time 33.312460\n",
      "running iteration 50 with loss 0.295581 at time 65.090286\n",
      "running iteration 75 with loss 0.325648 at time 96.951592\n",
      "running iteration 100 with loss 0.316920 at time 128.831110\n",
      "running iteration 125 with loss 0.309444 at time 160.727727\n",
      "running iteration 150 with loss 0.311251 at time 192.612945\n",
      "running iteration 175 with loss 0.305071 at time 224.514509\n",
      "running iteration 200 with loss 0.320760 at time 256.372719\n",
      "running iteration 225 with loss 0.337454 at time 288.258147\n",
      "average training loss 0.331387\n",
      "test loss 0.301815\n",
      "Total run time was 344.864455\n",
      "New best model found. Saving\n",
      "results/0815_csu_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 1\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.294207 at time 1.274893\n",
      "running iteration 25 with loss 0.276648 at time 33.140879\n",
      "running iteration 50 with loss 0.239016 at time 65.035341\n",
      "running iteration 75 with loss 0.269997 at time 96.933662\n",
      "running iteration 100 with loss 0.249980 at time 128.791680\n",
      "running iteration 125 with loss 0.265503 at time 160.567712\n",
      "running iteration 150 with loss 0.258376 at time 192.306292\n",
      "running iteration 175 with loss 0.259361 at time 224.039348\n",
      "running iteration 200 with loss 0.276377 at time 255.781045\n",
      "running iteration 225 with loss 0.285884 at time 287.558910\n",
      "average training loss 0.269105\n",
      "test loss 0.245179\n",
      "Total run time was 344.201849\n",
      "New best model found. Saving\n",
      "results/0815_csu_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 2\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.241034 at time 1.288136\n",
      "running iteration 25 with loss 0.228804 at time 33.209218\n",
      "running iteration 50 with loss 0.190284 at time 65.125283\n",
      "running iteration 75 with loss 0.217056 at time 97.053730\n",
      "running iteration 100 with loss 0.208742 at time 128.975409\n",
      "running iteration 125 with loss 0.220403 at time 160.894477\n",
      "running iteration 150 with loss 0.217202 at time 192.814372\n",
      "running iteration 175 with loss 0.216698 at time 224.739921\n",
      "running iteration 200 with loss 0.230440 at time 256.647031\n",
      "running iteration 225 with loss 0.256447 at time 288.556786\n",
      "average training loss 0.227623\n",
      "test loss 0.210612\n",
      "Total run time was 345.208374\n",
      "New best model found. Saving\n",
      "results/0815_csu_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 3\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.196252 at time 1.278036\n",
      "running iteration 25 with loss 0.189665 at time 33.014006\n",
      "running iteration 50 with loss 0.160991 at time 64.751180\n",
      "running iteration 75 with loss 0.189336 at time 96.511543\n",
      "running iteration 100 with loss 0.183482 at time 128.271309\n",
      "running iteration 125 with loss 0.192228 at time 160.211142\n",
      "running iteration 150 with loss 0.199775 at time 192.142359\n",
      "running iteration 175 with loss 0.192261 at time 224.068047\n",
      "running iteration 200 with loss 0.209986 at time 255.984438\n",
      "running iteration 225 with loss 0.223943 at time 287.849747\n",
      "average training loss 0.201287\n",
      "test loss 0.192490\n",
      "Total run time was 344.411263\n",
      "New best model found. Saving\n",
      "results/0815_csu_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 4\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.177901 at time 1.279303\n",
      "running iteration 25 with loss 0.175701 at time 33.193002\n",
      "running iteration 50 with loss 0.141510 at time 65.053376\n",
      "running iteration 75 with loss 0.173412 at time 96.952647\n",
      "running iteration 100 with loss 0.166667 at time 128.821874\n",
      "running iteration 125 with loss 0.176619 at time 160.694150\n",
      "running iteration 150 with loss 0.183713 at time 192.514874\n",
      "running iteration 175 with loss 0.172964 at time 224.255077\n",
      "running iteration 200 with loss 0.191355 at time 255.968945\n",
      "running iteration 225 with loss 0.210449 at time 287.711185\n",
      "average training loss 0.183950\n",
      "test loss 0.180740\n",
      "Total run time was 344.307141\n",
      "New best model found. Saving\n",
      "results/0815_csu_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 5\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.166543 at time 1.294920\n",
      "running iteration 25 with loss 0.165523 at time 33.180958\n",
      "running iteration 50 with loss 0.131569 at time 65.069614\n",
      "running iteration 75 with loss 0.159360 at time 96.959846\n",
      "running iteration 100 with loss 0.161515 at time 128.825444\n",
      "running iteration 125 with loss 0.156847 at time 160.726500\n",
      "running iteration 150 with loss 0.172016 at time 192.623628\n",
      "running iteration 175 with loss 0.157263 at time 224.516055\n",
      "running iteration 200 with loss 0.182565 at time 256.413928\n",
      "running iteration 225 with loss 0.204120 at time 288.294332\n",
      "average training loss 0.171141\n",
      "test loss 0.175352\n",
      "Total run time was 345.125059\n",
      "New best model found. Saving\n",
      "results/0815_csu_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 6\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.153252 at time 1.278418\n",
      "running iteration 25 with loss 0.158035 at time 33.031839\n",
      "running iteration 50 with loss 0.123871 at time 64.800117\n",
      "running iteration 75 with loss 0.149768 at time 96.594730\n",
      "running iteration 100 with loss 0.148829 at time 128.384260\n",
      "running iteration 125 with loss 0.150877 at time 160.181095\n",
      "running iteration 150 with loss 0.160945 at time 192.103047\n",
      "running iteration 175 with loss 0.149441 at time 224.025789\n",
      "running iteration 200 with loss 0.173345 at time 255.962480\n",
      "running iteration 225 with loss 0.198345 at time 287.851934\n",
      "average training loss 0.160356\n",
      "test loss 0.172209\n",
      "Total run time was 344.623367\n",
      "New best model found. Saving\n",
      "results/0815_csu_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 7\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.142568 at time 1.285982\n",
      "running iteration 25 with loss 0.147695 at time 33.184613\n",
      "running iteration 50 with loss 0.111794 at time 65.076315\n",
      "running iteration 75 with loss 0.141454 at time 96.964387\n",
      "running iteration 100 with loss 0.142321 at time 128.856459\n",
      "running iteration 125 with loss 0.141895 at time 160.768847\n",
      "running iteration 150 with loss 0.147053 at time 192.656792\n",
      "running iteration 175 with loss 0.141017 at time 224.396551\n",
      "running iteration 200 with loss 0.162012 at time 256.116731\n",
      "running iteration 225 with loss 0.182898 at time 287.864411\n",
      "average training loss 0.150529\n",
      "test loss 0.171166\n",
      "Total run time was 344.440954\n",
      "New best model found. Saving\n",
      "results/0815_csu_mm_reps\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 8\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.133656 at time 1.295375\n",
      "running iteration 25 with loss 0.140012 at time 33.201576\n",
      "running iteration 50 with loss 0.103442 at time 65.111627\n",
      "running iteration 75 with loss 0.129612 at time 96.990835\n",
      "running iteration 100 with loss 0.131332 at time 128.862823\n",
      "running iteration 125 with loss 0.134024 at time 160.757307\n",
      "running iteration 150 with loss 0.140553 at time 192.642562\n",
      "running iteration 175 with loss 0.132289 at time 224.547577\n",
      "running iteration 200 with loss 0.151385 at time 256.453851\n",
      "running iteration 225 with loss 0.176958 at time 288.334507\n",
      "average training loss 0.141954\n",
      "test loss 0.173921\n",
      "Total run time was 345.080948\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 9\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.130172 at time 1.275606\n",
      "running iteration 25 with loss 0.131917 at time 33.033978\n",
      "running iteration 50 with loss 0.094038 at time 64.725368\n",
      "running iteration 75 with loss 0.125955 at time 96.450459\n",
      "running iteration 100 with loss 0.129775 at time 128.148209\n",
      "running iteration 125 with loss 0.123765 at time 159.873127\n",
      "running iteration 150 with loss 0.133598 at time 191.695537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iteration 175 with loss 0.123291 at time 223.536139\n",
      "running iteration 200 with loss 0.144430 at time 255.419788\n",
      "running iteration 225 with loss 0.167822 at time 287.283023\n",
      "average training loss 0.133790\n",
      "test loss 0.176896\n",
      "Total run time was 343.759295\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 10\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.126768 at time 1.274337\n",
      "running iteration 25 with loss 0.125277 at time 33.166925\n",
      "running iteration 50 with loss 0.090365 at time 65.042288\n",
      "running iteration 75 with loss 0.122057 at time 96.909256\n",
      "running iteration 100 with loss 0.114951 at time 128.715877\n",
      "running iteration 125 with loss 0.120887 at time 160.562841\n",
      "running iteration 150 with loss 0.124664 at time 192.438064\n",
      "running iteration 175 with loss 0.114800 at time 224.268281\n",
      "running iteration 200 with loss 0.138902 at time 256.041842\n",
      "running iteration 225 with loss 0.153591 at time 287.782280\n",
      "average training loss 0.125883\n",
      "test loss 0.182672\n",
      "Total run time was 344.402617\n",
      "validation Loss Increase\n",
      "Stopping early because of increasing validation loss\n"
     ]
    }
   ],
   "source": [
    "from trainModel import trainModel\n",
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "trainModel(helperObj = helper, embeddings = embeddings, hyperParamDict = hyperParamDict, \n",
    "          xDev = xDev, xTrain = xTrain, yDev = yDev, yTrain = yTrain, \n",
    "           lastTrueWordIdx_dev = lastTrueWordIdx_dev, \n",
    "           lastTrueWordIdx_train = lastTrueWordIdx_train,\n",
    "           training_epochs = training_epochs, \n",
    "           output_path = output_path, batchSizeTrain = batch_size,\n",
    "           sizeList = sizeList,\n",
    "           maxIncreasingLossCount = 100, batchSizeDev = 1500, chatty = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the session closed. You should be able to see your results in the `output_path` directory you specified earlier.\n",
    "\n",
    "To evaluate the results and generate plots and such, please check out `predictionEvaluation.ipynb` in the same repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xDev[xDev == -1] = 0\n",
    "# xTrain[xTrain == -1] = 0\n",
    "# trueWordIdxs = tf.placeholder(tf.int32, shape = (None,1))\n",
    "# with tf.Session() as session:\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     modelDict = reloadModel(session = session,\n",
    "#                             saverCheckPointPath = output_path,\n",
    "#                             saverMetaPath = output_path + '/bestModel.meta')\n",
    "#     print('here we go')\n",
    "#     for i in range(3):\n",
    "#         pred_y = session.run(modelDict['y_last'],feed_dict={modelDict['xPlaceHolder']: xDev,\n",
    "#                                       modelDict['trueWordIdxs']:lastTrueWordIdx_dev,\n",
    "#                                       modelDict['outputKeepProb']: 1.0,\n",
    "#                                       modelDict['inputKeepProb']: 1.0}, ) \n",
    "#         validLoss = tf.nn.sigmoid_cross_entropy_with_logits(logits = pred_y, \n",
    "#                                              labels = tf.cast(yDev, tf.float32))\n",
    "#         validLoss = tf.reduce_mean(validLoss)\n",
    "#         validLoss = validLoss.eval()\n",
    "#         print('test loss %f'%(validLoss))\n",
    "#         print('***********************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
