{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/taggerSystem/')\n",
    "from data_util import load_and_preprocess_data, load_embeddings, ModelHelper, lastTrueWordIdxs\n",
    "from simpleLSTMWithNNetModel import Model, reloadModel\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_train = \"data/icd9NotesDataTable_train.csv\"\n",
    "# data_valid = \"data/icd9NotesDataTable_valid.csv\"\n",
    "data_train = \"data/smallIcd9NotesDataTable_train.csv\"\n",
    "data_valid = \"data/smallIcd9NotesDataTable_valid.csv\"\n",
    "# vocab = \"src/taggerSystem/data_hw3_delete/vocab.txt\"\n",
    "# wordVecs = \"src/taggerSystem/data_hw3_delete/wordVectors.txt\"\n",
    "vocab = 'data/icd9Vocab.txt'# vocab for our data\n",
    "wordVecs = 'data/newgloveicd9.txt'# len 300 word vectors\n",
    "# output_path = 'results/model_description'\n",
    "# log_output = output_path + \"log\"\n",
    "# vocab = 'data/icd9Vocab.txt'# vocab for our data\n",
    "# wordVecs = 'data/newgloveicd9.txt'# len 300 word vectors\n",
    "# START_TOKEN = \"<s>\"\n",
    "# END_TOKEN = \"</s>\"\n",
    "NUM = \"NNNUMMM\"\n",
    "UNK = \"UUUNKKK\"\n",
    "EMBED_SIZE = 50 # this should not be manually set and should instead come from the word\n",
    "# vectors. Or maybe it's good that we have to set it so we know for sure the size\n",
    "# of word vecs we're using\n",
    "maxAllowedNoteLength = 500\n",
    "max_grad_norm = 5\n",
    "codeIdx = 9\n",
    "textIdx = 6\n",
    "num_layers = 4\n",
    "learning_rate = 0.001\n",
    "training_epochs = 5\n",
    "batch_size = 1\n",
    "# total_batches = (xTrain.shape[0]//batch_size)\n",
    "# print('Total number of batches per epoch %d'%(total_batches))\n",
    "n_input = 1\n",
    "# n_steps = 10\n",
    "n_hidden = 200\n",
    "# n_classes = helper.n_labels\n",
    "output_keep_prob = 0.5\n",
    "input_keep_prob = 1# not sure why there are two but example used 0.5 for output. Check with TA\n",
    "numLayers = 1\n",
    "# embeddingSize = embeddings.shape[1]\n",
    "# print('Embedding size is %d'%(embeddingSize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_path = input(prompt = 'Where should models and performances be saved to?')\n",
    "if modelRunOutputPath == '':\n",
    "    output_path = 'results/temp'\n",
    "if not os.path.exists(modelRunOutputPath):\n",
    "    os.makedirs(modelRunOutputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "#     data_train = data_train, data_valid = data_valid, \n",
    "#     maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "#     codeIdx = codeIdx, textIdx = textIdx,\n",
    "#     helperLoadPath = 'results/tempSaveTester/')\n",
    "helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = codeIdx, textIdx = textIdx)\n",
    "embeddings = load_embeddings(vocab, wordVecs, helper, embeddingSize = EMBED_SIZE)\n",
    "lastTrueWordIdx_train = lastTrueWordIdxs(train)\n",
    "lastTrueWordIdx_dev = lastTrueWordIdxs(dev)\n",
    "helper.save('results/tempSaveTester/')# token2id and max length saved to output_path\n",
    "# handler = logging.FileHandler(log_output)\n",
    "# handler.setLevel(logging.DEBUG)\n",
    "# handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "# logging.getLogger().addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'18': 2, '123': 4, '4240': 1, '486': 0, '45': 5, '456': 3}\n",
      "{'horse': 3, 'dog': 1, '<s>': 10, 'CASE:aa': 8, 'UUUNKKK': 9, 'cat': 2, 'the': 5, 'CASE:Aa': 5, 'deadpool': 4, 'CASE:AA': 7, '</s>': 11, 'CASE:aA': 6}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(helper.icdDict)\n",
    "# print(helper.tok2id)\n",
    "print(helper.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'456': 3, '4240': 1, '45': 5, '486': 0, '123': 4, '18': 2}\n",
      "{'horse': 3, 'CASE:Aa': 5, 'dog': 1, 'the': 5, 'CASE:AA': 7, 'deadpool': 4, 'CASE:aA': 6, '<s>': 10, '</s>': 11, 'cat': 2, 'UUUNKKK': 9, 'CASE:aa': 8}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(helper.icdDict)\n",
    "# print(helper.tok2id)\n",
    "print(helper.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperParamDict = {'EMBED_SIZE': EMBED_SIZE,\n",
    "                  'maxNoteLength': maxAllowedNoteLength,\n",
    "                  'maxGradNorm': max_grad_norm,\n",
    "                  'outputKeepProb': output_keep_prob,\n",
    "                  'inputKeepProb': input_keep_prob,\n",
    "                  'learningRate': learning_rate,\n",
    "                  'trainingEpochsMax': training_epochs,\n",
    "                  'batchSize': batch_size,\n",
    "                  'n_hidden': n_hidden,\n",
    "                 'numLayers': numLayers}\n",
    "# with open(os.path.join(modelRunOutputPath, 'hyperParamDict.pickle'), 'wb') as handle:\n",
    "#     pickle.dump(hyperParamsDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EMBED_SIZE': 50,\n",
       " 'batchSize': 1,\n",
       " 'inputKeepProb': 1,\n",
       " 'learningRate': 0.001,\n",
       " 'maxGradNorm': 5,\n",
       " 'maxNoteLength': 500,\n",
       " 'n_hidden': 200,\n",
       " 'numLayers': 1,\n",
       " 'outputKeepProb': 0.5,\n",
       " 'trainingEpochsMax': 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperParamDict = hyperParamsDict\n",
    "hyperParamDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper.n_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EMBED_SIZE': 50,\n",
       " 'batchSize': 1,\n",
       " 'inputKeepProb': 1,\n",
       " 'learningRate': 0.001,\n",
       " 'maxGradNorm': 5,\n",
       " 'maxNoteLength': 500,\n",
       " 'n_hidden': 200,\n",
       " 'numLayers': 1,\n",
       " 'outputKeepProb': 0.5,\n",
       " 'trainingEpochsMax': 5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperParamDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# x = tf.placeholder(tf.int32, shape= (None, helper.max_length))\n",
    "# yTruth = tf.placeholder(tf.int32, shape = (None, helper.n_labels))\n",
    "# y_steps = tf.placeholder(tf.int32, shape = (None, helper.n_labels))# not sure what this is\n",
    "# trueWordIdxs = tf.placeholder(tf.int32, shape = (None,1))# vector which holds true word\n",
    "# outputKeepProb = tf.placeholder(tf.float32, shape=())\n",
    "# inputKeepProb = tf.placeholder(tf.float32, shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(?, 4, 50)\n",
      "shape of embeddings\n",
      "(?, 4, 50)\n",
      "U shape\n",
      "(200, 6)\n",
      "bias shape\n",
      "(6,)\n",
      "<class 'tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell'>\n",
      "<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object at 0x7fdb82050a20>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(?, 4, 50)\n",
      "cell output size\n",
      "200\n",
      "cell state size\n",
      "(LSTMStateTuple(c=200, h=200),)\n",
      "output shape\n",
      "(?, 4, 200)\n",
      "offset shape\n",
      "(?, 1)\n",
      "output shape new shape\n",
      "(?, 200)\n",
      "flattened indices shape\n",
      "(?, 1)\n",
      "output flattened shape\n",
      "(?, 200)\n",
      "output wx + b\n",
      "(?, 6)\n",
      "(?, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliver/anaconda3/envs/clinicalNoteTagger/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# model = Model(xPlaceHolder = x, yPlaceHolder = yTruth, embeddings = embeddings, \n",
    "#               hyperParamDict = hyperParamDict)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from simpleLSTMWithNNetModel import Model\n",
    "tf.reset_default_graph()\n",
    "model = Model(nColsInput = helper.max_length, nLabels = helper.n_labels,\n",
    "             embeddings = embeddings, hyperParamDict = hyperParamDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.685747\n",
      "test loss 0.658781\n",
      "***********************************************\n",
      "train loss 0.644480\n",
      "test loss 0.631446\n",
      "***********************************************\n",
      "train loss 0.640387\n",
      "test loss 0.606006\n",
      "***********************************************\n",
      "train loss 0.619426\n",
      "test loss 0.582254\n",
      "***********************************************\n",
      "train loss 0.596487\n",
      "test loss 0.559412\n",
      "***********************************************\n",
      "train loss 0.576961\n",
      "test loss 0.537442\n",
      "***********************************************\n",
      "train loss 0.536125\n",
      "test loss 0.516530\n",
      "***********************************************\n",
      "train loss 0.529557\n",
      "test loss 0.497012\n",
      "***********************************************\n",
      "train loss 0.508127\n",
      "test loss 0.479050\n",
      "***********************************************\n",
      "train loss 0.484719\n",
      "test loss 0.463060\n",
      "***********************************************\n",
      "saving model\n"
     ]
    }
   ],
   "source": [
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "# all_saver = tf.train.Saver()\n",
    "# tf.add_to_collection('y_last', y_last)\n",
    "# tf.add_to_collection('x', x)\n",
    "# tf.add_to_collection('trueWordIdxs', trueWordIdxs)\n",
    "# tf.add_to_collection('outputKeepProb', outputKeepProb)\n",
    "# tf.add_to_collection('inputKeepProb', inputKeepProb)\n",
    "# tf.add_to_collection('pretrainedEmbeddings', pretrainedEmbeddings)\n",
    "with tf.Session() as session:\n",
    "#         all_saver.restore(session, 'tempDelete/bestModel.data-00000-of-00001')\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(10):\n",
    "#          for b in range(total_batches):\n",
    "#             offset = min((b * batch_size), yTrain.shape[0])\n",
    "#             batch_x = xTrain[offset:(offset + batch_size), :]\n",
    "#             batch_y = yTrain[offset:(offset + batch_size), :]\n",
    "#             batchTrueWordIdxs = lastTrueWordIdx_train[offset:(offset + batch_size)]\n",
    "        _, trainingError = session.run([model.optimize, model.loss_function],\n",
    "                           feed_dict={model.xPlaceHolder: xTrain, \n",
    "                                        model.yPlaceHolder: yTrain, \n",
    "                                        model.trueWordIdxs: lastTrueWordIdx_train,\n",
    "                                        model.outputKeepProb: output_keep_prob, \n",
    "                                        model.inputKeepProb: input_keep_prob})\n",
    "        pred_y = session.run(model.y_last,feed_dict={model.xPlaceHolder: xDev,\n",
    "                                      model.trueWordIdxs:lastTrueWordIdx_dev,\n",
    "                                      model.outputKeepProb: 1,\n",
    "                                      model.inputKeepProb: 1}, ) # must be set to one for predictions.\n",
    "        validLoss = tf.nn.sigmoid_cross_entropy_with_logits(logits = pred_y, \n",
    "                                             labels = tf.cast(yDev, tf.float32))\n",
    "        validLoss = tf.reduce_mean(validLoss)\n",
    "        validLoss = validLoss.eval()\n",
    "        print('train loss %f'% (trainingError))\n",
    "        print('test loss %f'%(validLoss))\n",
    "        print('***********************************************')\n",
    "        if i == 9:\n",
    "            print('saving model')\n",
    "            model.save(session = session, savePath = 'results/tempSaveTester/bestModel')\n",
    "#         model.save(savePath = 'results/temp/newSave',session = session, \n",
    "#                    trueWordIdxs = trueWordIdxs, yTruth = yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_13:0' shape=(?, 4) dtype=int32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xPlaceHolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.get_collection('pretrainedEmbeddings'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 0.463060\n",
      "***********************************************\n",
      "test loss 0.463060\n",
      "***********************************************\n",
      "test loss 0.463060\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "# outputKeepProb = tf.placeholder(tf.float32, shape=())\n",
    "# inputKeepProb = tf.placeholder(tf.float32, shape=())\n",
    "# maxLength = helper.max_length\n",
    "# nLabels = helper.n_labels\n",
    "# xPlaceHolder = tf.placeholder(tf.int32, shape= (None, maxLength))\n",
    "# pretrainedEmbeddings = tf.get_collection('pretrainedEmbeddings')\n",
    "# yPlaceHolder = tf.placeholder(tf.int32, shape = (None, nLabels))\n",
    "trueWordIdxs = tf.placeholder(tf.int32, shape = (None,1))\n",
    "with tf.Session() as session:\n",
    "#         all_saver.restore(session, 'tempDelete/bestModel.data-00000-of-00001')\n",
    "    tf.global_variables_initializer().run()\n",
    "    new_saver = tf.train.import_meta_graph('results/tempSaveTester/bestModel.meta')\n",
    "    new_saver.restore(session, tf.train.latest_checkpoint('results/tempSaveTester/'))\n",
    "    y_last = tf.get_collection('y_last')[0]\n",
    "    outputKeepProb = tf.get_collection('outputKeepProb')[0]\n",
    "    inputKeepProb = tf.get_collection('inputKeepProb')[0]\n",
    "    xPlaceHolder = tf.get_collection('xPlaceHolder')[0]\n",
    "    trueWordIdxs = tf.get_collection('trueWordIdxs')[0]\n",
    "#     y_last = tf.get_collection('y_last')[0]\n",
    "    pretrainedEmbeddings = tf.get_collection('pretrainedEmbeddings')[0]\n",
    "    for i in range(3):\n",
    "#         _, trainingError = session.run([model.optimize, model.loss_function],\n",
    "#                            feed_dict={model.xPlaceHolder: xTrain, \n",
    "#                                         model.yPlaceHolder: yTrain, \n",
    "#                                         model.trueWordIdxs: lastTrueWordIdx_train,\n",
    "#                                         model.outputKeepProb: output_keep_prob, \n",
    "#                                         model.inputKeepProb: input_keep_prob})\n",
    "#         print('ok')\n",
    "        pred_y = session.run(y_last,feed_dict={xPlaceHolder: xDev,\n",
    "                                      trueWordIdxs:lastTrueWordIdx_dev,\n",
    "                                      outputKeepProb: 1.0,\n",
    "                                      inputKeepProb: 1.0}, ) # must be set to one for predictions.\n",
    "#         print('leggo')\n",
    "        validLoss = tf.nn.sigmoid_cross_entropy_with_logits(logits = pred_y, \n",
    "                                             labels = tf.cast(yDev, tf.float32))\n",
    "        validLoss = tf.reduce_mean(validLoss)\n",
    "        validLoss = validLoss.eval()\n",
    "#         print('train loss %f'% (trainingError))\n",
    "        print('test loss %f'%(validLoss))\n",
    "        print('***********************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "# outputKeepProb = tf.placeholder(tf.float32, shape=())\n",
    "# inputKeepProb = tf.placeholder(tf.float32, shape=())\n",
    "# maxLength = helper.max_length\n",
    "# nLabels = helper.n_labels\n",
    "# xPlaceHolder = tf.placeholder(tf.int32, shape= (None, maxLength))\n",
    "# pretrainedEmbeddings = tf.get_collection('pretrainedEmbeddings')\n",
    "# yPlaceHolder = tf.placeholder(tf.int32, shape = (None, nLabels))\n",
    "trueWordIdxs = tf.placeholder(tf.int32, shape = (None,1))\n",
    "with tf.Session() as session:\n",
    "#         all_saver.restore(session, 'tempDelete/bestModel.data-00000-of-00001')\n",
    "    tf.global_variables_initializer().run()\n",
    "    modelDict = reloadModel(session = session,\n",
    "                            saverCheckPointPath = 'results/tempSaveTester/',\n",
    "                            saverMetaPath = 'results/tempSaveTester/bestModel.meta')\n",
    "#     new_saver = tf.train.import_meta_graph('results/tempSaveTester/bestModel.meta')\n",
    "#     new_saver.restore(session, tf.train.latest_checkpoint('results/tempSaveTester/'))\n",
    "#     y_last = tf.get_collection('y_last')[0]\n",
    "#     outputKeepProb = tf.get_collection('outputKeepProb')[0]\n",
    "#     inputKeepProb = tf.get_collection('inputKeepProb')[0]\n",
    "#     xPlaceHolder = tf.get_collection('xPlaceHolder')[0]\n",
    "#     trueWordIdxs = tf.get_collection('trueWordIdxs')[0]\n",
    "# #     y_last = tf.get_collection('y_last')[0]\n",
    "#     pretrainedEmbeddings = tf.get_collection('pretrainedEmbeddings')[0]\n",
    "    for i in range(3):\n",
    "#         _, trainingError = session.run([model.optimize, model.loss_function],\n",
    "#                            feed_dict={model.xPlaceHolder: xTrain, \n",
    "#                                         model.yPlaceHolder: yTrain, \n",
    "#                                         model.trueWordIdxs: lastTrueWordIdx_train,\n",
    "#                                         model.outputKeepProb: output_keep_prob, \n",
    "#                                         model.inputKeepProb: input_keep_prob})\n",
    "#         print('ok')\n",
    "#         pred_y = session.run(y_last,feed_dict={xPlaceHolder: xDev,\n",
    "#                                       trueWordIdxs:lastTrueWordIdx_dev,\n",
    "#                                       outputKeepProb: 1.0,\n",
    "#                                       inputKeepProb: 1.0}, ) # must be set to one for predictions.\n",
    "#         print('leggo')\n",
    "        pred_y = session.run(modelDict['y_last'],feed_dict={modelDict['xPlaceHolder']: xDev,\n",
    "                                      modelDict['trueWordIdxs']:lastTrueWordIdx_dev,\n",
    "                                      modelDict['outputKeepProb']: 1.0,\n",
    "                                      modelDict['inputKeepProb']: 1.0}, ) \n",
    "        validLoss = tf.nn.sigmoid_cross_entropy_with_logits(logits = pred_y, \n",
    "                                             labels = tf.cast(yDev, tf.float32))\n",
    "        validLoss = tf.reduce_mean(validLoss)\n",
    "        validLoss = validLoss.eval()\n",
    "#         print('train loss %f'% (trainingError))\n",
    "        print('test loss %f'%(validLoss))\n",
    "        print('***********************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import functools\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "\n",
    "# def LSTM(x, weight, bias, trueWordIdxs, outputKeepProb, inputKeepProb):\n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(n_hidden,state_is_tuple = True)\n",
    "#     cell = tf.contrib.rnn.DropoutWrapper(cell=cell, output_keep_prob = outputKeepProb, \n",
    "#                                          input_keep_prob = inputKeepProb)\n",
    "#     cell = tf.contrib.rnn.MultiRNNCell(cells=[cell] * num_layers, state_is_tuple=True)\n",
    "# #     If we ever wanna get fancy we can try the above.\n",
    "#     print(type(cell))\n",
    "#     print(cell)\n",
    "#     print(type(x))\n",
    "#     print(x.get_shape())\n",
    "#     print('cell output size')\n",
    "#     print(cell.output_size)\n",
    "#     print('cell state size')\n",
    "#     print(cell.state_size)\n",
    "\n",
    "#     output, state = tf.nn.dynamic_rnn(cell = cell, inputs = x, dtype = tf.float32)\n",
    "#     print('output shape')\n",
    "#     print(output.get_shape())\n",
    "#     # new code\n",
    "#     offset = tf.expand_dims(tf.range(0, batch_size, dtype = tf.int32)*helper.max_length, 1)\n",
    "#     offset = tf.expand_dims(tf.range(0, tf.shape(x)[0], dtype = tf.int32)*helper.max_length, 1)\n",
    "#     print('offset shape')\n",
    "#     print(offset.get_shape())\n",
    "#     output = tf.reshape(output,[-1, n_hidden]) # collapses the 3d matrix into a 2d\n",
    "#     # matrix where all matrices are stacked on top of eachother\n",
    "#     print('output shape new shape')\n",
    "#     print(output.get_shape())\n",
    "#     flattened_indices = trueWordIdxs + offset\n",
    "#     print('flattened indices shape')\n",
    "#     print(flattened_indices.get_shape())\n",
    "#     output_flattened = tf.gather(output, flattened_indices)\n",
    "#     output_flattened = tf.reshape(output_flattened, [-1, n_hidden])\n",
    "#     print('output flattened shape')\n",
    "#     print(output_flattened.get_shape())\n",
    "#     output_logits = tf.add(tf.matmul(output_flattened,weight),bias)\n",
    "#     print('output wx + b')\n",
    "#     print(output_logits.get_shape())\n",
    "#     return output_logits\n",
    "\n",
    "\n",
    "# def doublewrap(function):\n",
    "#     \"\"\"\n",
    "#     A decorator decorator, allowing to use the decorator to be used without\n",
    "#     parentheses if not arguments are provided. All arguments must be optional.\n",
    "#     \"\"\"\n",
    "#     @functools.wraps(function)\n",
    "#     def decorator(*args, **kwargs):\n",
    "#         if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
    "#             return function(args[0])\n",
    "#         else:\n",
    "#             return lambda wrapee: function(wrapee, *args, **kwargs)\n",
    "#     return decorator\n",
    "\n",
    "\n",
    "# @doublewrap\n",
    "# def define_scope(function, scope=None, *args, **kwargs):\n",
    "#     \"\"\"\n",
    "#     A decorator for functions that define TensorFlow operations. The wrapped\n",
    "#     function will only be executed once. Subsequent calls to it will directly\n",
    "#     return the result so that operations are added to the graph only once.\n",
    "#     The operations added by the function live within a tf.variable_scope(). If\n",
    "#     this decorator is used with arguments, they will be forwarded to the\n",
    "#     variable scope. The scope name defaults to the name of the wrapped\n",
    "#     function.\n",
    "#     \"\"\"\n",
    "#     attribute = '_cache_' + function.__name__\n",
    "#     name = scope or function.__name__\n",
    "#     @property\n",
    "#     @functools.wraps(function)\n",
    "#     def decorator(self):\n",
    "#         if not hasattr(self, attribute):\n",
    "#             with tf.variable_scope(name, *args, **kwargs):\n",
    "#                 setattr(self, attribute, function(self))\n",
    "#         return getattr(self, attribute)\n",
    "#     return decorator\n",
    "\n",
    "\n",
    "# class Model:\n",
    "\n",
    "#     def __init__(self, xPlaceHolder, yPlaceHolder, embeddings, hyperParamDict):\n",
    "#         self.xPlaceHolder = xPlaceHolder\n",
    "#         self.yPlaceHolder = yPlaceHolder\n",
    "# #         self.embeddings = embeddings\n",
    "#         self.pretrainedEmbeddings = tf.Variable(embeddings)\n",
    "#         self.hyperParamDict = hyperParamDict\n",
    "#         self.y_last\n",
    "#         self.loss_function\n",
    "#         self.optimize\n",
    "\n",
    "#     @define_scope#(initializer=tf.contrib.slim.xavier_initializer())\n",
    "#     def y_last(self):\n",
    "# #         print(wtfStr)\n",
    "#         n_classes = int(self.yPlaceHolder.shape[1])\n",
    "#         x = self.xPlaceHolder\n",
    "#         U = tf.get_variable(name = 'U', \n",
    "#                             shape = (self.hyperParamDict['n_hidden'], n_classes), \n",
    "#                         initializer = tf.contrib.layers.xavier_initializer())\n",
    "#         bias = tf.get_variable(name = 'bias', shape = [n_classes], \n",
    "#                                initializer = tf.constant_initializer(0))\n",
    "# #         pretrainedEmbeddings = tf.Variable(self.embeddings)\n",
    "#         wordEmbeddings = tf.nn.embedding_lookup(params = self.pretrainedEmbeddings, ids = x)\n",
    "#         print(wordEmbeddings.get_shape())\n",
    "#         print('shape of embeddings')\n",
    "#         print(wordEmbeddings.get_shape())\n",
    "#         print('U shape')\n",
    "#         print(U.get_shape())\n",
    "#         print('bias shape')\n",
    "#         print(bias.get_shape())\n",
    "#         y_last = LSTM(wordEmbeddings,U,bias, trueWordIdxs, outputKeepProb, inputKeepProb)# TODO is y_last the correct thing to return?\n",
    "#         print(y_last.get_shape())\n",
    "#         return(y_last)\n",
    "\n",
    "#     @define_scope\n",
    "#     def optimize(self):\n",
    "#         optimizer = tf.train.AdamOptimizer(\n",
    "#             learning_rate = self.hyperParamDict['learning_rate'])\n",
    "#         # optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate) dis from hw3\n",
    "#         tvars = tf.trainable_variables()\n",
    "#         grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss_function, tvars), \n",
    "#                                           max_grad_norm)\n",
    "#         gradientVars = zip(grads, tvars)\n",
    "#         train_op = optimizer.apply_gradients(gradientVars)\n",
    "#         return(train_op)\n",
    "# #         logprob = tf.log(self.prediction + 1e-12)\n",
    "# #         cross_entropy = -tf.reduce_sum(self.label * logprob)\n",
    "# #         optimizer = tf.train.RMSPropOptimizer(0.03)\n",
    "# #         return optimizer.minimize(cross_entropy)\n",
    "\n",
    "#     @define_scope\n",
    "#     def loss_function(self):\n",
    "#         batchError = tf.nn.sigmoid_cross_entropy_with_logits(logits = self.y_last, \n",
    "#                                                  labels = tf.cast(self.yPlaceHolder, tf.float32))\n",
    "#         loss_function = tf.reduce_mean(batchError)\n",
    "#         return(loss_function)\n",
    "    \n",
    "#     def save(self, savePath, session):\n",
    "#         \"\"\"\n",
    "#         Saves the computation map so that it can be loaded later for use\n",
    "#         in computation, or training.\n",
    "#         Might remove stuff needed for training (x, trueWordsIdx)\n",
    "#         \"\"\"\n",
    "#         all_saver = tf.train.Saver()\n",
    "#         tf.add_to_collection('y_last', self.y_last)\n",
    "#         tf.add_to_collection('xPlaceHolder', self.xPlaceHolder)# this just makes loading easier. Will have to\n",
    "#         # stop doing this in future because not necessary\n",
    "#         tf.add_to_collection('trueWordIdxs', trueWordIdxs)\n",
    "#         tf.add_to_collection('outputKeepProb', outputKeepProb)\n",
    "#         tf.add_to_collection('inputKeepProb', inputKeepProb)\n",
    "#         tf.add_to_collection('pretrainedEmbeddings', self.pretrainedEmbeddings)\n",
    "#         tf.add_to_collection('yPlaceHolder', self.yPlaceHolder)\n",
    "#         all_saver.save(session, savePath)\n",
    "        \n",
    "        \n",
    "# #     def load(session, saverMetaPath, saverCheckPointPath):\n",
    "# # #         saveMetaPath = 'results/temp/bestModel.meta'\n",
    "# # #         saverCheckPointPath = 'results/temp/'\n",
    "# #         new_saver = tf.train.import_meta_graph(saverMetaPath)\n",
    "# #         new_saver.restore(session, tf.train.latest_checkpoint(saverCheckPointPath))\n",
    "# #         xPlaceHolder = tf.get_collection('xPlaceHolder')[0]\n",
    "# #         trueWordIdxs = tf.get_collection('trueWordIdxs')[0]\n",
    "# #         outputKeepProb = tf.get_collection('outputKeepProb')[0]\n",
    "# #         inputKeepProb = tf.get_collection('inputKeepProb')[0]\n",
    "# #         pretrainedEmbeddings = tf.get_collection('pretrainedEmbeddings')[0]\n",
    "# #         yTruth = tf.get_collection('yTruth')[0]\n",
    "# #         y_last = tf.get_collection('y_last')[0]\n",
    "# #         self.xPlaceHolder = xPlaceHolder\n",
    "# #         self.yPlaceHolder = yPlaceHolder\n",
    "# # #         self.embeddings = embeddings\n",
    "# #         self.pretrainedEmbeddings = tf.Variable(embeddings)\n",
    "# #         self.hyperParamDict = hyperParamDict\n",
    "# #         self.y_last\n",
    "# #         self.loss_function\n",
    "# #         self.optimize\n",
    "# #         return(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
