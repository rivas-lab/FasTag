{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# So code is automatically reloaded when saved in different modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from scipy import stats\n",
    "import tensorflow as tf\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# import sklearn.metrics\n",
    "from sklearn import metrics\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import sys\n",
    "import pprint\n",
    "sys.path.append('src/taggerSystem/')\n",
    "from data_util import load_and_preprocess_data, load_embeddings, ModelHelper, lastTrueWordIdxs\n",
    "logger = logging.getLogger(\"hw3.q2\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_train = \"data/icd9NotesDataTable_train.csv\"\n",
    "# data_valid = \"data/icd9NotesDataTable_valid.csv\"\n",
    "data_train = \"data/smallIcd9NotesDataTable_train.csv\"\n",
    "data_valid = \"data/smallIcd9NotesDataTable_valid.csv\"\n",
    "vocab = \"src/taggerSystem/data_hw3_delete/vocab.txt\"\n",
    "wordVecs = \"src/taggerSystem/data_hw3_delete/wordVectors.txt\"\n",
    "output_path = 'results/model_description'\n",
    "# log_output = output_path + \"log\"\n",
    "# vocab = 'data/icd9Vocab.txt'# vocab for our data\n",
    "# wordVecs = 'data/newgloveicd9.txt'# len 300 word vectors\n",
    "output_path = 'results/{}/{:%Y%m%d_%H%M%S}/\".format(self.cell, datetime.now())'\n",
    "log_output = output_path + \"log\"\n",
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "NUM = \"NNNUMMM\"\n",
    "UNK = \"UUUNKKK\"\n",
    "EMBED_SIZE = 50 # this should not be manually set and should instead come from the word\n",
    "# vectors. Or maybe it's good that we have to set it so we know for sure the size\n",
    "# of word vecs we're using\n",
    "maxAllowedNoteLength = 500\n",
    "max_grad_norm = 5\n",
    "codeIdx = 9\n",
    "textIdx = 6\n",
    "num_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where should models and performances be saved to?\n"
     ]
    }
   ],
   "source": [
    "modelRunOutputPath = input(prompt = 'Where should models and performances be saved to?')\n",
    "if modelRunOutputPath == '':\n",
    "    modelRunOutputPath = 'results/temp'\n",
    "if not os.path.exists(modelRunOutputPath):\n",
    "    os.makedirs(modelRunOutputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data and Word Embeddings\n",
    "You'll probably only need to work with xDev, yDev and xTrain, yTrain. The X matrices hold all word IDs in the order they appear in the note. yDev is a matrix of indicator vectors for icd9 presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = codeIdx, textIdx = textIdx)\n",
    "embeddings = load_embeddings(vocab, wordVecs, helper, embeddingSize = EMBED_SIZE)\n",
    "lastTrueWordIdx_train = lastTrueWordIdxs(train)\n",
    "lastTrueWordIdx_dev = lastTrueWordIdxs(dev)\n",
    "helper.save(modelRunOutputPath)# token2id and max length saved to output_path\n",
    "# handler = logging.FileHandler(log_output)\n",
    "# handler.setLevel(logging.DEBUG)\n",
    "# handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "# logging.getLogger().addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(modelRunOutputPath, 'yDev.gz'),yDev)\n",
    "#np.save(os.path.join(modelRunOutputPath, 'xDev.npy'), xDev)\n",
    "np.savetxt(os.path.join(modelRunOutputPath, 'devTrueIdxs.gz'), lastTrueWordIdx_dev)\n",
    "# yDev = np.load(os.path.join(modelRunOutputPath, 'yDev.npy'))\n",
    "# xDev = np.load(os.path.join(modelRunOutputPath, 'xDev.npy'))\n",
    "# lastTrueWordIdx_dev = np.load(os.path.join(modelRunOutputPath, 'devTrueIdxs.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max note length 4\n",
      "Number of Icd9 codes 6\n",
      "icd9 present\n",
      "xDev shape: nObs = 7, nWords = 4\n",
      "yDev shape: nObs = 7, nClasses = 6\n",
      "xTrain shape: nObs = 7, nWords = 4\n",
      "yTrain shape: nObs = 7, nClasses = 6\n",
      "Embeddings shape: nWords = 13, wordVec len = 50\n",
      "{'123': 0, '18': 2, '4240': 3, '45': 5, '456': 1, '486': 4}\n"
     ]
    }
   ],
   "source": [
    "print('Max note length %d'%(helper.max_length))\n",
    "print('Number of Icd9 codes %d'%(helper.n_labels))\n",
    "print('icd9 present')\n",
    "print('xDev shape: nObs = %d, nWords = %d'%(xDev.shape))\n",
    "print('yDev shape: nObs = %d, nClasses = %d'%(yDev.shape))\n",
    "print('xTrain shape: nObs = %d, nWords = %d'%(xTrain.shape))\n",
    "print('yTrain shape: nObs = %d, nClasses = %d'%(yTrain.shape))\n",
    "print('Embeddings shape: nWords = %d, wordVec len = %d'%(embeddings.shape))\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(helper.icdDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches per epoch 7\n",
      "Embedding size is 50\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "batch_size = 1\n",
    "total_batches = (xTrain.shape[0]//batch_size)\n",
    "print('Total number of batches per epoch %d'%(total_batches))\n",
    "n_input = 1\n",
    "n_steps = 10\n",
    "n_hidden = 200\n",
    "n_classes = helper.n_labels\n",
    "output_keep_prob = 0.5\n",
    "input_keep_prob = 1# not sure why there are two but example used 0.5 for output. Check with TA\n",
    "embeddingSize = embeddings.shape[1]\n",
    "print('Embedding size is %d'%(embeddingSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# embeddingsSaved = embeddings\n",
    "# embeddings = embeddings[:, list(range(50))]\n",
    "# print(embeddingsSaved.shape)\n",
    "# print(embeddings.shape)\n",
    "# EMBED_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EMBED_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperParamsDict = {'EMBED_SIZE': EMBED_SIZE,\n",
    "                  'maxNoteLength': maxAllowedNoteLength,\n",
    "                  'maxGradNorm': max_grad_norm,\n",
    "                  'outputKeepProb': output_keep_prob,\n",
    "                  'inputKeepProb': input_keep_prob,\n",
    "                  'learningRate': learning_rate,\n",
    "                  'trainingEpochsMax': training_epochs,\n",
    "                  'batchSize': batch_size,\n",
    "                  'n_hidden': n_hidden}\n",
    "with open(os.path.join(modelRunOutputPath, 'hyperParamDict.pickle'), 'wb') as handle:\n",
    "    pickle.dump(hyperParamsDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EMBED_SIZE': 50,\n",
       " 'batchSize': 1,\n",
       " 'inputKeepProb': 1,\n",
       " 'learningRate': 0.001,\n",
       " 'maxGradNorm': 5,\n",
       " 'maxNoteLength': 500,\n",
       " 'n_hidden': 200,\n",
       " 'outputKeepProb': 0.5,\n",
       " 'trainingEpochsMax': 10}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperParamsDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.int32, shape= (None, helper.max_length))\n",
    "yTruth = tf.placeholder(tf.int32, shape = (None, helper.n_labels))\n",
    "y_steps = tf.placeholder(tf.int32, shape = (None, helper.n_labels))# not sure what this is\n",
    "trueWordIdxs = tf.placeholder(tf.int32, shape = (None,1))# vector which holds true word\n",
    "outputKeepProb = tf.placeholder(tf.float32, shape=())\n",
    "inputKeepProb = tf.placeholder(tf.float32, shape=())\n",
    "# indices of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as session:\n",
    "#     nClasses = 2\n",
    "#     nWords = 2\n",
    "#     nBatches = 3\n",
    "#     trueIndices = tf.constant([1,0,1], shape = (nBatches, 1))\n",
    "#     foo = tf.constant([1,2,3,4,5,6,7,8,9,10,11,12], shape = (nBatches,nWords,nClasses))\n",
    "#     # foo[:, 1] # [2, 5]\n",
    "# #     indices = tf.constant([1, 0, 1])\n",
    "#     print(foo.eval())\n",
    "    \n",
    "# #     print('transpose')\n",
    "# #     foo = tf.transpose(foo,[1,0,2])\n",
    "# #     print(foo.eval())\n",
    "    \n",
    "#     print('flattening')\n",
    "#     foo = tf.reshape(foo,[-1,nClasses])\n",
    "#     print(foo.eval())\n",
    "    \n",
    "#     print('offset')\n",
    "#     offset = tf.expand_dims(tf.range(0, nBatches, dtype = tf.int32) * nWords, 1)\n",
    "#     print(offset.get_shape())\n",
    "#     print(offset.eval())\n",
    "#     print('old indices')\n",
    "#     print(trueIndices.eval())\n",
    "#     print('new indices')\n",
    "#     flattened_indices = trueIndices + offset\n",
    "#     print(flattened_indices.eval())\n",
    "#     print(flattened_indices.get_shape())\n",
    "#     print('gathering')\n",
    "#     print(tf.gather(foo, flattened_indices).eval())\n",
    "#     print(tf.gather(foo, flattened_indices).get_shape())\n",
    "#     print('correcting shape')\n",
    "#     foo = tf.gather(foo, flattened_indices)\n",
    "#     foo = tf.reshape(foo, [-1, nClasses])\n",
    "#     print(foo)\n",
    "#     print(foo.get_shape())\n",
    "# # foo[:, indexes] # [2, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(output_keep_prob)\n",
    "print(input_keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LSTM(x, weight, bias, trueWordIdxs, outputKeepProb, inputKeepProb):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(n_hidden,state_is_tuple = True)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell=cell, output_keep_prob = outputKeepProb, \n",
    "                                         input_keep_prob = inputKeepProb)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells=[cell] * num_layers, state_is_tuple=True)\n",
    "#     If we ever wanna get fancy we can try the above.\n",
    "    print(type(cell))\n",
    "    print(cell)\n",
    "    print(type(x))\n",
    "    print(x.get_shape())\n",
    "    print('cell output size')\n",
    "    print(cell.output_size)\n",
    "    print('cell state size')\n",
    "    print(cell.state_size)\n",
    "\n",
    "    output, state = tf.nn.dynamic_rnn(cell = cell, inputs = x, dtype = tf.float32)\n",
    "    print('output shape')\n",
    "    print(output.get_shape())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # new code\n",
    "    offset = tf.expand_dims(tf.range(0, batch_size, dtype = tf.int32)*helper.max_length, 1)\n",
    "    offset = tf.expand_dims(tf.range(0, tf.shape(x)[0], dtype = tf.int32)*helper.max_length, 1)\n",
    "#     offset = tf.expand_dims(tf.range(0, nBatches, dtype = tf.int32) * nWords, 1)\n",
    "    print('offset shape')\n",
    "    print(offset.get_shape())\n",
    "#     1/0\n",
    "#     foo = tf.reshape(foo,[-1,nClasses])\n",
    "#     7/0\n",
    "#     print(tf.shape(output)[2])\n",
    "#     print(output.get_shape()[2])\n",
    "    output = tf.reshape(output,[-1, n_hidden]) # collapses the 3d matrix into a 2d\n",
    "    # matrix where all matrices are stacked on top of eachother\n",
    "    print('output shape new shape')\n",
    "    print(output.get_shape())\n",
    "    flattened_indices = trueWordIdxs + offset\n",
    "    print('flattened indices shape')\n",
    "    print(flattened_indices.get_shape())\n",
    "    output_flattened = tf.gather(output, flattened_indices)\n",
    "    output_flattened = tf.reshape(output_flattened, [-1, n_hidden])\n",
    "    print('output flattened shape')\n",
    "    print(output_flattened.get_shape())\n",
    "    output_logits = tf.add(tf.matmul(output_flattened,weight),bias)\n",
    "    print('output wx + b')\n",
    "    print(output_logits.get_shape())\n",
    "    return output_logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #old code\n",
    "    output_flattened = tf.gather(tf.transpose(output,[1,0,2]), helper.max_length - 1)\n",
    "    # should rename to output last really.\n",
    "    # here is where you're grabbing the last output\n",
    "    print('output flattened shape')\n",
    "    print(output_flattened.get_shape())\n",
    "    output_logits = tf.add(tf.matmul(output_flattened,weight),bias)\n",
    "# #     flattened_indices = trueIndices + offset\n",
    "#     print('flattened indices shape')\n",
    "#     print(flattened_indices.get_shape())\n",
    "    print('output wx + b')\n",
    "    print(output_logits.get_shape())\n",
    "    output_last = output_logits\n",
    "    print('output last shape')\n",
    "    print(output_last.get_shape())\n",
    "    return output_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4, 50)\n",
      "shape of embeddings\n",
      "(?, 4, 50)\n",
      "U shape\n",
      "(200, 6)\n",
      "bias shape\n",
      "(6,)\n",
      "<class 'tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell'>\n",
      "<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object at 0x7fc6e6a850b8>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(?, 4, 50)\n",
      "cell output size\n",
      "200\n",
      "cell state size\n",
      "(LSTMStateTuple(c=200, h=200), LSTMStateTuple(c=200, h=200), LSTMStateTuple(c=200, h=200), LSTMStateTuple(c=200, h=200))\n",
      "output shape\n",
      "(?, 4, 200)\n",
      "offset shape\n",
      "(?, 1)\n",
      "output shape new shape\n",
      "(?, 200)\n",
      "flattened indices shape\n",
      "(?, 1)\n",
      "output flattened shape\n",
      "(?, 200)\n",
      "output wx + b\n",
      "(?, 6)\n",
      "(?, 6)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('RNN_OutsideCell', reuse = False) as scope:\n",
    "    U = tf.get_variable(name = 'U', shape = (n_hidden, n_classes), \n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "    bias = tf.get_variable(name = 'bias', shape = [n_classes], \n",
    "                           initializer = tf.constant_initializer(0))\n",
    "    pretrainedEmbeddings = tf.Variable(embeddings)\n",
    "    wordEmbeddings = tf.nn.embedding_lookup(params = pretrainedEmbeddings, ids = x)\n",
    "    print(wordEmbeddings.get_shape())\n",
    "    print('shape of embeddings')\n",
    "    print(wordEmbeddings.get_shape())\n",
    "    print('U shape')\n",
    "    print(U.get_shape())\n",
    "    print('bias shape')\n",
    "    print(bias.get_shape())\n",
    "    y_last = LSTM(wordEmbeddings,U,bias, trueWordIdxs, outputKeepProb, inputKeepProb)# TODO is y_last the correct thing to return?\n",
    "    print(y_last.get_shape())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliver/anaconda3/envs/clinicalNoteTagger/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "batchError = tf.nn.sigmoid_cross_entropy_with_logits(logits = y_last, \n",
    "                                                 labels = tf.cast(yTruth, tf.float32))\n",
    "loss_function = tf.reduce_mean(batchError)\n",
    "\n",
    "\n",
    "\n",
    "# hw3 way\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate) dis from hw3\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss_function, tvars), max_grad_norm)\n",
    "gradientVars = zip(grads, tvars)\n",
    "train_op = optimizer.apply_gradients(gradientVars)\n",
    "\n",
    "# rando way\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# gvs = optimizer.compute_gradients(cost)\n",
    "# capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "# train_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "\n",
    "# train_op = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "# hacky work around for issue where -1 is padding but now maps to UUNNNKKK vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochAvgLoss = np.zeros(training_epochs)\n",
    "epochAvgLossValid = np.zeros(training_epochs)\n",
    "validLossIncreasingCount = 0\n",
    "maxIncreasingLossCount = 3\n",
    "prevValidLoss = np.inf\n",
    "minValidLoss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochPredictions = np.zeros(shape = [training_epochs, yDev.shape[0], yDev.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minValidLoss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# x = tf.placeholder(tf.int32, shape= (None, helper.max_length))\n",
    "# yTruth = tf.placeholder(tf.int32, shape = (None, helper.n_labels))\n",
    "# y_steps = tf.placeholder(tf.int32, shape = (None, helper.n_labels))# not sure what this is\n",
    "# trueWordIdxs = tf.placeholder(tf.int32, shape = (None,1))# vector which holds true word\n",
    "# outputKeepProb = tf.placeholder(tf.float32, shape=())\n",
    "# inputKeepProb = tf.placeholder(tf.float32, shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************\n",
      "***************************\n",
      "Running on epoch 0\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.693352\n",
      "average loss 0.687686\n",
      "test loss 0.676242\n",
      "Total run time was 0.517650\n",
      "New best model found. Saving\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 1\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.666012\n",
      "average loss 0.639375\n",
      "test loss 0.600651\n",
      "Total run time was 0.465339\n",
      "New best model found. Saving\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 2\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.560708\n",
      "average loss 0.530206\n",
      "test loss 0.445092\n",
      "Total run time was 0.365347\n",
      "New best model found. Saving\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 3\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.242976\n",
      "average loss 0.508250\n",
      "test loss 0.460142\n",
      "Total run time was 0.388031\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 4\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.070247\n",
      "average loss 0.450144\n",
      "test loss 0.457489\n",
      "Total run time was 0.630117\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 5\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.040110\n",
      "average loss 0.503448\n",
      "test loss 0.417191\n",
      "Total run time was 0.665290\n",
      "New best model found. Saving\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 6\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.096241\n",
      "average loss 0.429541\n",
      "test loss 0.398990\n",
      "Total run time was 0.617609\n",
      "New best model found. Saving\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 7\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.141105\n",
      "average loss 0.417713\n",
      "test loss 0.380100\n",
      "Total run time was 0.433668\n",
      "New best model found. Saving\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 8\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.057429\n",
      "average loss 0.379192\n",
      "test loss 0.365292\n",
      "Total run time was 0.486513\n",
      "New best model found. Saving\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 9\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.052920\n",
      "average loss 0.372790\n",
      "test loss 0.350417\n",
      "Total run time was 0.428804\n",
      "New best model found. Saving\n"
     ]
    }
   ],
   "source": [
    "all_saver = tf.train.Saver()\n",
    "tf.add_to_collection('y_last', y_last)\n",
    "tf.add_to_collection('x', x)\n",
    "tf.add_to_collection('trueWordIdxs', trueWordIdxs)\n",
    "tf.add_to_collection('outputKeepProb', outputKeepProb)\n",
    "tf.add_to_collection('inputKeepProb', inputKeepProb)\n",
    "tf.add_to_collection('pretrainedEmbeddings', pretrainedEmbeddings)\n",
    "with open(os.path.join(modelRunOutputPath, 'runOutput.txt'), 'w') as f:\n",
    "    with tf.Session() as session:\n",
    "#         all_saver.restore(session, 'tempDelete/bestModel.data-00000-of-00001')\n",
    "        tf.global_variables_initializer().run()\n",
    "        for epoch in range(training_epochs):\n",
    "            totalError = 0.0\n",
    "            f.write(\"\"\"***************************\n",
    "***************************\n",
    "Running on epoch %d\n",
    "***************************\n",
    "***************************\n",
    "***************************\\n\"\"\" %(epoch))\n",
    "            print('***************************')\n",
    "            print('***************************')\n",
    "            print('Running on epoch %d'% (epoch))\n",
    "            print('***************************')\n",
    "            print('***************************')\n",
    "            start = time.time()\n",
    "            for b in range(total_batches):\n",
    "#             for b in range(1):\n",
    "#             for b in range(total_batches + 1):\n",
    "#                 offset = (b * batch_size) % (yTrain.shape[0] - batch_size)\n",
    "                offset = min((b * batch_size), yTrain.shape[0])\n",
    "                batch_x = xTrain[offset:(offset + batch_size), :]\n",
    "                batch_y = yTrain[offset:(offset + batch_size), :]\n",
    "                batchTrueWordIdxs = lastTrueWordIdx_train[offset:(offset + batch_size)]\n",
    "#                 print(batch_x)\n",
    "#                 print(batchTrueWordIdxs)\n",
    "#                 print(batchTrueWordIdxs.shape)\n",
    "#                 print(batch_y)\n",
    "#                 print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')\n",
    "                _, c = session.run([train_op, loss_function],feed_dict={x: batch_x, \n",
    "                                                                        yTruth: batch_y, \n",
    "                                                                        trueWordIdxs: batchTrueWordIdxs,\n",
    "                                                                       outputKeepProb: output_keep_prob, \n",
    "                                                                        inputKeepProb: input_keep_prob})\n",
    "                if(b%25 == 0):\n",
    "                    f.write('running iteration %d with loss %3f \\n'% (b, c))\n",
    "                    print('running iteration %d with loss %3f'% (b, c))\n",
    "                totalError = totalError + c\n",
    "#             print('here we go')\n",
    "#             smallerSizeForComp = 13181\n",
    "#             smallerSizeForComp = 6590\n",
    "#             smallerSizeForComp = 3295\n",
    "            batchSizeDev = 3295\n",
    "            totalBatchesDev = (xDev.shape[0]//batchSizeDev)\n",
    "            pred_y = np.full(shape = yDev.shape, fill_value = -1.0, dtype = np.float32)\n",
    "            for b in range(totalBatchesDev + 1):\n",
    "#                 print(b)\n",
    "                offset = min((b * batchSizeDev), yDev.shape[0])\n",
    "                devBatch_x = xDev[offset:(offset + batchSizeDev), :]\n",
    "                devBatchTrueWordIdxs = lastTrueWordIdx_dev[offset:(offset + batchSizeDev)]\n",
    "                pred_yBatch = session.run(y_last,feed_dict={x: devBatch_x,\n",
    "                                                      trueWordIdxs:devBatchTrueWordIdxs,\n",
    "                                                      outputKeepProb: 1,\n",
    "                                                        inputKeepProb: 1}, ) # must be set to one for predictions.\n",
    "                pred_y[offset:(offset + batchSizeDev), :] = pred_yBatch\n",
    "#             print(pred_y.shape)\n",
    "            if (pred_y == -1).all():\n",
    "                print('negative values exist. This means indexing is off in pred_yBatch')\n",
    "                1/0\n",
    "#             print('got preds')\n",
    "            validLoss = tf.nn.sigmoid_cross_entropy_with_logits(logits = pred_y, \n",
    "                                                 labels = tf.cast(yDev, tf.float32))\n",
    "#             print('validLoss')\n",
    "            validLoss = tf.reduce_mean(validLoss)\n",
    "            validLoss = validLoss.eval()\n",
    "            epochPredictions[epoch,:,:] = pred_y\n",
    "            epochAvgLossValid[epoch] = validLoss\n",
    "            epochAvgLoss[epoch] = totalError/total_batches\n",
    "            print('average loss %f'% (totalError/total_batches))\n",
    "            print('test loss %f'%(validLoss))\n",
    "#             print('previous valid loss %f'%(prevValidLoss))\n",
    "            print('Total run time was %3f'% (time.time() - start))\n",
    "            f.write('average loss %f \\n'%(totalError/total_batches)) \n",
    "            f.write('test loss %f \\n'%(validLoss))\n",
    "            f.write('Total run time was %3f \\n'% (time.time() - start))\n",
    "            if validLoss <= minValidLoss:\n",
    "                validLossIncreasingCount = 0\n",
    "                minValidLoss = validLoss\n",
    "                print('New best model found. Saving')\n",
    "                all_saver.save(session, os.path.join(modelRunOutputPath, 'bestModel'))\n",
    "                # save model\n",
    "            if validLoss > prevValidLoss:\n",
    "                print('validation Loss Increase')\n",
    "                validLossIncreasingCount += 1\n",
    "#             print('increasing loss count %d'%(validLossIncreasingCount))\n",
    "            prevValidLoss = validLoss\n",
    "            if validLossIncreasingCount == maxIncreasingLossCount:\n",
    "                print('Stopping early because of increasing validation loss')\n",
    "                f.write('Stopping early because of increasing validation loss')\n",
    "                break\n",
    "#             if(epoch%10 == 0):\n",
    "#                 all_saver.save(session, os.path.join(modelRunOutputPath, 'checkpointModel'), \n",
    "#                                global_step = epoch)\n",
    "#             print(epochAvgLossValid[0:(epoch+1)])\n",
    "#             print(epochAvgLoss[0:(epoch+1)])\n",
    "lastEpoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.692554\n",
      "test loss 0.688521\n",
      "***********************************************\n",
      "train loss 0.688572\n",
      "test loss 0.683234\n",
      "***********************************************\n",
      "train loss 0.684168\n",
      "test loss 0.675837\n",
      "***********************************************\n",
      "train loss 0.675975\n",
      "test loss 0.664854\n",
      "***********************************************\n",
      "train loss 0.662930\n",
      "test loss 0.648139\n",
      "***********************************************\n",
      "train loss 0.654467\n",
      "test loss 0.623220\n",
      "***********************************************\n",
      "train loss 0.634896\n",
      "test loss 0.587198\n",
      "***********************************************\n",
      "train loss 0.600656\n",
      "test loss 0.540134\n",
      "***********************************************\n",
      "train loss 0.573571\n",
      "test loss 0.490091\n",
      "***********************************************\n",
      "train loss 0.524332\n",
      "test loss 0.454270\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "with tf.Session() as session:\n",
    "#         all_saver.restore(session, 'tempDelete/bestModel.data-00000-of-00001')\n",
    "    tf.global_variables_initializer().run()\n",
    "    for _ in range(training_epochs):\n",
    "        _, trainingError = session.run([train_op, loss_function],\n",
    "                           feed_dict={x: xTrain, \n",
    "                                        yTruth: yTrain, \n",
    "                                        trueWordIdxs: lastTrueWordIdx_train,\n",
    "                                        outputKeepProb: output_keep_prob, \n",
    "                                        inputKeepProb: input_keep_prob})\n",
    "        pred_y = session.run(y_last,feed_dict={x: xDev,\n",
    "                                      trueWordIdxs:lastTrueWordIdx_dev,\n",
    "                                      outputKeepProb: 1,\n",
    "                                      inputKeepProb: 1}, ) # must be set to one for predictions.\n",
    "        validLoss = tf.nn.sigmoid_cross_entropy_with_logits(logits = pred_y, \n",
    "                                             labels = tf.cast(yDev, tf.float32))\n",
    "        validLoss = tf.reduce_mean(validLoss)\n",
    "        validLoss = validLoss.eval()\n",
    "        print('train loss %f'% (trainingError))\n",
    "        print('test loss %f'%(validLoss))\n",
    "        print('***********************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
