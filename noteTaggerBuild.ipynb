{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/taggerSystem/')\n",
    "from data_util import load_and_preprocess_data, load_embeddings, ModelHelper, lastTrueWordIdxs\n",
    "from simpleLSTMWithNNetModel import Model, reloadModel\n",
    "from trainModel import trainModel\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pickle\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train = \"data/icd9NotesDataTable_train.csv\"\n",
    "data_valid = \"data/icd9NotesDataTable_valid.csv\"\n",
    "# data_train = \"data/smallIcd9NotesDataTable_train.csv\"\n",
    "# data_valid = \"data/smallIcd9NotesDataTable_valid.csv\"\n",
    "# vocab = \"src/taggerSystem/data_hw3_delete/vocab.txt\"\n",
    "# wordVecs = \"src/taggerSystem/data_hw3_delete/wordVectors.txt\"\n",
    "vocab = 'data/icd9Vocab.txt'# vocab for our data\n",
    "wordVecs = 'data/newgloveicd9.txt'# len 300 word vectors\n",
    "# output_path = 'results/model_description'\n",
    "# log_output = output_path + \"log\"\n",
    "# vocab = 'data/icd9Vocab.txt'# vocab for our data\n",
    "# wordVecs = 'data/newgloveicd9.txt'# len 300 word vectors\n",
    "# START_TOKEN = \"<s>\"\n",
    "# END_TOKEN = \"</s>\"\n",
    "NUM = \"NNNUMMM\"\n",
    "UNK = \"UUUNKKK\"\n",
    "EMBED_SIZE = 300 # this should not be manually set and should instead come from the word\n",
    "# vectors. Or maybe it's good that we have to set it so we know for sure the size\n",
    "# of word vecs we're using\n",
    "maxAllowedNoteLength = 1000\n",
    "max_grad_norm = 5\n",
    "codeIdx = 9\n",
    "textIdx = 6\n",
    "num_layers = 4\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 256\n",
    "n_input = 1\n",
    "# n_steps = 10\n",
    "n_hidden = 200\n",
    "# n_classes = helper.n_labels\n",
    "output_keep_prob = 0.5\n",
    "input_keep_prob = 1# not sure why there are two but example used 0.5 for output. Check with TA\n",
    "numLayers = 1\n",
    "# embeddingSize = embeddings.shape[1]\n",
    "# print('Embedding size is %d'%(embeddingSize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where should models and performances be saved to?rerunWithLongerNotes\n"
     ]
    }
   ],
   "source": [
    "output_path = input(prompt = 'Where should models and performances be saved to?')\n",
    "output_path = os.path.join('results', output_path)\n",
    "if output_path == '':\n",
    "    output_path = 'results/temp'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/rerunWithLongerNotes\n"
     ]
    }
   ],
   "source": [
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliverbdw4/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((39541, 1500), -1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/home/oliverbdw4/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((39541, 19), -1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/home/oliverbdw4/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((13181, 1500), -1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/home/oliverbdw4/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((13181, 19), -1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches per epoch 154\n",
      "Max note length 1500\n",
      "Number of Icd9 codes 19\n",
      "There are a total of 19 icd9 codes\n",
      "{   'cat:1': 14,\n",
      "    'cat:10': 10,\n",
      "    'cat:11': 8,\n",
      "    'cat:12': 5,\n",
      "    'cat:13': 13,\n",
      "    'cat:14': 1,\n",
      "    'cat:15': 6,\n",
      "    'cat:16': 18,\n",
      "    'cat:17': 7,\n",
      "    'cat:18': 2,\n",
      "    'cat:19': 11,\n",
      "    'cat:2': 17,\n",
      "    'cat:3': 9,\n",
      "    'cat:4': 3,\n",
      "    'cat:5': 12,\n",
      "    'cat:6': 0,\n",
      "    'cat:7': 16,\n",
      "    'cat:8': 15,\n",
      "    'cat:9': 4}\n",
      "icd9 present\n",
      "xDev shape: nObs = 13181, nWords = 1500\n",
      "yDev shape: nObs = 13181, nClasses = 19\n",
      "xTrain shape: nObs = 39541, nWords = 1500\n",
      "yTrain shape: nObs = 39541, nClasses = 19\n",
      "Embeddings shape: nWords = 10008, wordVec len = 300\n"
     ]
    }
   ],
   "source": [
    "# helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "#     data_train = data_train, data_valid = data_valid, \n",
    "#     maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "#     codeIdx = codeIdx, textIdx = textIdx,\n",
    "#     helperLoadPath = 'results/temp/')\n",
    "helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = codeIdx, textIdx = textIdx)\n",
    "embeddings = load_embeddings(vocab, wordVecs, helper, embeddingSize = EMBED_SIZE)\n",
    "lastTrueWordIdx_train = lastTrueWordIdxs(train)\n",
    "lastTrueWordIdx_dev = lastTrueWordIdxs(dev)\n",
    "helper.save(output_path)# token2id and max length saved to output_path\n",
    "# np.savetxt(os.path.join(output_path, 'yDev.gz'),yDev)\n",
    "# #np.save(os.path.join(modelRunOutputPath, 'xDev.npy'), xDev)\n",
    "# np.savetxt(os.path.join(output_path, 'devTrueIdxs.gz'), lastTrueWordIdx_dev)\n",
    "\n",
    "total_batches = (xTrain.shape[0]//batch_size)\n",
    "print('Total number of batches per epoch %d'%(total_batches))\n",
    "print('Max note length %d'%(helper.max_length))\n",
    "print('Number of Icd9 codes %d'%(helper.n_labels))\n",
    "print('There are a total of {} icd9 codes'.format(len(helper.icdDict.keys())))\n",
    "pp.pprint(helper.icdDict)\n",
    "print('icd9 present')\n",
    "print('xDev shape: nObs = %d, nWords = %d'%(xDev.shape))\n",
    "print('yDev shape: nObs = %d, nClasses = %d'%(yDev.shape))\n",
    "print('xTrain shape: nObs = %d, nWords = %d'%(xTrain.shape))\n",
    "print('yTrain shape: nObs = %d, nClasses = %d'%(yTrain.shape))\n",
    "print('Embeddings shape: nWords = %d, wordVec len = %d'%(embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max note length 500\n",
      "Number of Icd9 codes 19\n",
      "There are a total of 19 icd9 codes\n",
      "{   'cat:1': 16,\n",
      "    'cat:10': 13,\n",
      "    'cat:11': 5,\n",
      "    'cat:12': 14,\n",
      "    'cat:13': 4,\n",
      "    'cat:14': 17,\n",
      "    'cat:15': 15,\n",
      "    'cat:16': 12,\n",
      "    'cat:17': 3,\n",
      "    'cat:18': 9,\n",
      "    'cat:19': 11,\n",
      "    'cat:2': 2,\n",
      "    'cat:3': 6,\n",
      "    'cat:4': 10,\n",
      "    'cat:5': 18,\n",
      "    'cat:6': 8,\n",
      "    'cat:7': 1,\n",
      "    'cat:8': 0,\n",
      "    'cat:9': 7}\n",
      "icd9 present\n",
      "xDev shape: nObs = 13181, nWords = 500\n",
      "yDev shape: nObs = 13181, nClasses = 19\n",
      "xTrain shape: nObs = 39541, nWords = 500\n",
      "yTrain shape: nObs = 39541, nClasses = 19\n",
      "Embeddings shape: nWords = 10008, wordVec len = 300\n"
     ]
    }
   ],
   "source": [
    "print('Max note length %d'%(helper.max_length))\n",
    "print('Number of Icd9 codes %d'%(helper.n_labels))\n",
    "print('There are a total of {} icd9 codes'.format(len(helper.icdDict.keys())))\n",
    "pp.pprint(helper.icdDict)\n",
    "print('icd9 present')\n",
    "print('xDev shape: nObs = %d, nWords = %d'%(xDev.shape))\n",
    "print('yDev shape: nObs = %d, nClasses = %d'%(yDev.shape))\n",
    "print('xTrain shape: nObs = %d, nWords = %d'%(xTrain.shape))\n",
    "print('yTrain shape: nObs = %d, nClasses = %d'%(yTrain.shape))\n",
    "print('Embeddings shape: nWords = %d, wordVec len = %d'%(embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'EMBED_SIZE': 300,\n",
      "    'batchSize': 256,\n",
      "    'inputKeepProb': 1,\n",
      "    'learningRate': 0.001,\n",
      "    'maxGradNorm': 5,\n",
      "    'maxNoteLength': 1500,\n",
      "    'n_hidden': 200,\n",
      "    'numLayers': 1,\n",
      "    'outputKeepProb': 0.5,\n",
      "    'trainingEpochsMax': 100}\n"
     ]
    }
   ],
   "source": [
    "hyperParamDict = {'EMBED_SIZE': EMBED_SIZE,\n",
    "                  'maxNoteLength': maxAllowedNoteLength,\n",
    "                  'maxGradNorm': max_grad_norm,\n",
    "                  'outputKeepProb': output_keep_prob,\n",
    "                  'inputKeepProb': input_keep_prob,\n",
    "                  'learningRate': learning_rate,\n",
    "                  'trainingEpochsMax': training_epochs,\n",
    "                  'batchSize': batch_size,\n",
    "                  'n_hidden': n_hidden,\n",
    "                 'numLayers': numLayers}\n",
    "pp.pprint(hyperParamDict)\n",
    "with open(os.path.join(output_path, 'hyperParamDict.pickle'), 'wb') as handle:\n",
    "    pickle.dump(hyperParamDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliverbdw4/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************\n",
      "***************************\n",
      "Running on epoch 0\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.709596 at time 1.884425\n",
      "running iteration 25 with loss 0.494430 at time 35.698525\n",
      "running iteration 50 with loss 0.482137 at time 70.560906\n",
      "running iteration 75 with loss 0.483538 at time 104.666325\n",
      "running iteration 100 with loss 0.468726 at time 138.492056\n",
      "running iteration 125 with loss 0.460433 at time 172.514305\n",
      "running iteration 150 with loss 0.448424 at time 206.141070\n",
      "average training loss 0.486404\n",
      "test loss 0.483693\n",
      "Total run time was 229.877392\n",
      "New best model found. Saving\n",
      "results/rerunWithLongerNotes\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 1\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.457124 at time 1.280112\n",
      "running iteration 25 with loss 0.441687 at time 35.658185\n",
      "running iteration 50 with loss 0.435545 at time 69.681813\n",
      "running iteration 75 with loss 0.451771 at time 103.498036\n",
      "running iteration 100 with loss 0.438486 at time 137.214766\n",
      "running iteration 125 with loss 0.443998 at time 171.161789\n",
      "running iteration 150 with loss 0.430049 at time 205.168664\n",
      "average training loss 0.445963\n",
      "test loss 0.474305\n",
      "Total run time was 228.659559\n",
      "New best model found. Saving\n",
      "results/rerunWithLongerNotes\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 2\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.444736 at time 1.427231\n",
      "running iteration 25 with loss 0.429213 at time 34.977149\n",
      "running iteration 50 with loss 0.428812 at time 68.878986\n",
      "running iteration 75 with loss 0.428250 at time 102.780791\n",
      "running iteration 100 with loss 0.423089 at time 136.032179\n",
      "running iteration 125 with loss 0.416523 at time 168.531412\n",
      "running iteration 150 with loss 0.415283 at time 201.661305\n",
      "average training loss 0.431073\n",
      "test loss 0.457923\n",
      "Total run time was 225.081450\n",
      "New best model found. Saving\n",
      "results/rerunWithLongerNotes\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 3\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.426582 at time 1.363317\n",
      "running iteration 25 with loss 0.408096 at time 34.403766\n",
      "running iteration 50 with loss 0.409788 at time 67.140026\n",
      "running iteration 75 with loss 0.411123 at time 99.426979\n",
      "running iteration 100 with loss 0.407019 at time 132.173829\n",
      "running iteration 125 with loss 0.398984 at time 164.261570\n",
      "running iteration 150 with loss 0.403005 at time 196.615816\n",
      "average training loss 0.412057\n",
      "test loss 0.445216\n",
      "Total run time was 219.839489\n",
      "New best model found. Saving\n",
      "results/rerunWithLongerNotes\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 4\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.412385 at time 1.237684\n",
      "running iteration 25 with loss 0.396846 at time 33.136960\n",
      "running iteration 50 with loss 0.397870 at time 65.381580\n",
      "running iteration 75 with loss 0.399875 at time 97.556114\n",
      "running iteration 100 with loss 0.392482 at time 130.021807\n",
      "running iteration 125 with loss 0.391006 at time 162.013375\n",
      "running iteration 150 with loss 0.388579 at time 194.131829\n",
      "average training loss 0.399959\n",
      "test loss 0.436925\n",
      "Total run time was 217.504487\n",
      "New best model found. Saving\n",
      "results/rerunWithLongerNotes\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 5\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.400744 at time 1.282952\n",
      "running iteration 25 with loss 0.387348 at time 33.250624\n",
      "running iteration 50 with loss 0.390266 at time 65.083402\n",
      "running iteration 75 with loss 0.386751 at time 97.034508\n",
      "running iteration 100 with loss 0.385247 at time 128.979216\n",
      "running iteration 125 with loss 0.384277 at time 161.410791\n",
      "running iteration 150 with loss 0.377094 at time 193.449200\n",
      "average training loss 0.390920\n",
      "test loss 0.426720\n",
      "Total run time was 216.725674\n",
      "New best model found. Saving\n",
      "results/rerunWithLongerNotes\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 6\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.387787 at time 1.415016\n",
      "running iteration 25 with loss 0.376664 at time 33.819175\n",
      "running iteration 50 with loss 0.377313 at time 65.946158\n",
      "running iteration 75 with loss 0.383153 at time 97.581953\n",
      "running iteration 100 with loss 0.365042 at time 129.668792\n",
      "running iteration 125 with loss 0.365897 at time 161.766820\n",
      "running iteration 150 with loss 0.359370 at time 193.601116\n",
      "average training loss 0.373827\n",
      "test loss 0.413110\n",
      "Total run time was 217.046657\n",
      "New best model found. Saving\n",
      "results/rerunWithLongerNotes\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 7\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.370437 at time 1.354932\n",
      "running iteration 25 with loss 0.360087 at time 33.291636\n",
      "running iteration 50 with loss 0.363809 at time 65.175234\n",
      "running iteration 75 with loss 0.358616 at time 96.946332\n",
      "running iteration 100 with loss 0.352297 at time 128.988550\n",
      "running iteration 125 with loss 0.349989 at time 160.780838\n",
      "running iteration 150 with loss 0.349490 at time 193.067500\n",
      "average training loss 0.358148\n",
      "test loss 0.406115\n",
      "Total run time was 216.372774\n",
      "New best model found. Saving\n",
      "results/rerunWithLongerNotes\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 8\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.355198 at time 1.239565\n",
      "running iteration 25 with loss 0.348475 at time 33.256694\n",
      "running iteration 50 with loss 0.353543 at time 64.847467\n",
      "running iteration 75 with loss 0.341799 at time 96.995737\n",
      "running iteration 100 with loss 0.340927 at time 128.941877\n",
      "running iteration 125 with loss 0.343287 at time 160.725536\n",
      "running iteration 150 with loss 0.328818 at time 193.249618\n",
      "average training loss 0.344860\n",
      "test loss 0.399213\n",
      "Total run time was 216.538265\n",
      "New best model found. Saving\n",
      "results/rerunWithLongerNotes\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 9\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.342214 at time 1.205298\n",
      "running iteration 25 with loss 0.336859 at time 33.036645\n",
      "running iteration 50 with loss 0.340375 at time 64.714495\n",
      "running iteration 75 with loss 0.338345 at time 96.849804\n",
      "running iteration 100 with loss 0.322854 at time 128.647134\n",
      "running iteration 125 with loss 0.333429 at time 160.583621\n",
      "running iteration 150 with loss 0.323871 at time 193.100201\n",
      "average training loss 0.333042\n",
      "test loss 0.396486\n",
      "Total run time was 216.505639\n",
      "New best model found. Saving\n",
      "results/rerunWithLongerNotes\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 10\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.331897 at time 1.218365\n",
      "running iteration 25 with loss 0.327192 at time 33.507336\n",
      "running iteration 50 with loss 0.331696 at time 65.643066\n",
      "running iteration 75 with loss 0.325732 at time 97.419386\n",
      "running iteration 100 with loss 0.316941 at time 129.419737\n",
      "running iteration 125 with loss 0.319253 at time 161.470749\n",
      "running iteration 150 with loss 0.307514 at time 193.476732\n",
      "average training loss 0.322894\n",
      "test loss 0.393551\n",
      "Total run time was 216.803714\n",
      "New best model found. Saving\n",
      "results/rerunWithLongerNotes\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 11\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.320978 at time 1.391213\n",
      "running iteration 25 with loss 0.319208 at time 33.776059\n",
      "running iteration 50 with loss 0.319984 at time 65.337184\n",
      "running iteration 75 with loss 0.316006 at time 97.467238\n",
      "running iteration 100 with loss 0.307127 at time 129.362141\n",
      "running iteration 125 with loss 0.310572 at time 161.169742\n",
      "running iteration 150 with loss 0.303824 at time 192.922354\n",
      "average training loss 0.312977\n",
      "test loss 0.394005\n",
      "Total run time was 216.243825\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 12\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.311492 at time 1.390712\n",
      "running iteration 25 with loss 0.303349 at time 33.294730\n",
      "running iteration 50 with loss 0.310007 at time 65.373798\n",
      "running iteration 75 with loss 0.305493 at time 97.208897\n",
      "running iteration 100 with loss 0.294758 at time 129.206097\n",
      "running iteration 125 with loss 0.304651 at time 160.879368\n",
      "running iteration 150 with loss 0.294627 at time 192.702185\n",
      "average training loss 0.304082\n",
      "test loss 0.398089\n",
      "Total run time was 216.529686\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 13\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.300947 at time 1.195811\n",
      "running iteration 25 with loss 0.295756 at time 33.244717\n",
      "running iteration 50 with loss 0.299978 at time 65.226100\n",
      "running iteration 75 with loss 0.300202 at time 96.821142\n",
      "running iteration 100 with loss 0.291946 at time 128.545835\n",
      "running iteration 125 with loss 0.293997 at time 160.930690\n",
      "running iteration 150 with loss 0.289686 at time 192.752287\n",
      "average training loss 0.295380\n",
      "test loss 0.403019\n",
      "Total run time was 216.306653\n",
      "validation Loss Increase\n",
      "Stopping early because of increasing validation loss\n"
     ]
    }
   ],
   "source": [
    "from trainModel import trainModel\n",
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "trainModel(helperObj = helper, embeddings = embeddings, hyperParamDict = hyperParamDict, \n",
    "          xDev = xDev, xTrain = xTrain, yDev = yDev, yTrain = yTrain, \n",
    "           lastTrueWordIdx_dev = lastTrueWordIdx_dev, \n",
    "           lastTrueWordIdx_train = lastTrueWordIdx_train,\n",
    "           training_epochs = training_epochs, \n",
    "           output_path = output_path, batchSizeTrain = batch_size,\n",
    "           maxIncreasingLossCount = 3, batchSizeDev = 1500, chatty = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 0.427154\n",
      "***********************************************\n",
      "test loss 0.427154\n",
      "***********************************************\n",
      "test loss 0.427154\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "trueWordIdxs = tf.placeholder(tf.int32, shape = (None,1))\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    modelDict = reloadModel(session = session,\n",
    "                            saverCheckPointPath = 'results/temp/',\n",
    "                            saverMetaPath = 'results/temp/bestModel.meta')\n",
    "    for i in range(3):\n",
    "        pred_y = session.run(modelDict['y_last'],feed_dict={modelDict['xPlaceHolder']: xDev,\n",
    "                                      modelDict['trueWordIdxs']:lastTrueWordIdx_dev,\n",
    "                                      modelDict['outputKeepProb']: 1.0,\n",
    "                                      modelDict['inputKeepProb']: 1.0}, ) \n",
    "        validLoss = tf.nn.sigmoid_cross_entropy_with_logits(logits = pred_y, \n",
    "                                             labels = tf.cast(yDev, tf.float32))\n",
    "        validLoss = tf.reduce_mean(validLoss)\n",
    "        validLoss = validLoss.eval()\n",
    "        print('test loss %f'%(validLoss))\n",
    "        print('***********************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xDev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7217b07cbf1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxDev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xDev' is not defined"
     ]
    }
   ],
   "source": [
    "xDev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
