{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the LSTM on your Data\n",
    "\n",
    "This notebook will take you through the steps necessary to train an LSTM to recognize ICD-9 codes, or items from similar dictionaries, from free text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Make sure that the below packages are installed on the server on which this program will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/taggerSystem/')\n",
    "from data_util import load_and_preprocess_data, load_embeddings, ModelHelper, lastTrueWordIdxs\n",
    "import tensorflow as tf\n",
    "from simpleLSTMWithNNetModel import Model, reloadModel\n",
    "from trainModel import trainModel\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pickle\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories\n",
    "\n",
    "Executing the following cell will prompt the user to type in the directories corresponding to the training and validation sets, the vocabulary, and the word vector mappings. Defaults are given in the comments below.\n",
    "\n",
    "Note that the training and test data needs to have one column dedicated to free text (`noteIdx`) and another dedicated to top-level ICD-9 codes (`codeIdx`) associated with each patient. Preferably, the latter should be strung together using '-' as the delimiter (e.g. for patient 1, 1-2-6-4).\n",
    "\n",
    "Please make sure that the parameters such as the embed size, maximum note length, learning rate, number of maximum training epochs, batch size, hidden layer size, number of neural net layers, and probabilities are to your specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put sample file with require file headers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adjust training data headers to match small icd9 training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the path to the training data? data/0815/mimic_0815_train.csv\n",
      "What is the path to the validation data? data/0815/mimic_0815_valid.csv\n",
      "What is the path to the vocabulary? data/icd9Vocab.txt\n",
      "What is the path to the vocabulary? data/newgloveicd9.txt\n",
      "Which column contains top-level ICD-9 codes (outputs) in the training and test data? Default is 9. 9\n",
      "Which column contains notes/text (inputs) in the training and test data? Default is 6. 6\n"
     ]
    }
   ],
   "source": [
    "data_train = raw_input('What is the path to the training data? ') #default: data/icd9NotesDataTable_train.csv\n",
    "data_valid = raw_input('What is the path to the validation data? ') #default: data/icd9NotesDataTable_valid.csv\n",
    "vocab = raw_input('What is the path to the vocabulary? ') #default: data/icd9Vocab.txt\n",
    "wordVecs = raw_input('What is the path to the vocabulary? ') #data/newgloveicd9.txt. These are length 300 word vectors from GloVE\n",
    "\n",
    "NUM = \"NNNUMMM\"\n",
    "UNK = \"UUUNKKK\"\n",
    "EMBED_SIZE = 300 # this should correspond to the length of the word vectors\n",
    "maxAllowedNoteLength = 1000\n",
    "max_grad_norm = 5\n",
    "codeIdx = raw_input('Which column contains top-level ICD-9 codes (outputs) in the training and test data? Default is 9. ')\n",
    "textIdx = raw_input('Which column contains notes/text (inputs) in the training and test data? Default is 6. ')\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 256\n",
    "n_hidden = 200\n",
    "output_keep_prob = 0.5\n",
    "input_keep_prob = 1\n",
    "numLayers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# Fixing some issue ashley had\n",
    "\n",
    "textIdx = int(textIdx)\n",
    "codeIdx = int(codeIdx)# I'm not sure how models were trinaed before if this part was broken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, make sure that the output path is specified as you would like. By default, the program saves the output in a folder with the name of your choice within the folder `results`.\n",
    "\n",
    "If there exists a folder with results that you would like to load again, use that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where are models and performances (to be) saved? 0815_mimic\n"
     ]
    }
   ],
   "source": [
    "output_path = raw_input('Where are models and performances (to be) saved? ')\n",
    "output_path = os.path.join('results', output_path)\n",
    "if output_path == 'results/':\n",
    "    output_path = 'results/temp'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Executing the following cell will ask whether or not there is a previously saved model; if not, the model will train features from scratch, and if so, the features will be loaded.\n",
    "\n",
    "Note that AZ added \"int() to the codeIdx and textIdx to resolve some errors that were preventing it from initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a previously trained model? [Y/n] m\n",
      "Please respond with 'yes' or 'no' (or 'y' or 'n').\n",
      "Is there a previously trained model? [Y/n] m\n",
      "Please respond with 'yes' or 'no' (or 'y' or 'n').\n",
      "Is there a previously trained model? [Y/n] n\n",
      "Total number of batches per epoch: 154\n",
      "Maximum note length: 1000\n",
      "Number of ICD-9 codes: 19\n",
      "There are a total of: 19 ICD-9 codes\n",
      "{   'cat:1': 8,\n",
      "    'cat:10': 16,\n",
      "    'cat:11': 15,\n",
      "    'cat:12': 14,\n",
      "    'cat:13': 13,\n",
      "    'cat:14': 12,\n",
      "    'cat:15': 11,\n",
      "    'cat:16': 10,\n",
      "    'cat:17': 9,\n",
      "    'cat:18': 18,\n",
      "    'cat:19': 17,\n",
      "    'cat:2': 7,\n",
      "    'cat:3': 6,\n",
      "    'cat:4': 5,\n",
      "    'cat:5': 4,\n",
      "    'cat:6': 3,\n",
      "    'cat:7': 2,\n",
      "    'cat:8': 1,\n",
      "    'cat:9': 0}\n",
      "xDev shape: nObs = 13181, nWords = 1000\n",
      "yDev shape: nObs = 13181, nClasses = 19\n",
      "xTrain shape: nObs = 39541, nWords = 1000\n",
      "yTrain shape: nObs = 39541, nClasses = 19\n",
      "Embeddings shape: nWords = 10008, wordVec length = 300\n"
     ]
    }
   ],
   "source": [
    "sizeList = [n_hidden, 150, 75] # these are the weights we will be using\n",
    "\n",
    "def query_yes_no(question, default=\"yes\"):\n",
    "    \"\"\"Ask a yes/no question via raw_input() and return their answer.\n",
    "\n",
    "    \"question\" is a string that is presented to the user.\n",
    "    \"default\" is the presumed answer if the user just hits <Enter>.\n",
    "        It must be \"yes\" (the default), \"no\" or None (meaning\n",
    "        an answer is required of the user).\n",
    "\n",
    "    The \"answer\" return value is True for \"yes\" or False for \"no\".\n",
    "    \"\"\"\n",
    "    valid = {\"yes\": True, \"y\": True, \"ye\": True,\n",
    "             \"no\": False, \"n\": False}\n",
    "    if default is None:\n",
    "        prompt = \" [y/n] \"\n",
    "    elif default == \"yes\":\n",
    "        prompt = \" [Y/n] \"\n",
    "    elif default == \"no\":\n",
    "        prompt = \" [y/N] \"\n",
    "    else:\n",
    "        raise ValueError(\"invalid default answer: '%s'\" % default)\n",
    "\n",
    "    while True:\n",
    "        sys.stdout.write(question + prompt)\n",
    "        choice = raw_input().lower()\n",
    "        if default is not None and choice == '':\n",
    "            return valid[default]\n",
    "        elif choice in valid:\n",
    "            return valid[choice]\n",
    "        else:\n",
    "            sys.stdout.write(\"Please respond with 'yes' or 'no' \"\n",
    "                             \"(or 'y' or 'n').\\n\")\n",
    "            \n",
    "prev_model = query_yes_no(\"Is there a previously trained model?\")\n",
    "\n",
    "if prev_model:\n",
    "    helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = int(codeIdx), textIdx = int(textIdx),\n",
    "    helperLoadPath = output_path)\n",
    "else:\n",
    "    #print codeIdx\n",
    "    #print textIdx\n",
    "    helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = int(codeIdx), textIdx = int(textIdx))\n",
    "    \n",
    "embeddings = load_embeddings(vocab, wordVecs, helper, embeddingSize = EMBED_SIZE)\n",
    "lastTrueWordIdx_train = lastTrueWordIdxs(train)\n",
    "lastTrueWordIdx_dev = lastTrueWordIdxs(dev)\n",
    "helper.save(output_path) # token2id and max length saved to output_path\n",
    "sizeList.append(helper.n_labels)\n",
    "\n",
    "total_batches = (xTrain.shape[0]//batch_size)\n",
    "print('Total number of batches per epoch: %d'%(total_batches))\n",
    "print('Maximum note length: %d'%(helper.max_length))\n",
    "print('Number of ICD-9 codes: %d'%(helper.n_labels))\n",
    "print('There are a total of: {} ICD-9 codes'.format(len(helper.icdDict.keys())))\n",
    "pp.pprint(helper.icdDict)\n",
    "print('xDev shape: nObs = %d, nWords = %d'%(xDev.shape))\n",
    "print('yDev shape: nObs = %d, nClasses = %d'%(yDev.shape))\n",
    "print('xTrain shape: nObs = %d, nWords = %d'%(xTrain.shape))\n",
    "print('yTrain shape: nObs = %d, nClasses = %d'%(yTrain.shape))\n",
    "print('Embeddings shape: nWords = %d, wordVec length = %d'%(embeddings.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell initializes the dictionary of hyperparameters for the model that fully describe the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'EMBED_SIZE': 300,\n",
      "    'batchSize': 256,\n",
      "    'inputKeepProb': 1,\n",
      "    'learningRate': 0.001,\n",
      "    'maxGradNorm': 5,\n",
      "    'maxNoteLength': 1000,\n",
      "    'n_hidden': 200,\n",
      "    'numLayers': 1,\n",
      "    'outputKeepProb': 0.5,\n",
      "    'sizeList': [200, 150, 75, 19],\n",
      "    'trainingEpochsMax': 100}\n"
     ]
    }
   ],
   "source": [
    "hyperParamDict = {'EMBED_SIZE': EMBED_SIZE,\n",
    "                  'maxNoteLength': maxAllowedNoteLength,\n",
    "                  'maxGradNorm': max_grad_norm,\n",
    "                  'outputKeepProb': output_keep_prob,\n",
    "                  'inputKeepProb': input_keep_prob,\n",
    "                  'learningRate': learning_rate,\n",
    "                  'trainingEpochsMax': training_epochs,\n",
    "                  'batchSize': batch_size,\n",
    "                  'n_hidden': n_hidden,\n",
    "                 'numLayers': numLayers,\n",
    "                 'sizeList':sizeList}\n",
    "pp.pprint(hyperParamDict)\n",
    "with open(os.path.join(output_path, 'hyperParamDict.pickle'), 'wb') as handle:\n",
    "    pickle.dump(hyperParamDict, handle, protocol = 2)\n",
    "    #dumping with 2 because ALTUD uses python 2.7 right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, the model is trained (be wary - it will take some time; on an Amazon Deep Learning AMI, it took around an hour to train)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embeddings\n",
      "(?, 1000, 300)\n",
      "<class 'tensorflow.python.ops.rnn_cell_impl.MultiRNNCell'>\n",
      "<tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7facc4282650>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(?, 1000, 300)\n",
      "cell output size\n",
      "200\n",
      "cell state size\n",
      "(LSTMStateTuple(c=200, h=200),)\n",
      "output shape\n",
      "(?, 1000, 200)\n",
      "offset shape\n",
      "(?, 1)\n",
      "output shape new shape\n",
      "(?, 200)\n",
      "flattened indices shape\n",
      "(?, 1)\n",
      "output flattened shape\n",
      "(?, 200)\n",
      "(200, 150)\n",
      "W_1 shape\n",
      "(150,)\n",
      "b_1 shape\n",
      "(150, 75)\n",
      "W_2 shape\n",
      "(75,)\n",
      "bias shape\n",
      "(19,)\n",
      "U shape\n",
      "(75, 19)\n",
      "bias shape\n",
      "(19,)\n",
      "output wx + b\n",
      "(?, 19)\n",
      "(?, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************\n",
      "***************************\n",
      "Running on epoch 0\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.712959 at time 2.177852\n",
      "running iteration 25 with loss 0.504476 at time 33.703794\n",
      "running iteration 50 with loss 0.492231 at time 65.270715\n",
      "running iteration 75 with loss 0.486564 at time 96.834131\n",
      "running iteration 100 with loss 0.485935 at time 128.420802\n",
      "running iteration 125 with loss 0.487968 at time 160.004188\n",
      "running iteration 150 with loss 0.479473 at time 191.595145\n",
      "average training loss 0.499373\n",
      "test loss 0.495483\n",
      "Total run time was 211.958379\n",
      "New best model found. Saving\n",
      "results/0815_mimic\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 1\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.471719 at time 1.266999\n",
      "running iteration 25 with loss 0.460605 at time 32.876569\n",
      "running iteration 50 with loss 0.479632 at time 64.481843\n",
      "running iteration 75 with loss 0.449714 at time 96.077491\n",
      "running iteration 100 with loss 0.472928 at time 127.674738\n",
      "running iteration 125 with loss 0.449333 at time 159.273242\n",
      "running iteration 150 with loss 0.451407 at time 190.884063\n",
      "average training loss 0.458848\n",
      "test loss 0.482461\n",
      "Total run time was 211.252532\n",
      "New best model found. Saving\n",
      "results/0815_mimic\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 2\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.437640 at time 1.269781\n",
      "running iteration 25 with loss 0.441277 at time 32.884539\n",
      "running iteration 50 with loss 0.447193 at time 64.487423\n",
      "running iteration 75 with loss 0.443623 at time 96.096855\n",
      "running iteration 100 with loss 0.452362 at time 127.701825\n",
      "running iteration 125 with loss 0.445922 at time 159.302613\n",
      "running iteration 150 with loss 0.454650 at time 190.901173\n",
      "average training loss 0.447950\n",
      "test loss 0.490670\n",
      "Total run time was 211.275312\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 3\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.440794 at time 1.266419\n",
      "running iteration 25 with loss 0.448302 at time 32.878406\n",
      "running iteration 50 with loss 0.450570 at time 64.492101\n",
      "running iteration 75 with loss 0.442999 at time 96.088406\n",
      "running iteration 100 with loss 0.445964 at time 127.699923\n",
      "running iteration 125 with loss 0.441196 at time 159.307285\n",
      "running iteration 150 with loss 0.441732 at time 190.907680\n",
      "average training loss 0.443520\n",
      "test loss 0.478658\n",
      "Total run time was 211.298600\n",
      "New best model found. Saving\n",
      "results/0815_mimic\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 4\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.429401 at time 1.277857\n",
      "running iteration 25 with loss 0.436221 at time 32.897663\n",
      "running iteration 50 with loss 0.440325 at time 64.509288\n",
      "running iteration 75 with loss 0.435329 at time 96.112769\n",
      "running iteration 100 with loss 0.443683 at time 127.722516\n",
      "running iteration 125 with loss 0.434018 at time 159.335229\n",
      "running iteration 150 with loss 0.434786 at time 190.962681\n",
      "average training loss 0.436302\n",
      "test loss 0.473940\n",
      "Total run time was 211.386183\n",
      "New best model found. Saving\n",
      "results/0815_mimic\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 5\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.425211 at time 1.270045\n",
      "running iteration 25 with loss 0.429918 at time 32.886858\n",
      "running iteration 50 with loss 0.436007 at time 64.491181\n",
      "running iteration 75 with loss 0.431671 at time 96.101312\n",
      "running iteration 100 with loss 0.435983 at time 127.721319\n",
      "running iteration 125 with loss 0.421417 at time 159.336075\n",
      "running iteration 150 with loss 0.429796 at time 190.963991\n",
      "average training loss 0.428539\n",
      "test loss 0.465598\n",
      "Total run time was 211.406009\n",
      "New best model found. Saving\n",
      "results/0815_mimic\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 6\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.409919 at time 1.271817\n",
      "running iteration 25 with loss 0.427644 at time 32.872524\n",
      "running iteration 50 with loss 0.429791 at time 64.488449\n",
      "running iteration 75 with loss 0.426221 at time 96.101764\n",
      "running iteration 100 with loss 0.423624 at time 127.731879\n",
      "running iteration 125 with loss 0.416732 at time 159.348202\n",
      "running iteration 150 with loss 0.422586 at time 190.957753\n",
      "average training loss 0.420488\n",
      "test loss 0.459227\n",
      "Total run time was 211.381972\n",
      "New best model found. Saving\n",
      "results/0815_mimic\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 7\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.403998 at time 1.277235\n",
      "running iteration 25 with loss 0.415807 at time 32.887021\n",
      "running iteration 50 with loss 0.415882 at time 64.509466\n",
      "running iteration 75 with loss 0.421730 at time 96.128191\n",
      "running iteration 100 with loss 0.415188 at time 127.740643\n",
      "running iteration 125 with loss 0.402448 at time 159.358820\n",
      "running iteration 150 with loss 0.413002 at time 190.960355\n",
      "average training loss 0.412976\n",
      "test loss 0.453177\n",
      "Total run time was 211.437344\n",
      "New best model found. Saving\n",
      "results/0815_mimic\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 8\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.401847 at time 1.273569\n",
      "running iteration 25 with loss 0.414213 at time 32.885095\n",
      "running iteration 50 with loss 0.418005 at time 64.502009\n",
      "running iteration 75 with loss 0.414690 at time 96.126982\n",
      "running iteration 100 with loss 0.407461 at time 127.741695\n",
      "running iteration 125 with loss 0.400568 at time 159.362085\n",
      "running iteration 150 with loss 0.417236 at time 190.980127\n",
      "average training loss 0.407678\n",
      "test loss 0.452646\n",
      "Total run time was 211.466499\n",
      "New best model found. Saving\n",
      "results/0815_mimic\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 9\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.395060 at time 1.272828\n",
      "running iteration 25 with loss 0.407242 at time 32.906431\n",
      "running iteration 50 with loss 0.408238 at time 64.544702\n",
      "running iteration 75 with loss 0.410842 at time 96.175023\n",
      "running iteration 100 with loss 0.407264 at time 127.800614\n",
      "running iteration 125 with loss 0.394017 at time 159.426856\n",
      "running iteration 150 with loss 0.401336 at time 191.049171\n",
      "average training loss 0.401228\n",
      "test loss 0.444881\n",
      "Total run time was 211.512394\n",
      "New best model found. Saving\n",
      "results/0815_mimic\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 10\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.388962 at time 1.274139\n",
      "running iteration 25 with loss 0.394725 at time 32.904210\n",
      "running iteration 50 with loss 0.400421 at time 64.520843\n",
      "running iteration 75 with loss 0.398683 at time 96.146429\n",
      "running iteration 100 with loss 0.398064 at time 127.751802\n",
      "running iteration 125 with loss 0.384892 at time 159.364658\n",
      "running iteration 150 with loss 0.394280 at time 190.976036\n",
      "average training loss 0.393141\n",
      "test loss 0.442478\n",
      "Total run time was 211.423156\n",
      "New best model found. Saving\n",
      "results/0815_mimic\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 11\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.377963 at time 1.272405\n",
      "running iteration 25 with loss 0.390174 at time 32.897705\n",
      "running iteration 50 with loss 0.391938 at time 64.513530\n",
      "running iteration 75 with loss 0.391687 at time 96.125842\n",
      "running iteration 100 with loss 0.388056 at time 127.739379\n",
      "running iteration 125 with loss 0.377967 at time 159.361197\n",
      "running iteration 150 with loss 0.382312 at time 190.992735\n",
      "average training loss 0.385926\n",
      "test loss 0.436747\n",
      "Total run time was 211.430644\n",
      "New best model found. Saving\n",
      "results/0815_mimic\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 12\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.366400 at time 1.273442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running iteration 25 with loss 0.382165 at time 32.914015\n",
      "running iteration 50 with loss 0.384149 at time 64.524196\n",
      "running iteration 75 with loss 0.380359 at time 96.147809\n",
      "running iteration 100 with loss 0.376495 at time 127.761305\n",
      "running iteration 125 with loss 0.366010 at time 159.374370\n",
      "running iteration 150 with loss 0.370385 at time 191.002553\n",
      "average training loss 0.375875\n",
      "test loss 0.430305\n",
      "Total run time was 211.499705\n",
      "New best model found. Saving\n",
      "results/0815_mimic\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 13\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.360684 at time 1.274921\n",
      "running iteration 25 with loss 0.376052 at time 32.906664\n",
      "running iteration 50 with loss 0.375370 at time 64.528341\n",
      "running iteration 75 with loss 0.373947 at time 96.153116\n",
      "running iteration 100 with loss 0.366902 at time 127.774572\n",
      "running iteration 125 with loss 0.358759 at time 159.396725\n",
      "running iteration 150 with loss 0.362446 at time 191.035894\n",
      "average training loss 0.366588\n",
      "test loss 0.427252\n",
      "Total run time was 211.474467\n",
      "New best model found. Saving\n",
      "results/0815_mimic\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 14\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.354910 at time 1.278178\n",
      "running iteration 25 with loss 0.366473 at time 32.905685\n",
      "running iteration 50 with loss 0.369120 at time 64.524507\n",
      "running iteration 75 with loss 0.362539 at time 96.146215\n",
      "running iteration 100 with loss 0.359840 at time 127.785215\n",
      "running iteration 125 with loss 0.354357 at time 159.416945\n",
      "running iteration 150 with loss 0.354487 at time 191.049684\n",
      "average training loss 0.358474\n",
      "test loss 0.427786\n",
      "Total run time was 211.486985\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 15\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.347037 at time 1.267672\n",
      "running iteration 25 with loss 0.352557 at time 32.900474\n",
      "running iteration 50 with loss 0.360362 at time 64.528054\n",
      "running iteration 75 with loss 0.352764 at time 96.164143\n",
      "running iteration 100 with loss 0.351150 at time 127.791607\n",
      "running iteration 125 with loss 0.343705 at time 159.412642\n",
      "running iteration 150 with loss 0.346868 at time 191.045661\n",
      "average training loss 0.351457\n",
      "test loss 0.428959\n",
      "Total run time was 211.487708\n",
      "validation Loss Increase\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 16\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.336783 at time 1.265782\n",
      "running iteration 25 with loss 0.349233 at time 32.875214\n",
      "running iteration 50 with loss 0.351269 at time 64.496814\n",
      "running iteration 75 with loss 0.340651 at time 96.122232\n",
      "running iteration 100 with loss 0.346223 at time 127.748127\n",
      "running iteration 125 with loss 0.338065 at time 159.386439\n",
      "running iteration 150 with loss 0.335124 at time 191.002148\n",
      "average training loss 0.342931\n",
      "test loss 0.431456\n",
      "Total run time was 211.447044\n",
      "validation Loss Increase\n",
      "Stopping early because of increasing validation loss\n"
     ]
    }
   ],
   "source": [
    "from trainModel import trainModel\n",
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "trainModel(helperObj = helper, embeddings = embeddings, hyperParamDict = hyperParamDict, \n",
    "          xDev = xDev, xTrain = xTrain, yDev = yDev, yTrain = yTrain, \n",
    "           lastTrueWordIdx_dev = lastTrueWordIdx_dev, \n",
    "           lastTrueWordIdx_train = lastTrueWordIdx_train,\n",
    "           training_epochs = training_epochs, \n",
    "           output_path = output_path, batchSizeTrain = batch_size,\n",
    "           sizeList = sizeList,\n",
    "           maxIncreasingLossCount = 100, batchSizeDev = 1500, chatty = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the session closed. You should be able to see your results in the `output_path` directory you specified earlier.\n",
    "\n",
    "To evaluate the results and generate plots and such, please check out `predictionEvaluation.ipynb` in the same repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR USE IN LOADING OLD MODELS AND REEVALUATING\n",
    "# xDev[xDev == -1] = 0\n",
    "# xTrain[xTrain == -1] = 0\n",
    "# trueWordIdxs = tf.placeholder(tf.int32, shape = (None,1))\n",
    "# with tf.Session() as session:\n",
    "#     tf.global_variables_initializer().run()\n",
    "#     modelDict = reloadModel(session = session,\n",
    "#                             saverCheckPointPath = output_path,\n",
    "#                             saverMetaPath = output_path + '/bestModel.meta')\n",
    "#     print('here we go')\n",
    "#     for i in range(3):\n",
    "#         pred_y = session.run(modelDict['y_last'],feed_dict={modelDict['xPlaceHolder']: xDev,\n",
    "#                                       modelDict['trueWordIdxs']:lastTrueWordIdx_dev,\n",
    "#                                       modelDict['outputKeepProb']: 1.0,\n",
    "#                                       modelDict['inputKeepProb']: 1.0}, ) \n",
    "#         validLoss = tf.nn.sigmoid_cross_entropy_with_logits(logits = pred_y, \n",
    "#                                              labels = tf.cast(yDev, tf.float32))\n",
    "#         validLoss = tf.reduce_mean(validLoss)\n",
    "#         validLoss = validLoss.eval()\n",
    "#         print('test loss %f'%(validLoss))\n",
    "#         print('***********************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
