{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the LSTM on your Data\n",
    "\n",
    "This notebook will take you through the steps necessary to train an LSTM to recognize ICD-9 codes, or items from similar dictionaries, from free text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Make sure that the below packages are installed on the server on which this program will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/taggerSystem/')\n",
    "from data_util import load_and_preprocess_data, load_embeddings, ModelHelper, lastTrueWordIdxs\n",
    "import tensorflow as tf\n",
    "from simpleLSTMWithNNetModel import Model, reloadModel\n",
    "from trainModel import trainModel\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pickle\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories\n",
    "\n",
    "Executing the following cell will prompt the user to type in the directories corresponding to the training and validation sets, the vocabulary, and the word vector mappings. Defaults are given in the comments below.\n",
    "\n",
    "Note that the training and test data needs to have one column dedicated to free text (`noteIdx`) and another dedicated to top-level ICD-9 codes (`codeIdx`) associated with each patient. Preferably, the latter should be strung together using '-' as the delimiter (e.g. for patient 1, 1-2-6-4).\n",
    "\n",
    "Please make sure that the parameters such as the embed size, maximum note length, learning rate, number of maximum training epochs, batch size, hidden layer size, number of neural net layers, and probabilities are to your specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = raw_input('What is the path to the training data? ') #default: data/icd9NotesDataTable_train.csv\n",
    "data_valid = raw_input('What is the path to the validation data? ') #default: data/icd9NotesDataTable_valid.csv\n",
    "vocab = raw_input('What is the path to the vocabulary? ') #default: data/icd9Vocab.txt\n",
    "wordVecs = raw_input('What is the path to the vocabulary? ') #data/newgloveicd9.txt. These are length 300 word vectors from GloVE\n",
    "\n",
    "NUM = \"NNNUMMM\"\n",
    "UNK = \"UUUNKKK\"\n",
    "EMBED_SIZE = 300 # this should correspond to the length of the word vectors\n",
    "maxAllowedNoteLength = 1000\n",
    "max_grad_norm = 5\n",
    "codeIdx = raw_input('Which column contains top-level ICD-9 codes (outputs) in the training and test data? ') #default: 9\n",
    "textIdx = raw_input('Which column contains notes/text (inputs) in the training and test data? ') #default: 6\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 256\n",
    "n_hidden = 200\n",
    "output_keep_prob = 0.5\n",
    "input_keep_prob = 1\n",
    "numLayers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, make sure that the output path is specified as you would like. By default, the program saves the output in a folder with the name of your choice within the folder `results`.\n",
    "\n",
    "If there exists a folder with results that you would like to load again, use that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_path = raw_input('Where are models and performances (to be) saved? ')\n",
    "output_path = os.path.join('results', output_path)\n",
    "if output_path == 'results/':\n",
    "    output_path = 'results/temp'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Executing the following cell will ask whether or not there is a previously saved model; if not, the model will train features from scratch, and if so, the features will be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizeList = [n_hidden, 150, 75] # these are the weights we wil be using\n",
    "\n",
    "def query_yes_no(question, default=\"yes\"):\n",
    "    \"\"\"Ask a yes/no question via raw_input() and return their answer.\n",
    "\n",
    "    \"question\" is a string that is presented to the user.\n",
    "    \"default\" is the presumed answer if the user just hits <Enter>.\n",
    "        It must be \"yes\" (the default), \"no\" or None (meaning\n",
    "        an answer is required of the user).\n",
    "\n",
    "    The \"answer\" return value is True for \"yes\" or False for \"no\".\n",
    "    \"\"\"\n",
    "    valid = {\"yes\": True, \"y\": True, \"ye\": True,\n",
    "             \"no\": False, \"n\": False}\n",
    "    if default is None:\n",
    "        prompt = \" [y/n] \"\n",
    "    elif default == \"yes\":\n",
    "        prompt = \" [Y/n] \"\n",
    "    elif default == \"no\":\n",
    "        prompt = \" [y/N] \"\n",
    "    else:\n",
    "        raise ValueError(\"invalid default answer: '%s'\" % default)\n",
    "\n",
    "    while True:\n",
    "        sys.stdout.write(question + prompt)\n",
    "        choice = raw_input().lower()\n",
    "        if default is not None and choice == '':\n",
    "            return valid[default]\n",
    "        elif choice in valid:\n",
    "            return valid[choice]\n",
    "        else:\n",
    "            sys.stdout.write(\"Please respond with 'yes' or 'no' \"\n",
    "                             \"(or 'y' or 'n').\\n\")\n",
    "            \n",
    "prev_model = query_yes_no(\"Is there a previously trained model?\")\n",
    "\n",
    "if prev_model:\n",
    "    helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = codeIdx, textIdx = textIdx,\n",
    "    helperLoadPath = output_path)\n",
    "else:\n",
    "    helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = codeIdx, textIdx = textIdx)\n",
    "    \n",
    "embeddings = load_embeddings(vocab, wordVecs, helper, embeddingSize = EMBED_SIZE)\n",
    "lastTrueWordIdx_train = lastTrueWordIdxs(train)\n",
    "lastTrueWordIdx_dev = lastTrueWordIdxs(dev)\n",
    "helper.save(output_path) # token2id and max length saved to output_path\n",
    "sizeList.append(helper.n_labels)\n",
    "\n",
    "total_batches = (xTrain.shape[0]//batch_size)\n",
    "print('Total number of batches per epoch: %d'%(total_batches))\n",
    "print('Maximum note length: %d'%(helper.max_length))\n",
    "print('Number of ICD-9 codes: %d'%(helper.n_labels))\n",
    "print('There are a total of: {} ICD-9 codes'.format(len(helper.icdDict.keys())))\n",
    "pp.pprint(helper.icdDict)\n",
    "print('xDev shape: nObs = %d, nWords = %d'%(xDev.shape))\n",
    "print('yDev shape: nObs = %d, nClasses = %d'%(yDev.shape))\n",
    "print('xTrain shape: nObs = %d, nWords = %d'%(xTrain.shape))\n",
    "print('yTrain shape: nObs = %d, nClasses = %d'%(yTrain.shape))\n",
    "print('Embeddings shape: nWords = %d, wordVec length = %d'%(embeddings.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell initializes the dictionary of hyperparameters for the model that fully describe the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperParamDict = {'EMBED_SIZE': EMBED_SIZE,\n",
    "                  'maxNoteLength': maxAllowedNoteLength,\n",
    "                  'maxGradNorm': max_grad_norm,\n",
    "                  'outputKeepProb': output_keep_prob,\n",
    "                  'inputKeepProb': input_keep_prob,\n",
    "                  'learningRate': learning_rate,\n",
    "                  'trainingEpochsMax': training_epochs,\n",
    "                  'batchSize': batch_size,\n",
    "                  'n_hidden': n_hidden,\n",
    "                 'numLayers': numLayers,\n",
    "                 'sizeList':sizeList}\n",
    "pp.pprint(hyperParamDict)\n",
    "with open(os.path.join(output_path, 'hyperParamDict.pickle'), 'wb') as handle:\n",
    "    pickle.dump(hyperParamDict, handle, protocol = 2)\n",
    "    #dumping with 2 because ALTUD uses python 2.7 right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, the model is trained (be wary - it will take some time; on an Amazon Deep Learning AMI, it took around an hour to train)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from trainModel import trainModel\n",
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "trainModel(helperObj = helper, embeddings = embeddings, hyperParamDict = hyperParamDict, \n",
    "          xDev = xDev, xTrain = xTrain, yDev = yDev, yTrain = yTrain, \n",
    "           lastTrueWordIdx_dev = lastTrueWordIdx_dev, \n",
    "           lastTrueWordIdx_train = lastTrueWordIdx_train,\n",
    "           training_epochs = training_epochs, \n",
    "           output_path = output_path, batchSizeTrain = batch_size,\n",
    "           sizeList = sizeList,\n",
    "           maxIncreasingLossCount = 100, batchSizeDev = 1500, chatty = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the session closed. You should be able to see your results in the `output_path` directory you specified earlier.\n",
    "\n",
    "To evaluate the results and generate plots and such, please check out `predictionEvaluation.ipynb` in the same repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
