{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/taggerSystem/')\n",
    "from data_util import load_and_preprocess_data, load_embeddings, ModelHelper, lastTrueWordIdxs\n",
    "from simpleLSTMWithNNetModel import Model, reloadModel\n",
    "from trainModel import trainModel\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pickle\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_train = \"data/icd9NotesDataTable_train.csv\"\n",
    "# data_valid = \"data/icd9NotesDataTable_valid.csv\"\n",
    "data_train = \"data/smallIcd9NotesDataTable_train.csv\"\n",
    "data_valid = \"data/smallIcd9NotesDataTable_valid.csv\"\n",
    "vocab = \"src/taggerSystem/data_hw3_delete/vocab.txt\"\n",
    "wordVecs = \"src/taggerSystem/data_hw3_delete/wordVectors.txt\"\n",
    "# vocab = 'data/icd9Vocab.txt'# vocab for our data\n",
    "# wordVecs = 'data/newgloveicd9.txt'# len 300 word vectors\n",
    "# output_path = 'results/model_description'\n",
    "# log_output = output_path + \"log\"\n",
    "# vocab = 'data/icd9Vocab.txt'# vocab for our data\n",
    "# wordVecs = 'data/newgloveicd9.txt'# len 300 word vectors\n",
    "# START_TOKEN = \"<s>\"\n",
    "# END_TOKEN = \"</s>\"\n",
    "NUM = \"NNNUMMM\"\n",
    "UNK = \"UUUNKKK\"\n",
    "EMBED_SIZE = 50 # this should not be manually set and should instead come from the word\n",
    "# vectors. Or maybe it's good that we have to set it so we know for sure the size\n",
    "# of word vecs we're using\n",
    "maxAllowedNoteLength = 1000\n",
    "max_grad_norm = 5\n",
    "codeIdx = 9\n",
    "textIdx = 6\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 2\n",
    "n_input = 1\n",
    "# n_steps = 10\n",
    "n_hidden = 200\n",
    "numNNetLayers = 4\n",
    "# n_classes = helper.n_labels\n",
    "output_keep_prob = 0.5\n",
    "input_keep_prob = 1# not sure why there are two but example used 0.5 for output. Check with TA\n",
    "numLayers = 1\n",
    "# embeddingSize = embeddings.shape[1]\n",
    "# print('Embedding size is %d'%(embeddingSize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where should models and performances be saved to?\n"
     ]
    }
   ],
   "source": [
    "output_path = input(prompt = 'Where should models and performances be saved to?')\n",
    "output_path = os.path.join('results', output_path)\n",
    "if output_path == 'results/':\n",
    "    output_path = 'results/temp'\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/temp\n"
     ]
    }
   ],
   "source": [
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "temp = [1,2,3]\n",
    "print(temp)\n",
    "temp.insert(len(temp),4)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches per epoch 3\n",
      "Max note length 4\n",
      "Number of Icd9 codes 6\n",
      "There are a total of 6 icd9 codes\n",
      "{'123': 0, '18': 4, '4240': 2, '45': 1, '456': 5, '486': 3}\n",
      "icd9 present\n",
      "xDev shape: nObs = 7, nWords = 4\n",
      "yDev shape: nObs = 7, nClasses = 6\n",
      "xTrain shape: nObs = 7, nWords = 4\n",
      "yTrain shape: nObs = 7, nClasses = 6\n",
      "Embeddings shape: nWords = 13, wordVec len = 50\n"
     ]
    }
   ],
   "source": [
    "sizeList = [n_hidden, 150, 75] # these are the weights we wil be using\n",
    "# helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "#     data_train = data_train, data_valid = data_valid, \n",
    "#     maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "#     codeIdx = codeIdx, textIdx = textIdx,\n",
    "#     helperLoadPath = 'results/temp/')\n",
    "helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid, \n",
    "    maxAllowedNoteLength = maxAllowedNoteLength, \n",
    "    codeIdx = codeIdx, textIdx = textIdx)\n",
    "embeddings = load_embeddings(vocab, wordVecs, helper, embeddingSize = EMBED_SIZE)\n",
    "lastTrueWordIdx_train = lastTrueWordIdxs(train)\n",
    "lastTrueWordIdx_dev = lastTrueWordIdxs(dev)\n",
    "helper.save(output_path)# token2id and max length saved to output_path\n",
    "sizeList.append(helper.n_labels)\n",
    "# sizeList.(numNNetLayers,helper.n_labels)\n",
    "# np.savetxt(os.path.join(output_path, 'yDev.gz'),yDev)\n",
    "# #np.save(os.path.join(modelRunOutputPath, 'xDev.npy'), xDev)\n",
    "# np.savetxt(os.path.join(output_path, 'devTrueIdxs.gz'), lastTrueWordIdx_dev)\n",
    "\n",
    "total_batches = (xTrain.shape[0]//batch_size)\n",
    "print('Total number of batches per epoch %d'%(total_batches))\n",
    "print('Max note length %d'%(helper.max_length))\n",
    "print('Number of Icd9 codes %d'%(helper.n_labels))\n",
    "print('There are a total of {} icd9 codes'.format(len(helper.icdDict.keys())))\n",
    "pp.pprint(helper.icdDict)\n",
    "print('icd9 present')\n",
    "print('xDev shape: nObs = %d, nWords = %d'%(xDev.shape))\n",
    "print('yDev shape: nObs = %d, nClasses = %d'%(yDev.shape))\n",
    "print('xTrain shape: nObs = %d, nWords = %d'%(xTrain.shape))\n",
    "print('yTrain shape: nObs = %d, nClasses = %d'%(yTrain.shape))\n",
    "print('Embeddings shape: nWords = %d, wordVec len = %d'%(embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numNNetLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max note length 4\n",
      "Number of Icd9 codes 6\n",
      "There are a total of 6 icd9 codes\n",
      "{'123': 5, '18': 2, '4240': 4, '45': 0, '456': 1, '486': 3}\n",
      "icd9 present\n",
      "xDev shape: nObs = 7, nWords = 4\n",
      "yDev shape: nObs = 7, nClasses = 6\n",
      "xTrain shape: nObs = 7, nWords = 4\n",
      "yTrain shape: nObs = 7, nClasses = 6\n",
      "Embeddings shape: nWords = 13, wordVec len = 50\n"
     ]
    }
   ],
   "source": [
    "print('Max note length %d'%(helper.max_length))\n",
    "print('Number of Icd9 codes %d'%(helper.n_labels))\n",
    "print('There are a total of {} icd9 codes'.format(len(helper.icdDict.keys())))\n",
    "pp.pprint(helper.icdDict)\n",
    "print('icd9 present')\n",
    "print('xDev shape: nObs = %d, nWords = %d'%(xDev.shape))\n",
    "print('yDev shape: nObs = %d, nClasses = %d'%(yDev.shape))\n",
    "print('xTrain shape: nObs = %d, nWords = %d'%(xTrain.shape))\n",
    "print('yTrain shape: nObs = %d, nClasses = %d'%(yTrain.shape))\n",
    "print('Embeddings shape: nWords = %d, wordVec len = %d'%(embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200, 150, 75, 6]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'EMBED_SIZE': 50,\n",
      "    'batchSize': 2,\n",
      "    'inputKeepProb': 1,\n",
      "    'learningRate': 0.001,\n",
      "    'maxGradNorm': 5,\n",
      "    'maxNoteLength': 1000,\n",
      "    'n_hidden': 200,\n",
      "    'numLayers': 1,\n",
      "    'outputKeepProb': 0.5,\n",
      "    'sizeList': [200, 150, 75, 6],\n",
      "    'trainingEpochsMax': 20}\n"
     ]
    }
   ],
   "source": [
    "hyperParamDict = {'EMBED_SIZE': EMBED_SIZE,\n",
    "                  'maxNoteLength': maxAllowedNoteLength,\n",
    "                  'maxGradNorm': max_grad_norm,\n",
    "                  'outputKeepProb': output_keep_prob,\n",
    "                  'inputKeepProb': input_keep_prob,\n",
    "                  'learningRate': learning_rate,\n",
    "                  'trainingEpochsMax': training_epochs,\n",
    "                  'batchSize': batch_size,\n",
    "                  'n_hidden': n_hidden,\n",
    "                 'numLayers': numLayers,\n",
    "                 'sizeList':sizeList}\n",
    "pp.pprint(hyperParamDict)\n",
    "with open(os.path.join(output_path, 'hyperParamDict.pickle'), 'wb') as handle:\n",
    "    pickle.dump(hyperParamDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embeddings\n",
      "(?, 4, 50)\n",
      "<class 'tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell'>\n",
      "<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object at 0x7fa99ae5ef98>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(?, 4, 50)\n",
      "cell output size\n",
      "200\n",
      "cell state size\n",
      "(LSTMStateTuple(c=200, h=200),)\n",
      "output shape\n",
      "(?, 4, 200)\n",
      "offset shape\n",
      "(?, 1)\n",
      "output shape new shape\n",
      "(?, 200)\n",
      "flattened indices shape\n",
      "(?, 1)\n",
      "output flattened shape\n",
      "(?, 200)\n",
      "(200, 150)\n",
      "W_1 shape\n",
      "(150,)\n",
      "b_1 shape\n",
      "(150, 75)\n",
      "W_2 shape\n",
      "(75,)\n",
      "b_2 shape\n",
      "(6,)\n",
      "U shape\n",
      "(75, 6)\n",
      "bias shape\n",
      "(6,)\n",
      "output wx + b\n",
      "(?, 6)\n",
      "(?, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliver/anaconda3/envs/clinicalNoteTagger/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************\n",
      "***************************\n",
      "Running on epoch 0\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.684562 at time 0.180107\n",
      "average training loss 0.919147\n",
      "test loss 0.673587\n",
      "Total run time was 0.383716\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 1\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.662116 at time 0.018599\n",
      "average training loss 0.889346\n",
      "test loss 0.654156\n",
      "Total run time was 0.108622\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 2\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.623798 at time 0.015791\n",
      "average training loss 0.846234\n",
      "test loss 0.626552\n",
      "Total run time was 0.206234\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 3\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.590683 at time 0.011103\n",
      "average training loss 0.816029\n",
      "test loss 0.601430\n",
      "Total run time was 0.137447\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 4\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.538045 at time 0.018373\n",
      "average training loss 0.778923\n",
      "test loss 0.590418\n",
      "Total run time was 0.116859\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 5\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.506662 at time 0.012851\n",
      "average training loss 0.777062\n",
      "test loss 0.589914\n",
      "Total run time was 0.106310\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 6\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.484592 at time 0.023650\n",
      "average training loss 0.766349\n",
      "test loss 0.589398\n",
      "Total run time was 0.191039\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 7\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.483239 at time 0.019430\n",
      "average training loss 0.766019\n",
      "test loss 0.585227\n",
      "Total run time was 0.122310\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 8\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.482382 at time 0.013587\n",
      "average training loss 0.773312\n",
      "test loss 0.579763\n",
      "Total run time was 0.150173\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 9\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.492415 at time 0.011535\n",
      "average training loss 0.761735\n",
      "test loss 0.574974\n",
      "Total run time was 0.121971\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 10\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.492991 at time 0.011021\n",
      "average training loss 0.751077\n",
      "test loss 0.571689\n",
      "Total run time was 0.122058\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 11\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.498191 at time 0.010940\n",
      "average training loss 0.752299\n",
      "test loss 0.569755\n",
      "Total run time was 0.146643\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 12\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.486990 at time 0.010758\n",
      "average training loss 0.761895\n",
      "test loss 0.569226\n",
      "Total run time was 0.139560\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 13\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.523952 at time 0.014965\n",
      "average training loss 0.763822\n",
      "test loss 0.566429\n",
      "Total run time was 0.166867\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 14\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.506266 at time 0.015540\n",
      "average training loss 0.745680\n",
      "test loss 0.564153\n",
      "Total run time was 0.137643\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 15\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.482674 at time 0.012164\n",
      "average training loss 0.739084\n",
      "test loss 0.563054\n",
      "Total run time was 0.154603\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 16\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.482603 at time 0.024449\n",
      "average training loss 0.734771\n",
      "test loss 0.560292\n",
      "Total run time was 0.266838\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 17\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.482567 at time 0.014050\n",
      "average training loss 0.739123\n",
      "test loss 0.557502\n",
      "Total run time was 0.234394\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 18\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.485162 at time 0.025101\n",
      "average training loss 0.728001\n",
      "test loss 0.555045\n",
      "Total run time was 0.317145\n",
      "New best model found. Saving\n",
      "results/temp\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 19\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.483241 at time 0.011669\n",
      "average training loss 0.729114\n",
      "test loss 0.554041\n",
      "Total run time was 0.213444\n",
      "New best model found. Saving\n",
      "results/temp\n"
     ]
    }
   ],
   "source": [
    "from trainModel import trainModel\n",
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "trainModel(helperObj = helper, embeddings = embeddings, hyperParamDict = hyperParamDict, \n",
    "          xDev = xDev, xTrain = xTrain, yDev = yDev, yTrain = yTrain, \n",
    "           lastTrueWordIdx_dev = lastTrueWordIdx_dev, \n",
    "           lastTrueWordIdx_train = lastTrueWordIdx_train,\n",
    "           training_epochs = training_epochs, \n",
    "           output_path = output_path, batchSizeTrain = batch_size,\n",
    "           sizeList = sizeList,\n",
    "           maxIncreasingLossCount = 3, batchSizeDev = 1500, chatty = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/temp'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here we go\n",
      "test loss 0.597082\n",
      "***********************************************\n",
      "test loss 0.597082\n",
      "***********************************************\n",
      "test loss 0.597082\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "trueWordIdxs = tf.placeholder(tf.int32, shape = (None,1))\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    modelDict = reloadModel(session = session,\n",
    "                            saverCheckPointPath = 'results/temp/',\n",
    "                            saverMetaPath = 'results/temp/bestModel.meta')\n",
    "    print('here we go')\n",
    "    for i in range(3):\n",
    "        pred_y = session.run(modelDict['y_last'],feed_dict={modelDict['xPlaceHolder']: xDev,\n",
    "                                      modelDict['trueWordIdxs']:lastTrueWordIdx_dev,\n",
    "                                      modelDict['outputKeepProb']: 1.0,\n",
    "                                      modelDict['inputKeepProb']: 1.0}, ) \n",
    "        validLoss = tf.nn.sigmoid_cross_entropy_with_logits(logits = pred_y, \n",
    "                                             labels = tf.cast(yDev, tf.float32))\n",
    "        validLoss = tf.reduce_mean(validLoss)\n",
    "        validLoss = validLoss.eval()\n",
    "        print('test loss %f'%(validLoss))\n",
    "        print('***********************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xDev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7217b07cbf1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxDev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xDev' is not defined"
     ]
    }
   ],
   "source": [
    "xDev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
