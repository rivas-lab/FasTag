{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# So code is automatically reloaded when saved in different modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn.metrics\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import sys\n",
    "import pprint\n",
    "sys.path.append('src/taggerSystem/')\n",
    "from my_util import print_sentence, write_conll, read_conll\n",
    "from my_data_util import load_and_preprocess_data, load_embeddings, ModelHelper\n",
    "logger = logging.getLogger(\"hw3.q2\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_train = \"data/icd9NotesDataTable_train.csv\"\n",
    "# data_valid = \"data/icd9NotesDataTable_valid.csv\"\n",
    "data_train = \"data/smallIcd9NotesDataTable_train.csv\"\n",
    "data_valid = \"data/smallIcd9NotesDataTable_valid.csv\"\n",
    "vocab = \"src/taggerSystem/data_hw3_delete/vocab.txt\"\n",
    "wordVecs = \"src/taggerSystem/data_hw3_delete/wordVectors.txt\"\n",
    "output_path = 'results/{}/{:%Y%m%d_%H%M%S}/\".format(self.cell, datetime.now())'\n",
    "log_output = output_path + \"log\"\n",
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "NUM = \"NNNUMMM\"\n",
    "UNK = \"UUUNKKK\"\n",
    "EMBED_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data and Word Embeddings\n",
    "You'll probably only need to work with xDev, yDev and xTrain, yTrain. The X matrices hold all word IDs in the order they appear in the note. yDev is a matrix of indicator vectors for icd9 presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Loading training data...\n",
      "INFO:Done. Read 7 notes\n",
      "INFO:Loading dev data...\n",
      "INFO:Done. Read 7 notes\n",
      "INFO:Total read time 0.011189\n",
      "INFO:Built dictionary for 11 features.\n",
      "INFO:There are a total of 6 ICD codes\n",
      "INFO:Initialized embeddings.\n"
     ]
    }
   ],
   "source": [
    "helper, train, dev, train_raw, dev_raw, xTrain, yTrain, xDev, yDev = load_and_preprocess_data(\n",
    "    data_train = data_train, data_valid = data_valid)\n",
    "embeddings = load_embeddings(vocab, wordVecs, helper)\n",
    "helper.save(output_path)# token2id and max length saved to output_path\n",
    "handler = logging.FileHandler(log_output)\n",
    "handler.setLevel(logging.DEBUG)\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "logging.getLogger().addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "not overwriting xDev and yDev. You probably do not want this anyway\n"
     ]
    }
   ],
   "source": [
    "overWrite = input()\n",
    "# Never overwrite unless you've loaded the correct yDev and xDev. Should only be run\n",
    "# when on Azure with full data set\n",
    "if overWrite == 'yes':\n",
    "    print('overwriting xDev and yDev')\n",
    "    np.save('results/yDev.npy',yDev)\n",
    "    np.save('results/xDev.npy', xDev)\n",
    "else:\n",
    "    print('not overwriting xDev and yDev. You probably do not want this anyway')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max note length 4\n",
      "Number of Icd9 codes 6\n",
      "icd9 present\n",
      "xDev shape: nObs = 7, nWords = 4\n",
      "yDev shape: nObs = 7, nClasses = 6\n",
      "xTrain shape: nObs = 7, nWords = 4\n",
      "yTrain shape: nObs = 7, nClasses = 6\n",
      "{'123': 5, '18': 4, '4240': 0, '45': 2, '456': 1, '486': 3}\n"
     ]
    }
   ],
   "source": [
    "print('Max note length %d'%(helper.max_length))\n",
    "print('Number of Icd9 codes %d'%(helper.n_labels))\n",
    "print('icd9 present')\n",
    "print('xDev shape: nObs = %d, nWords = %d'%(xDev.shape))\n",
    "print('yDev shape: nObs = %d, nClasses = %d'%(yDev.shape))\n",
    "print('xTrain shape: nObs = %d, nWords = %d'%(xTrain.shape))\n",
    "print('yTrain shape: nObs = %d, nClasses = %d'%(yTrain.shape))\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(helper.icdDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches per epoch 7\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 1\n",
    "total_batches = (xTrain.shape[0]//batch_size)\n",
    "print('Total number of batches per epoch %d'%(total_batches))\n",
    "n_input = 1\n",
    "n_steps = 10\n",
    "n_hidden = 30\n",
    "n_classes = helper.n_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.int32, shape= (None, helper.max_length))\n",
    "yTruth = tf.placeholder(tf.int32, shape = (None, helper.n_labels))\n",
    "y_steps = tf.placeholder(tf.int32, shape = (None, helper.n_labels))# not sure what this is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LSTM(x, weight, bias):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(n_hidden,state_is_tuple = True)\n",
    "    print(type(cell))\n",
    "    print(cell)\n",
    "    print(type(x))\n",
    "    print(x.get_shape())\n",
    "    print('cell output size')\n",
    "    print(cell.output_size)\n",
    "    print('cell state size')\n",
    "    print(cell.state_size)\n",
    "\n",
    "    output, state = tf.nn.dynamic_rnn(cell = cell, inputs = x, dtype = tf.float32)\n",
    "    print('output shape')\n",
    "    print(output.get_shape())\n",
    "\n",
    "    output_flattened = tf.gather(tf.transpose(output,[1,0,2]), helper.max_length - 1)\n",
    "    # here is where you're grabbing the last output\n",
    "    print('output flattened shape')\n",
    "    print(output_flattened.get_shape())\n",
    "    output_logits = tf.add(tf.matmul(output_flattened,weight),bias)\n",
    "\n",
    "    print('output wx + b')\n",
    "    print(output_logits.get_shape())\n",
    "    output_last = output_logits\n",
    "    print('output last shape')\n",
    "    print(output_last.get_shape())\n",
    "    return output_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4, 50)\n",
      "shape of embeddings\n",
      "(?, 4, 50)\n",
      "U shape\n",
      "(30, 6)\n",
      "bias shape\n",
      "(6,)\n",
      "<class 'tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell'>\n",
      "<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x7fbcf84d5908>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(?, 4, 50)\n",
      "cell output size\n",
      "30\n",
      "cell state size\n",
      "LSTMStateTuple(c=30, h=30)\n",
      "output shape\n",
      "(?, 4, 30)\n",
      "output flattened shape\n",
      "(?, 30)\n",
      "output wx + b\n",
      "(?, 6)\n",
      "output last shape\n",
      "(?, 6)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('RNN_OutsideCell', reuse = False) as scope:\n",
    "    U = tf.get_variable(name = 'U', shape = (n_hidden, n_classes), \n",
    "                    initializer = tf.contrib.layers.xavier_initializer())\n",
    "    bias = tf.get_variable(name = 'bias', shape = [n_classes], \n",
    "                           initializer = tf.constant_initializer(0))\n",
    "    pretrainedEmbeddings = tf.Variable(embeddings)\n",
    "    wordEmbeddings = tf.nn.embedding_lookup(params = pretrainedEmbeddings, ids = x)\n",
    "    print(wordEmbeddings.get_shape())\n",
    "    print('shape of embeddings')\n",
    "    print(wordEmbeddings.get_shape())\n",
    "    print('U shape')\n",
    "    print(U.get_shape())\n",
    "    print('bias shape')\n",
    "    print(bias.get_shape())\n",
    "    y_last = LSTM(wordEmbeddings,U,bias)# TODO is y_last the correct thing to return?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliver/anaconda3/envs/clinicalNoteTagger/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "batchError = tf.nn.sigmoid_cross_entropy_with_logits(logits = y_last, \n",
    "                                                 labels = tf.cast(yTruth, tf.float32))\n",
    "loss_function = tf.reduce_mean(batchError)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xDev[xDev == -1] = 0\n",
    "xTrain[xTrain == -1] = 0\n",
    "# hacky work around for issue where -1 is padding but now maps to UUNNNKKK vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochAvgLoss = np.zeros(training_epochs)\n",
    "epochAvgLossValid = np.zeros(training_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochPredictions = np.zeros(shape = [training_epochs, yDev.shape[0], yDev.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************\n",
      "***************************\n",
      "Running on epoch 0\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.720330\n",
      "average loss 0.693065\n",
      "test loss 0.015680\n",
      "Total run time was 0.091300\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 1\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.635587\n",
      "average loss 0.649448\n",
      "test loss 0.015680\n",
      "Total run time was 0.049698\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 2\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.563132\n",
      "average loss 0.612057\n",
      "test loss 0.015680\n",
      "Total run time was 0.048444\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 3\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.495370\n",
      "average loss 0.576976\n",
      "test loss 0.015680\n",
      "Total run time was 0.058321\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 4\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.429063\n",
      "average loss 0.543204\n",
      "test loss 0.015680\n",
      "Total run time was 0.050445\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 5\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.364578\n",
      "average loss 0.511423\n",
      "test loss 0.015680\n",
      "Total run time was 0.061557\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 6\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.304904\n",
      "average loss 0.483019\n",
      "test loss 0.015680\n",
      "Total run time was 0.065798\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 7\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.253413\n",
      "average loss 0.458904\n",
      "test loss 0.015680\n",
      "Total run time was 0.059515\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 8\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.211942\n",
      "average loss 0.438702\n",
      "test loss 0.015680\n",
      "Total run time was 0.052191\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 9\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.180079\n",
      "average loss 0.420929\n",
      "test loss 0.015680\n",
      "Total run time was 0.074115\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 10\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.155834\n",
      "average loss 0.403943\n",
      "test loss 0.015680\n",
      "Total run time was 0.082291\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 11\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.136767\n",
      "average loss 0.386812\n",
      "test loss 0.015680\n",
      "Total run time was 0.136886\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 12\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.120793\n",
      "average loss 0.369424\n",
      "test loss 0.015680\n",
      "Total run time was 0.171009\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 13\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.106578\n",
      "average loss 0.352021\n",
      "test loss 0.015680\n",
      "Total run time was 0.157238\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 14\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.093633\n",
      "average loss 0.334924\n",
      "test loss 0.015680\n",
      "Total run time was 0.175849\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 15\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.082104\n",
      "average loss 0.318461\n",
      "test loss 0.015680\n",
      "Total run time was 0.099619\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 16\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.072289\n",
      "average loss 0.302857\n",
      "test loss 0.015680\n",
      "Total run time was 0.128371\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 17\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.064214\n",
      "average loss 0.288155\n",
      "test loss 0.015680\n",
      "Total run time was 0.102109\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 18\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.057626\n",
      "average loss 0.274245\n",
      "test loss 0.015680\n",
      "Total run time was 0.087416\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 19\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.052218\n",
      "average loss 0.260957\n",
      "test loss 0.015680\n",
      "Total run time was 0.076070\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 20\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.047746\n",
      "average loss 0.248136\n",
      "test loss 0.015680\n",
      "Total run time was 0.102482\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 21\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.044032\n",
      "average loss 0.235674\n",
      "test loss 0.015680\n",
      "Total run time was 0.129967\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 22\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.040930\n",
      "average loss 0.223511\n",
      "test loss 0.015680\n",
      "Total run time was 0.109239\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 23\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.038309\n",
      "average loss 0.211620\n",
      "test loss 0.015680\n",
      "Total run time was 0.080510\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 24\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.036050\n",
      "average loss 0.199997\n",
      "test loss 0.015680\n",
      "Total run time was 0.124828\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 25\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.034056\n",
      "average loss 0.188646\n",
      "test loss 0.015680\n",
      "Total run time was 0.120900\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 26\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.032251\n",
      "average loss 0.177578\n",
      "test loss 0.015680\n",
      "Total run time was 0.081141\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 27\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.030591\n",
      "average loss 0.166803\n",
      "test loss 0.015680\n",
      "Total run time was 0.072558\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 28\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.029052\n",
      "average loss 0.156341\n",
      "test loss 0.015680\n",
      "Total run time was 0.059307\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 29\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.027626\n",
      "average loss 0.146225\n",
      "test loss 0.015680\n",
      "Total run time was 0.060711\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 30\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.026314\n",
      "average loss 0.136511\n",
      "test loss 0.015680\n",
      "Total run time was 0.076404\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 31\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.025117\n",
      "average loss 0.127275\n",
      "test loss 0.015680\n",
      "Total run time was 0.058361\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 32\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.024038\n",
      "average loss 0.118599\n",
      "test loss 0.015680\n",
      "Total run time was 0.109291\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 33\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.023076\n",
      "average loss 0.110552\n",
      "test loss 0.015680\n",
      "Total run time was 0.096924\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 34\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.022225\n",
      "average loss 0.103164\n",
      "test loss 0.015680\n",
      "Total run time was 0.114598\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 35\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.021474\n",
      "average loss 0.096433\n",
      "test loss 0.015680\n",
      "Total run time was 0.101864\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 36\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.020804\n",
      "average loss 0.090326\n",
      "test loss 0.015680\n",
      "Total run time was 0.088080\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 37\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.020194\n",
      "average loss 0.084802\n",
      "test loss 0.015680\n",
      "Total run time was 0.111426\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 38\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.019624\n",
      "average loss 0.079812\n",
      "test loss 0.015680\n",
      "Total run time was 0.096038\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 39\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.019082\n",
      "average loss 0.075308\n",
      "test loss 0.015680\n",
      "Total run time was 0.105845\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 40\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.018559\n",
      "average loss 0.071241\n",
      "test loss 0.015680\n",
      "Total run time was 0.093228\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 41\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.018054\n",
      "average loss 0.067560\n",
      "test loss 0.015680\n",
      "Total run time was 0.126978\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 42\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.017566\n",
      "average loss 0.064221\n",
      "test loss 0.015680\n",
      "Total run time was 0.081675\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 43\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.017098\n",
      "average loss 0.061182\n",
      "test loss 0.015680\n",
      "Total run time was 0.074543\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 44\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.016649\n",
      "average loss 0.058404\n",
      "test loss 0.015680\n",
      "Total run time was 0.114394\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 45\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.016221\n",
      "average loss 0.055855\n",
      "test loss 0.015680\n",
      "Total run time was 0.073893\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 46\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.015812\n",
      "average loss 0.053508\n",
      "test loss 0.015680\n",
      "Total run time was 0.066964\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 47\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.015422\n",
      "average loss 0.051339\n",
      "test loss 0.015680\n",
      "Total run time was 0.073188\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 48\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.015048\n",
      "average loss 0.049329\n",
      "test loss 0.015680\n",
      "Total run time was 0.108012\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 49\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.014690\n",
      "average loss 0.047459\n",
      "test loss 0.015680\n",
      "Total run time was 0.090791\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 50\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.014346\n",
      "average loss 0.045716\n",
      "test loss 0.015680\n",
      "Total run time was 0.126956\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 51\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.014015\n",
      "average loss 0.044088\n",
      "test loss 0.015680\n",
      "Total run time was 0.113356\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 52\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.013696\n",
      "average loss 0.042563\n",
      "test loss 0.015680\n",
      "Total run time was 0.132382\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 53\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.013387\n",
      "average loss 0.041132\n",
      "test loss 0.015680\n",
      "Total run time was 0.123370\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 54\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.013089\n",
      "average loss 0.039788\n",
      "test loss 0.015680\n",
      "Total run time was 0.117456\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 55\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.012801\n",
      "average loss 0.038521\n",
      "test loss 0.015680\n",
      "Total run time was 0.149792\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 56\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.012521\n",
      "average loss 0.037327\n",
      "test loss 0.015680\n",
      "Total run time was 0.090434\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 57\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.012250\n",
      "average loss 0.036199\n",
      "test loss 0.015680\n",
      "Total run time was 0.074742\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 58\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.011987\n",
      "average loss 0.035131\n",
      "test loss 0.015680\n",
      "Total run time was 0.076474\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 59\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.011732\n",
      "average loss 0.034119\n",
      "test loss 0.015680\n",
      "Total run time was 0.140008\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 60\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.011485\n",
      "average loss 0.033159\n",
      "test loss 0.015680\n",
      "Total run time was 0.074371\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 61\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.011245\n",
      "average loss 0.032247\n",
      "test loss 0.015680\n",
      "Total run time was 0.104285\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 62\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.011012\n",
      "average loss 0.031378\n",
      "test loss 0.015680\n",
      "Total run time was 0.100857\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 63\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.010786\n",
      "average loss 0.030551\n",
      "test loss 0.015680\n",
      "Total run time was 0.098849\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 64\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.010567\n",
      "average loss 0.029762\n",
      "test loss 0.015680\n",
      "Total run time was 0.073007\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 65\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.010354\n",
      "average loss 0.029008\n",
      "test loss 0.015680\n",
      "Total run time was 0.082805\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 66\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.010148\n",
      "average loss 0.028288\n",
      "test loss 0.015680\n",
      "Total run time was 0.077432\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 67\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.009948\n",
      "average loss 0.027599\n",
      "test loss 0.015680\n",
      "Total run time was 0.075663\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 68\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.009753\n",
      "average loss 0.026939\n",
      "test loss 0.015680\n",
      "Total run time was 0.097745\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 69\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.009565\n",
      "average loss 0.026307\n",
      "test loss 0.015680\n",
      "Total run time was 0.089469\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 70\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.009382\n",
      "average loss 0.025700\n",
      "test loss 0.015680\n",
      "Total run time was 0.110958\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 71\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.009204\n",
      "average loss 0.025117\n",
      "test loss 0.015680\n",
      "Total run time was 0.110447\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 72\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.009031\n",
      "average loss 0.024557\n",
      "test loss 0.015680\n",
      "Total run time was 0.134880\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 73\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.008864\n",
      "average loss 0.024019\n",
      "test loss 0.015680\n",
      "Total run time was 0.116117\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 74\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.008701\n",
      "average loss 0.023500\n",
      "test loss 0.015680\n",
      "Total run time was 0.108526\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 75\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.008543\n",
      "average loss 0.023001\n",
      "test loss 0.015680\n",
      "Total run time was 0.086923\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 76\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.008389\n",
      "average loss 0.022520\n",
      "test loss 0.015680\n",
      "Total run time was 0.079348\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 77\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.008240\n",
      "average loss 0.022056\n",
      "test loss 0.015680\n",
      "Total run time was 0.079884\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 78\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.008095\n",
      "average loss 0.021609\n",
      "test loss 0.015680\n",
      "Total run time was 0.091400\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 79\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.007954\n",
      "average loss 0.021176\n",
      "test loss 0.015680\n",
      "Total run time was 0.123331\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 80\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.007817\n",
      "average loss 0.020759\n",
      "test loss 0.015680\n",
      "Total run time was 0.242548\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 81\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.007683\n",
      "average loss 0.020355\n",
      "test loss 0.015680\n",
      "Total run time was 0.197147\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 82\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.007553\n",
      "average loss 0.019965\n",
      "test loss 0.015680\n",
      "Total run time was 0.097158\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 83\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.007427\n",
      "average loss 0.019587\n",
      "test loss 0.015680\n",
      "Total run time was 0.085512\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 84\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.007303\n",
      "average loss 0.019221\n",
      "test loss 0.015680\n",
      "Total run time was 0.084839\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 85\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.007184\n",
      "average loss 0.018867\n",
      "test loss 0.015680\n",
      "Total run time was 0.104100\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 86\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.007067\n",
      "average loss 0.018523\n",
      "test loss 0.015680\n",
      "Total run time was 0.112123\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 87\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.006953\n",
      "average loss 0.018190\n",
      "test loss 0.015680\n",
      "Total run time was 0.132844\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 88\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.006842\n",
      "average loss 0.017867\n",
      "test loss 0.015680\n",
      "Total run time was 0.386820\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 89\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.006734\n",
      "average loss 0.017553\n",
      "test loss 0.015680\n",
      "Total run time was 0.114625\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 90\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.006629\n",
      "average loss 0.017249\n",
      "test loss 0.015680\n",
      "Total run time was 0.100129\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 91\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.006526\n",
      "average loss 0.016953\n",
      "test loss 0.015680\n",
      "Total run time was 0.107395\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 92\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.006426\n",
      "average loss 0.016666\n",
      "test loss 0.015680\n",
      "Total run time was 0.171273\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 93\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.006328\n",
      "average loss 0.016387\n",
      "test loss 0.015680\n",
      "Total run time was 0.194895\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 94\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.006233\n",
      "average loss 0.016115\n",
      "test loss 0.015680\n",
      "Total run time was 0.173262\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 95\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.006139\n",
      "average loss 0.015851\n",
      "test loss 0.015680\n",
      "Total run time was 0.168047\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 96\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.006049\n",
      "average loss 0.015594\n",
      "test loss 0.015680\n",
      "Total run time was 0.126330\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 97\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.005960\n",
      "average loss 0.015344\n",
      "test loss 0.015680\n",
      "Total run time was 0.128216\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 98\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.005873\n",
      "average loss 0.015100\n",
      "test loss 0.015680\n",
      "Total run time was 0.183407\n",
      "***************************\n",
      "***************************\n",
      "Running on epoch 99\n",
      "***************************\n",
      "***************************\n",
      "running iteration 0 with loss 0.005789\n",
      "average loss 0.014862\n",
      "test loss 0.015680\n",
      "Total run time was 0.118788\n"
     ]
    }
   ],
   "source": [
    "with open('runOutput.txt', 'w') as f:\n",
    "    with tf.Session() as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for epoch in range(training_epochs):\n",
    "            totalError = 0.0\n",
    "            f.write(\"\"\"***************************\n",
    "***************************\n",
    "Running on epoch %d\n",
    "***************************\n",
    "***************************\n",
    "***************************\\n\"\"\" %(epoch))\n",
    "            print('***************************')\n",
    "            print('***************************')\n",
    "            print('Running on epoch %d'% (epoch))\n",
    "            print('***************************')\n",
    "            print('***************************')\n",
    "            start = time.time()\n",
    "            for b in range(total_batches):\n",
    "                offset = (b * batch_size) % (yTrain.shape[0] - batch_size)\n",
    "                batch_x = xTrain[offset:(offset + batch_size), :]\n",
    "                batch_y = yTrain[offset:(offset + batch_size), :]\n",
    "                _, c = session.run([train_op, loss_function],feed_dict={x: batch_x, yTruth : batch_y})\n",
    "                if(b%25 == 0):\n",
    "                    f.write('running iteration %d with loss %3f \\n'% (b, c))\n",
    "                    print('running iteration %d with loss %3f'% (b, c))\n",
    "                totalError = totalError + c\n",
    "            pred_y = session.run(y_last,feed_dict={x:xDev})\n",
    "            validLoss = tf.nn.sigmoid_cross_entropy_with_logits(logits = pred_y, \n",
    "                                                 labels = tf.cast(yDev, tf.float32))\n",
    "            validLoss = tf.reduce_mean(testLoss)\n",
    "            validLoss = validLoss.eval()\n",
    "            epochPredictions[epoch,:,:] = pred_y\n",
    "            epochAvgLossValid[epoch] = validLoss\n",
    "            epochAvgLoss[epoch] = totalError/total_batches\n",
    "            print('average loss %f'% (totalError/total_batches))\n",
    "            print('test loss %f'%(testLoss))\n",
    "            print('Total run time was %3f'% (time.time() - start))\n",
    "            f.write('average loss %f \\n'%(totalError/total_batches)) \n",
    "            f.write('test loss %f \\n'%(testLoss))\n",
    "            f.write('Total run time was %3f \\n'% (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Performance Eval\n",
    "Below evaluates how we did on this set. Please only overwrite performance if you know the previous performance in this directory has been saved, and don't load the previous saved performance unless you want to use the current data. After you save performance you should manually copy the npy files over to results and give them  descriptive name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load previous\n",
      "loading performance from current directory\n"
     ]
    }
   ],
   "source": [
    "overWritePerformance = input()\n",
    "# only overwrite performance in this directory if you've saved\n",
    "# it to results with an appropriate name.\n",
    "# Only load if you want to lose the stats for the current run\n",
    "if overWritePerformance == 'yes':\n",
    "    print('overwriting performance in the current directory')\n",
    "    np.save('epochPreds.npy', epochPredictions)\n",
    "    np.save('epochAvgLoss.npy', epochAvgLoss)\n",
    "    np.save('epochAvgLossValid.npy', epochAvgLossValid)\n",
    "elif overWritePerformance == 'load previous':\n",
    "    print('loading performance from current directory')\n",
    "    epochPredictions = np.load('epochPreds.npy')\n",
    "    epochAvgLoss = np.load('epochAvgLoss.npy')\n",
    "    epochAvgLossValid = np.load('epochAvgLossValid.npy')\n",
    "else:\n",
    "    print('not overwriting or loadingperformance. Good call')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 13182, 20)\n",
      "(100,)\n",
      "(100,)\n",
      "(7, 6)\n"
     ]
    }
   ],
   "source": [
    "print(epochPredictions.shape)\n",
    "print(epochAvgLoss.shape)\n",
    "print(epochAvgLossValid.shape)\n",
    "print(yDev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
